{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k051L-Y3rTu"
   },
   "source": [
    "\n",
    "# Training a toy NMT model en-pt with the tatoeba corpus and JoeyNMT 2.0\n",
    "\n",
    "This notebook is based on [this demo](https://github.com/joeynmt/joeynmt/blob/main/notebooks/quick-start-with-joeynmt2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un8VWHfq5a-T"
   },
   "source": [
    "> ‚ö† **Important:** Before you start, set runtime type to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_s7s4uevEtx",
    "outputId": "c58870bf-7812-4013-b93e-e323e84dbc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  3 17:55:21 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 26%   25C    P0    52W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 29%   26C    P0    53W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 30%   27C    P0    57W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:D9:00.0 Off |                  N/A |\n",
      "| 10%   24C    P0    57W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "242SJA2q5dRr"
   },
   "source": [
    "Make sure that you have a compatible PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pQwoOS-OvMLf",
    "outputId": "0bd93e90-3ff0-4812-f5a1-7eb255a5116e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rHdT56SF1J3"
   },
   "source": [
    "Install joeynmt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxQUAeHKR7Im",
    "outputId": "eb9697ec-8159-4cf5-a68c-3e2768d6d203"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/lina-conti/joeynmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e ./joeynmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJBa6lx26Hdx"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Download\n",
    "We'll use English - Portuguese translations from the [Tatoeba](https://tatoeba.org/) collection ([CC-BY 2.0 FR](https://creativecommons.org/licenses/by/2.0/fr/)).\n",
    "\n",
    "[Tatoeba](https://huggingface.co/datasets/tatoeba) corpus is available in Huggingface's datasets library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/lconti/en-pt_tatoeba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUvuySKGzyfP",
    "outputId": "188199bc-d145-4475-fa54-168108b4727c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-01 11:07:17--  https://raw.githubusercontent.com/may-/datasets/master/datasets/tatoeba/tatoeba.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4425 (4.3K) [text/plain]\n",
      "Saving to: '/home/lconti/en-pt_tatoeba/data/get_tatoeba.py'\n",
      "\n",
      "/home/lconti/en-pt_ 100%[===================>]   4.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-01 11:07:18 (40.1 MB/s) - '/home/lconti/en-pt_tatoeba/data/get_tatoeba.py' saved [4425/4425]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir {data_dir}\n",
    "!wget -O {data_dir}/get_tatoeba.py https://raw.githubusercontent.com/may-/datasets/master/datasets/tatoeba/tatoeba.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NjaFsxw6IFU"
   },
   "source": [
    "The Tatoeba dataset on HuggingFace Hub doesn't have dev and test split, but train split only. So let's split the data manually and save it locally.\n",
    "\n",
    "> üìù Note that most of the dataset loading scripts in Huggingface have pre-defined train-dev-test splits, e.g. [wmt17](https://huggingface.co/datasets/wmt17). In that case, you can skip this step, please go to the Vocabulary generation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "6690085804c940ccb8dbd87e3f563678",
      "6505b17c75e64c709fc6a91cc362444d",
      "3a00af231d8a44e9972f6916948418fd",
      "e79f7e89c05d4ce4935f53929b8aaaa1",
      "4c447e15d9354c3c9a47c3ff843e74ce",
      "65a841a34dd549f4bbe9fed60fc03853",
      "31fdb10ecb2040f8bd5e07834b422ca0",
      "79a7716ac54b4f50981033df15f06f8e",
      "c26c414246154062bbd0682340613e40",
      "e73400a7748241d29340d21f94784a2e",
      "4a5a5a13229f404f9e4085751b0e2104"
     ]
    },
    "id": "eTPhy7M8vWw0",
    "outputId": "77d1d31c-edb9-40d5-c64e-62d54b81db35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-pt-lang1=en,lang2=pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset get_tatoeba/en-pt to /tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b356d5b36a470da41cdf3ecc9b8a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-pt-lang1=en,lang2=pt\n",
      "Reusing dataset get_tatoeba (/tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267)\n",
      "Using custom data configuration en-pt-lang1=en,lang2=pt\n",
      "Reusing dataset get_tatoeba (/tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset get_tatoeba downloaded and prepared to /tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'translation'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'translation'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'translation'],\n",
       "     num_rows: 215647\n",
       " }))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tatoeba_kwargs = {\n",
    "  \"path\": f\"{data_dir}/get_tatoeba.py\",\n",
    "  \"lang1\": \"en\",\n",
    "  \"lang2\": \"pt\",\n",
    "  \"ignore_verifications\": True,\n",
    "  \"cache_dir\": \"/tmp/.cache/huggingface\"\n",
    "}\n",
    "\n",
    "tatoeba_dev = load_dataset(split=\"train[:1000]\", **tatoeba_kwargs)\n",
    "tatoeba_test = load_dataset(split=\"train[1000:2000]\", **tatoeba_kwargs)\n",
    "tatoeba_train = load_dataset(split=\"train[2000:]\", **tatoeba_kwargs)\n",
    "\n",
    "tatoeba_dev, tatoeba_test, tatoeba_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mibraaK86JP0"
   },
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUFSJnWYvWzq",
    "outputId": "4a7ed79c-c7bb-4035-a0e8-38f7e515a734"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': \"Let's try something.\", 'pt': 'Vamos tentar alguma coisa!'},\n",
       " {'en': \"Let's try something.\", 'pt': 'Vamos tentar algo!'},\n",
       " {'en': \"Let's try something.\", 'pt': 'Vamos tentar algo.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba_dev['translation'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcBH2XbsvW2U",
    "outputId": "eae2a167-91a6-4038-fb33-0177bb7d1fc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': \"You're my type.\", 'pt': 'Voc√™ √© o meu tipo.'},\n",
       " {'en': \"You're irresistible.\", 'pt': 'Voc√™ √© irresist√≠vel.'},\n",
       " {'en': 'Could you call again later, please?',\n",
       "  'pt': 'Voc√™ poderia telefonar de novo mais tarde, por favor?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba_test['translation'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwGxfTTGvW44",
    "outputId": "f256020a-8672-4c8e-a2c0-35a6467cbbc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'What do you want now?', 'pt': 'O que voc√™ deseja agora?'},\n",
       " {'en': 'Do you love music?', 'pt': 'Voc√™ ama m√∫sica?'},\n",
       " {'en': 'Do you love music?', 'pt': 'Voc√™ aprecia a m√∫sica?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba_train['translation'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlCBTXOH6KSw"
   },
   "source": [
    "Save the train-dev-test splits in local dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jz97rpCkvW7w"
   },
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "dataset_dict = DatasetDict({ \n",
    "  \"train\": tatoeba_train,\n",
    "  \"validation\": tatoeba_dev,\n",
    "  \"test\": tatoeba_test\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1RMfXeT-V1m"
   },
   "source": [
    "### Vocabulary\n",
    "\n",
    "We will use the [sentencepiece](https://github.com/google/sentencepiece) library to split words into subwords (BPE) according to their frequency in the training corpus.\n",
    "\n",
    "`build_vocab.py` script will train the BPE model and creates joint vocabulary. It takes the same config file as the joeynmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fJML2jYR1PlG"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"tatoeba_enpt_sp\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"{data_dir}/train\"\n",
    "    dev: \"{data_dir}/validation\"\n",
    "    test: \"{data_dir}/test\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    #dataset_cfg:           # not necessary for manually saved pyarray daraset\n",
    "    #    name: \"en-pt\"\n",
    "    sample_dev_subset: 200\n",
    "    src:\n",
    "        lang: \"en\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "    trg:\n",
    "        lang: \"pt\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "\"\"\".format(data_dir=data_dir)\n",
    "with (Path(data_dir) / \"config_normal.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhZ19OVyAEQH"
   },
   "source": [
    "Call the script with `--joint` flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "v3cW4WdDPpXT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-01 11:20:25--  https://raw.githubusercontent.com/joeynmt/joeynmt/main/scripts/build_vocab.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10277 (10K) [text/plain]\n",
      "Saving to: '/home/lconti/en-pt_tatoeba/data/build_vocab.py'\n",
      "\n",
      "/home/lconti/en-pt_ 100%[===================>]  10.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-01 11:20:25 (92.8 MB/s) - '/home/lconti/en-pt_tatoeba/data/build_vocab.py' saved [10277/10277]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O {data_dir}/build_vocab.py https://raw.githubusercontent.com/joeynmt/joeynmt/main/scripts/build_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMkbAmPz1Pnx",
    "outputId": "6d50d281-47f4-4fa0-bcc5-b3d65bccbd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:02<00:00, 72.46ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 215642/215642 [00:28<00:00, 7436.45ex/s]\n",
      "### Training sentencepiece...\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/tmp/sentencepiece_6py8tfgs.txt --model_prefix=/home/lconti/en-pt_tatoeba/data/sp --model_type=unigram --vocab_size=32000 --character_coverage=1.0 --accept_language=en,pt --unk_piece=<unk> --bos_piece=<s> --eos_piece=</s> --pad_piece=<pad> --unk_id=0 --bos_id=2 --eos_id=3 --pad_id=1 --vocabulary_output_piece_score=false\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /tmp/sentencepiece_6py8tfgs.txt\n",
      "  input_format: \n",
      "  model_prefix: /home/lconti/en-pt_tatoeba/data/sp\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  accept_language: en\n",
      "  accept_language: pt\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 0\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /tmp/sentencepiece_6py8tfgs.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 431284 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=14706124\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=170\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 431284 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 157962 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 431284\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 101842\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 101842 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=54102 obj=10.9792 num_tokens=201010 num_tokens/piece=3.71539\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=43250 obj=8.65434 num_tokens=201758 num_tokens/piece=4.66492\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35194 obj=8.60258 num_tokens=209558 num_tokens/piece=5.95437\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35161 obj=8.59272 num_tokens=209737 num_tokens/piece=5.96505\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: /home/lconti/en-pt_tatoeba/data/sp.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: /home/lconti/en-pt_tatoeba/data/sp.vocab\n",
      "### Copying /home/lconti/en-pt_tatoeba/data/sp.vocab to /home/lconti/en-pt_tatoeba/data/vocab.txt ...\n",
      "### Done.\n"
     ]
    }
   ],
   "source": [
    "!python {data_dir}/build_vocab.py {data_dir}/config_normal.yaml --joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gmLvwo9AO2i"
   },
   "source": [
    "The generated vocabulary looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HN1n1gTKARR7",
    "outputId": "afcfb609-8435-4de6-d91b-d9f3ee8ab0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\r\n",
      "<pad>\r\n",
      "<s>\r\n",
      "</s>\r\n",
      ".\r\n",
      "‚ñÅTom\r\n",
      "'\r\n",
      "‚ñÅI\r\n",
      "?\r\n",
      "‚ñÅa\r\n",
      "‚ñÅto\r\n",
      "‚ñÅque\r\n",
      "s\r\n",
      ",\r\n",
      "‚ñÅthe\r\n",
      "‚ñÅde\r\n",
      "‚ñÅyou\r\n",
      "t\r\n",
      "‚ñÅo\r\n",
      "‚ñÅn√£o\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 {data_dir}/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6cN9CPtAaPl"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Joey NMT reads model and training hyperparameters from a configuration file. We're generating this now to configure paths in the appropriate places.\n",
    "\n",
    "The configuration below builds a small Transformer model with shared embeddings between source and target language on the base of the subword vocabularies created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/lconti/en-pt_tatoeba/models/normal_tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OJcLOr_S2BTD"
   },
   "outputs": [],
   "source": [
    "config += \"\"\"\n",
    "testing:\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 256\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 100\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    #return_prob: \"hyp\"\n",
    "    #return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "\n",
    "training:\n",
    "    #load_model: \"{model_dir}/latest.ckpt\"\n",
    "    #reset_best_ckpt: False\n",
    "    #reset_scheduler: False\n",
    "    #reset_optimizer: False\n",
    "    #reset_iter_state: False\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999]\n",
    "    scheduling: \"warmupinversesquareroot\"\n",
    "    learning_rate_warmup: 2000\n",
    "    learning_rate: 0.0002\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    loss: \"crossentropy\"\n",
    "    batch_size: 512\n",
    "    batch_type: \"token\"\n",
    "    batch_multiplier: 4\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 10\n",
    "    updates: 20000\n",
    "    validation_freq: 1000\n",
    "    logging_freq: 100\n",
    "    model_dir: \"{model_dir}\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\".format(model_dir=model_dir)\n",
    "with (Path(data_dir) / \"config_normal.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w45HbBfeMW38"
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH0-HBLLBLmR"
   },
   "source": [
    "### Run training\n",
    "‚è≥ This will take a while. Model parameters will be stored on mounted google drive. The log reports the training process, look out for the prints of example translations and the BLEU evaluation scores to get an impression of the current quality.\n",
    "\n",
    "> ‚õî If you execute this twice, you might get an error that the model directory already exists. You can specify in the configuration to overwrite it, or delete it manually (`!rm -r {model_dir}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTbfgVOq2BfB",
    "outputId": "b667e5d9-4587-4f92-f8a5-308b7826ac51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 11:33:41,216 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\r\n",
      "2022-08-01 11:33:41,216 - INFO - joeynmt.helpers -                           cfg.name : tatoeba_enpt_sp\r\n",
      "2022-08-01 11:33:41,216 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -                     cfg.data.train : /home/lconti/en-pt_tatoeba/data/train\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/lconti/en-pt_tatoeba/data/validation\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -                      cfg.data.test : /home/lconti/en-pt_tatoeba/data/test\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : 200\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/lconti/en-pt_tatoeba/data/vocab.txt\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : sentencepiece\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/data/sp.model\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : pt\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100\r\n",
      "2022-08-01 11:33:41,217 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/lconti/en-pt_tatoeba/data/vocab.txt\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : sentencepiece\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/data/sp.model\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 256\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\r\n",
      "2022-08-01 11:33:41,218 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 2000\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0002\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -            cfg.training.batch_size : 512\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -                cfg.training.epochs : 10\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -               cfg.training.updates : 20000\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/lconti/en-pt_tatoeba/model\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\r\n",
      "2022-08-01 11:33:41,219 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\r\n",
      "2022-08-01 11:33:41,220 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\r\n",
      "2022-08-01 11:33:41,221 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 11:33:41,255 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-01 11:33:41,544 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-01 11:33:41,552 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-01 11:33:41,552 - INFO - joeynmt.data - Loading train set...\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:03<00:00, 69.49ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 215642/215642 [00:29<00:00, 7243.66ex/s]\n",
      "2022-08-01 11:34:15,656 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-01 11:34:48,734 - INFO - joeynmt.data - Loading dev set...\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 76.94ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 8702.72ex/s]\n",
      "2022-08-01 11:34:50,046 - INFO - joeynmt.data - Loading test set...\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.49ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 10127.45ex/s]\n",
      "2022-08-01 11:34:51,234 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-01 11:34:51,235 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=215642, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=train, path=/home/lconti/en-pt_tatoeba/data/train)\n",
      "2022-08-01 11:34:51,235 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/data/validation)\n",
      "2022-08-01 11:34:51,235 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/data/test)\n",
      "2022-08-01 11:34:51,235 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] ‚ñÅWhat ‚ñÅdo ‚ñÅyou ‚ñÅwant ‚ñÅnow ?\n",
      "\t[TRG] ‚ñÅO ‚ñÅque ‚ñÅvoc√™ ‚ñÅdeseja ‚ñÅagora ?\n",
      "2022-08-01 11:34:51,236 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ‚ñÅTom (6) ' (7) ‚ñÅI (8) ? (9) ‚ñÅa\n",
      "2022-08-01 11:34:51,236 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ‚ñÅTom (6) ' (7) ‚ñÅI (8) ? (9) ‚ñÅa\n",
      "2022-08-01 11:34:51,236 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-01 11:34:51,236 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-01 11:34:51,257 - WARNING - joeynmt.tokenizers - /home/lconti/en-pt_tatoeba/model/sp.model already exists. Stop copying.\n",
      "2022-08-01 11:34:51,258 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-01 11:34:52,273 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-01 11:34:52,282 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-01 11:34:52,284 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-08-01 11:34:56,529 - INFO - joeynmt.builders - Adam(lr=0.0002, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-08-01 11:34:56,529 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=2000, decay_rate=0.008944, peak_rate=0.0002, min_rate=1e-08)\n",
      "2022-08-01 11:34:56,529 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 4\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 128\n",
      "\teffective batch size (w. parallel & accumulation): 2048\n",
      "2022-08-01 11:34:56,530 - INFO - joeynmt.training - EPOCH 1\n",
      "/home/lconti/anaconda3/envs/jnmt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2022-08-01 11:37:40,317 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     2.153650, Batch Acc: 0.000798, Tokens per Sec:      582, Lr: 0.000010\n",
      "2022-08-01 11:39:52,190 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.034654, Batch Acc: 0.001079, Tokens per Sec:      710, Lr: 0.000020\n",
      "2022-08-01 11:41:52,257 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     1.857771, Batch Acc: 0.001237, Tokens per Sec:      788, Lr: 0.000030\n",
      "2022-08-01 11:43:52,623 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     1.632019, Batch Acc: 0.002482, Tokens per Sec:      783, Lr: 0.000040\n",
      "2022-08-01 11:45:53,248 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     1.445068, Batch Acc: 0.002389, Tokens per Sec:      791, Lr: 0.000050\n",
      "2022-08-01 11:47:53,508 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     1.348051, Batch Acc: 0.002109, Tokens per Sec:      785, Lr: 0.000060\n",
      "2022-08-01 11:50:19,036 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     1.245431, Batch Acc: 0.002151, Tokens per Sec:      652, Lr: 0.000070\n",
      "2022-08-01 11:52:49,241 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     1.179126, Batch Acc: 0.002716, Tokens per Sec:      635, Lr: 0.000080\n",
      "2022-08-01 11:55:17,998 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     1.187509, Batch Acc: 0.002063, Tokens per Sec:      639, Lr: 0.000090\n",
      "2022-08-01 11:57:47,339 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     1.090793, Batch Acc: 0.003530, Tokens per Sec:      635, Lr: 0.000100\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 99.65ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 9722.00ex/s]\n",
      "2022-08-01 11:57:49,063 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=1000\n",
      "2022-08-01 11:57:49,063 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 12:00:13,749 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 12:00:13,749 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.20, loss:   4.59, ppl:  98.18, acc:   0.27, generation: 144.6175[sec], evaluation: 0.0443[sec]\n",
      "2022-08-01 12:00:13,750 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 12:00:14,237 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 12:00:14,240 - INFO - joeynmt.training - \tSource:     This is my friend Rachel. We went to high school together.\n",
      "2022-08-01 12:00:14,240 - INFO - joeynmt.training - \tReference:  Essa √© minha amiga Rachel, n√≥s fomos juntos ao col√©gio.\n",
      "2022-08-01 12:00:14,240 - INFO - joeynmt.training - \tHypothesis: O √© √© de a a a a a √© de a a a a a um um um.\n",
      "2022-08-01 12:00:14,240 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 12:00:14,242 - INFO - joeynmt.training - \tSource:     \"Yes, orange juice please,\" says Mike.\n",
      "2022-08-01 12:00:14,242 - INFO - joeynmt.training - \tReference:  \"Sim, suco de laranja, por favor\", diz Mike.\n",
      "2022-08-01 12:00:14,242 - INFO - joeynmt.training - \tHypothesis: O n√£o n√£o n√£o n√£o um um um uma uma uma uma uma uma uma uma uma uma uma uma uma uma.\n",
      "2022-08-01 12:00:14,242 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 12:00:14,244 - INFO - joeynmt.training - \tSource:     \"This is what I was looking for!\" he exclaimed.\n",
      "2022-08-01 12:00:14,244 - INFO - joeynmt.training - \tReference:  \"Era isso que eu estava procurando!\" Ele exclamou.\n",
      "2022-08-01 12:00:14,244 - INFO - joeynmt.training - \tHypothesis: Voc√™ n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o √©.\n",
      "2022-08-01 12:00:14,244 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 12:00:14,246 - INFO - joeynmt.training - \tSource:     I have to go shopping. I'll be back in an hour.\n",
      "2022-08-01 12:00:14,246 - INFO - joeynmt.training - \tReference:  Eu tenho que fazer compras. Voltarei em uma hora.\n",
      "2022-08-01 12:00:14,246 - INFO - joeynmt.training - \tHypothesis: Eu n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o n√£o.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 12:02:40,873 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     1.111693, Batch Acc: 0.002789, Tokens per Sec:      634, Lr: 0.000110\n",
      "2022-08-01 12:05:07,983 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     1.034755, Batch Acc: 0.003340, Tokens per Sec:      643, Lr: 0.000120\n",
      "2022-08-01 12:07:33,440 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     0.999859, Batch Acc: 0.003175, Tokens per Sec:      647, Lr: 0.000130\n",
      "2022-08-01 12:10:01,040 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     0.922331, Batch Acc: 0.003699, Tokens per Sec:      648, Lr: 0.000140\n",
      "2022-08-01 12:12:24,275 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     0.957557, Batch Acc: 0.003696, Tokens per Sec:      661, Lr: 0.000150\n",
      "2022-08-01 12:14:30,006 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     0.861257, Batch Acc: 0.004069, Tokens per Sec:      752, Lr: 0.000160\n",
      "2022-08-01 12:16:30,777 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     0.860447, Batch Acc: 0.004245, Tokens per Sec:      784, Lr: 0.000170\n",
      "2022-08-01 12:18:31,511 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     0.819823, Batch Acc: 0.004675, Tokens per Sec:      792, Lr: 0.000180\n",
      "2022-08-01 12:20:32,190 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     0.823252, Batch Acc: 0.004488, Tokens per Sec:      779, Lr: 0.000190\n",
      "2022-08-01 12:22:32,510 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     0.757391, Batch Acc: 0.004994, Tokens per Sec:      792, Lr: 0.000200\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 109.66ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 3105.21ex/s]\n",
      "2022-08-01 12:22:34,998 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=2000\n",
      "2022-08-01 12:22:34,998 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 12:25:25,754 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 12:25:25,762 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   2.80, loss:   3.43, ppl:  30.86, acc:   0.40, generation: 170.6555[sec], evaluation: 0.0817[sec]\n",
      "2022-08-01 12:25:25,763 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 12:25:26,613 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 12:25:26,615 - INFO - joeynmt.training - \tSource:     \"Haven't we met somewhere before?\" asked the student.\n",
      "2022-08-01 12:25:26,615 - INFO - joeynmt.training - \tReference:  \"J√° n√£o nos conhecemos de algum lugar?\" perguntou o estudante.\n",
      "2022-08-01 12:25:26,615 - INFO - joeynmt.training - \tHypothesis: \" \" \"Voc√™ n√£o se se deu em casa em dia.\n",
      "2022-08-01 12:25:26,615 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 12:25:26,616 - INFO - joeynmt.training - \tSource:     You are saying you intentionally hide your good looks?\n",
      "2022-08-01 12:25:26,616 - INFO - joeynmt.training - \tReference:  Voc√™ est√° dizendo que esconde sua beleza intencionalmente?\n",
      "2022-08-01 12:25:26,616 - INFO - joeynmt.training - \tHypothesis: Voc√™ √© que voc√™ est√° ser ser a sua seu pai?\n",
      "2022-08-01 12:25:26,616 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 12:25:26,617 - INFO - joeynmt.training - \tSource:     Did you miss me?\n",
      "2022-08-01 12:25:26,617 - INFO - joeynmt.training - \tReference:  A senhora sentiu saudades minhas?\n",
      "2022-08-01 12:25:26,617 - INFO - joeynmt.training - \tHypothesis: Voc√™ me me me me ajudar?\n",
      "2022-08-01 12:25:26,617 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 12:25:26,619 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-01 12:25:26,619 - INFO - joeynmt.training - \tReference:  O senhor vai me esquecer um dia.\n",
      "2022-08-01 12:25:26,619 - INFO - joeynmt.training - \tHypothesis: Voc√™ vai me me me ajudar um dia.\n",
      "2022-08-01 12:25:53,842 - INFO - joeynmt.training - Epoch   1: total training loss 2510.17\n",
      "2022-08-01 12:25:53,842 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-08-01 12:28:01,615 - INFO - joeynmt.training - Epoch   2, Step:     2100, Batch Loss:     0.752290, Batch Acc: 0.005754, Tokens per Sec:      619, Lr: 0.000195\n",
      "2022-08-01 12:30:29,010 - INFO - joeynmt.training - Epoch   2, Step:     2200, Batch Loss:     0.678026, Batch Acc: 0.005425, Tokens per Sec:      650, Lr: 0.000191\n",
      "2022-08-01 12:32:56,791 - INFO - joeynmt.training - Epoch   2, Step:     2300, Batch Loss:     0.652269, Batch Acc: 0.005166, Tokens per Sec:      650, Lr: 0.000187\n",
      "2022-08-01 12:35:26,049 - INFO - joeynmt.training - Epoch   2, Step:     2400, Batch Loss:     0.700165, Batch Acc: 0.005392, Tokens per Sec:      634, Lr: 0.000183\n",
      "2022-08-01 12:37:54,658 - INFO - joeynmt.training - Epoch   2, Step:     2500, Batch Loss:     0.615067, Batch Acc: 0.005429, Tokens per Sec:      632, Lr: 0.000179\n",
      "2022-08-01 12:40:24,232 - INFO - joeynmt.training - Epoch   2, Step:     2600, Batch Loss:     0.628798, Batch Acc: 0.005619, Tokens per Sec:      635, Lr: 0.000175\n",
      "2022-08-01 12:42:52,012 - INFO - joeynmt.training - Epoch   2, Step:     2700, Batch Loss:     0.564753, Batch Acc: 0.006325, Tokens per Sec:      646, Lr: 0.000172\n",
      "2022-08-01 12:45:21,612 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     0.554228, Batch Acc: 0.005951, Tokens per Sec:      635, Lr: 0.000169\n",
      "2022-08-01 12:47:46,883 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     0.513713, Batch Acc: 0.006445, Tokens per Sec:      652, Lr: 0.000166\n",
      "2022-08-01 12:49:46,881 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     0.606755, Batch Acc: 0.005547, Tokens per Sec:      783, Lr: 0.000163\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 178.72ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 9141.74ex/s]\n",
      "2022-08-01 12:49:47,795 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=3000\n",
      "2022-08-01 12:49:47,795 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 12:50:30,957 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 12:50:30,958 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.84, loss:   2.57, ppl:  13.10, acc:   0.54, generation: 43.1277[sec], evaluation: 0.0276[sec]\n",
      "2022-08-01 12:50:30,958 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 12:50:31,389 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 12:50:31,392 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-01 12:50:31,392 - INFO - joeynmt.training - \tReference:  As senhoras me olvidar√£o um dia.\n",
      "2022-08-01 12:50:31,392 - INFO - joeynmt.training - \tHypothesis: Voc√™ vai me se casar com um dia.\n",
      "2022-08-01 12:50:31,393 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 12:50:31,395 - INFO - joeynmt.training - \tSource:     Are they all the same?\n",
      "2022-08-01 12:50:31,395 - INFO - joeynmt.training - \tReference:  S√£o todos do mesmo tipo?\n",
      "2022-08-01 12:50:31,395 - INFO - joeynmt.training - \tHypothesis: Eles s√£o todos o melhor?\n",
      "2022-08-01 12:50:31,395 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 12:50:31,397 - INFO - joeynmt.training - \tSource:     The wind calmed down.\n",
      "2022-08-01 12:50:31,397 - INFO - joeynmt.training - \tReference:  O vento se acalmou.\n",
      "2022-08-01 12:50:31,397 - INFO - joeynmt.training - \tHypothesis: O jogo se tornou.\n",
      "2022-08-01 12:50:31,397 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 12:50:31,399 - INFO - joeynmt.training - \tSource:     How do you spell \"pretty\"?\n",
      "2022-08-01 12:50:31,400 - INFO - joeynmt.training - \tReference:  Como se escreve \"pretty\"?\n",
      "2022-08-01 12:50:31,400 - INFO - joeynmt.training - \tHypothesis: Como voc√™ colocou \"Eu?\n",
      "2022-08-01 12:52:32,667 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     0.496068, Batch Acc: 0.006994, Tokens per Sec:      774, Lr: 0.000161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 12:54:33,575 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     0.565820, Batch Acc: 0.005704, Tokens per Sec:      790, Lr: 0.000158\n",
      "2022-08-01 12:56:33,791 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     0.596018, Batch Acc: 0.005703, Tokens per Sec:      780, Lr: 0.000156\n",
      "2022-08-01 12:58:58,749 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     0.551572, Batch Acc: 0.005913, Tokens per Sec:      658, Lr: 0.000153\n",
      "2022-08-01 13:01:30,826 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     0.498475, Batch Acc: 0.006715, Tokens per Sec:      629, Lr: 0.000151\n",
      "2022-08-01 13:03:56,886 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     0.539954, Batch Acc: 0.006345, Tokens per Sec:      655, Lr: 0.000149\n",
      "2022-08-01 13:06:26,392 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     0.512146, Batch Acc: 0.005858, Tokens per Sec:      637, Lr: 0.000147\n",
      "2022-08-01 13:08:55,143 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     0.525955, Batch Acc: 0.005593, Tokens per Sec:      641, Lr: 0.000145\n",
      "2022-08-01 13:11:23,419 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     0.510723, Batch Acc: 0.005472, Tokens per Sec:      636, Lr: 0.000143\n",
      "2022-08-01 13:13:48,856 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     0.501172, Batch Acc: 0.005895, Tokens per Sec:      644, Lr: 0.000141\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 180.30ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 3204.98ex/s]\n",
      "2022-08-01 13:13:50,656 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=4000\n",
      "2022-08-01 13:13:50,663 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 13:14:36,940 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 13:14:36,941 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.68, loss:   2.36, ppl:  10.63, acc:   0.58, generation: 46.2130[sec], evaluation: 0.0573[sec]\n",
      "2022-08-01 13:14:36,941 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 13:14:37,432 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/1000.ckpt\n",
      "2022-08-01 13:14:37,450 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 13:14:37,452 - INFO - joeynmt.training - \tSource:     \"Why aren't you going?\" \"Because I don't want to.\"\n",
      "2022-08-01 13:14:37,452 - INFO - joeynmt.training - \tReference:  \"Por que voc√™ n√£o vai?\" \"Porque n√£o estou a fim de ir.\"\n",
      "2022-08-01 13:14:37,452 - INFO - joeynmt.training - \tHypothesis: \"Voc√™ n√£o vai?\" \"Eu n√£o quero.\"\n",
      "2022-08-01 13:14:37,452 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 13:14:37,454 - INFO - joeynmt.training - \tSource:     It almost scared me not to see you online for a whole day.\n",
      "2022-08-01 13:14:37,454 - INFO - joeynmt.training - \tReference:  Quase me assustou n√£o te ver online por um dia inteiro.\n",
      "2022-08-01 13:14:37,454 - INFO - joeynmt.training - \tHypothesis: N√≥s quase me parecia que n√£o te ver em um dia todo.\n",
      "2022-08-01 13:14:37,454 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 13:14:37,456 - INFO - joeynmt.training - \tSource:     Most people think I'm crazy.\n",
      "2022-08-01 13:14:37,456 - INFO - joeynmt.training - \tReference:  A maioria das pessoas acha que eu estou louco.\n",
      "2022-08-01 13:14:37,456 - INFO - joeynmt.training - \tHypothesis: A maioria das pessoas acha que eu sou louco.\n",
      "2022-08-01 13:14:37,456 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 13:14:37,458 - INFO - joeynmt.training - \tSource:     This is not important.\n",
      "2022-08-01 13:14:37,458 - INFO - joeynmt.training - \tReference:  Isto n√£o √© importante.\n",
      "2022-08-01 13:14:37,458 - INFO - joeynmt.training - \tHypothesis: Isto n√£o √© importante.\n",
      "2022-08-01 13:15:21,537 - INFO - joeynmt.training - Epoch   2: total training loss 1177.40\n",
      "2022-08-01 13:15:21,538 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-08-01 13:17:04,746 - INFO - joeynmt.training - Epoch   3, Step:     4100, Batch Loss:     0.425837, Batch Acc: 0.010182, Tokens per Sec:      631, Lr: 0.000140\n",
      "2022-08-01 13:19:30,690 - INFO - joeynmt.training - Epoch   3, Step:     4200, Batch Loss:     0.485703, Batch Acc: 0.007173, Tokens per Sec:      650, Lr: 0.000138\n",
      "2022-08-01 13:21:54,868 - INFO - joeynmt.training - Epoch   3, Step:     4300, Batch Loss:     0.454452, Batch Acc: 0.006619, Tokens per Sec:      662, Lr: 0.000136\n",
      "2022-08-01 13:23:56,030 - INFO - joeynmt.training - Epoch   3, Step:     4400, Batch Loss:     0.436905, Batch Acc: 0.006912, Tokens per Sec:      780, Lr: 0.000135\n",
      "2022-08-01 13:25:55,723 - INFO - joeynmt.training - Epoch   3, Step:     4500, Batch Loss:     0.451294, Batch Acc: 0.006747, Tokens per Sec:      794, Lr: 0.000133\n",
      "2022-08-01 13:27:55,483 - INFO - joeynmt.training - Epoch   3, Step:     4600, Batch Loss:     0.420311, Batch Acc: 0.007274, Tokens per Sec:      794, Lr: 0.000132\n",
      "2022-08-01 13:29:55,660 - INFO - joeynmt.training - Epoch   3, Step:     4700, Batch Loss:     0.539118, Batch Acc: 0.005301, Tokens per Sec:      791, Lr: 0.000130\n",
      "2022-08-01 13:32:02,544 - INFO - joeynmt.training - Epoch   3, Step:     4800, Batch Loss:     0.380321, Batch Acc: 0.007070, Tokens per Sec:      751, Lr: 0.000129\n",
      "2022-08-01 13:34:32,357 - INFO - joeynmt.training - Epoch   3, Step:     4900, Batch Loss:     0.449362, Batch Acc: 0.007065, Tokens per Sec:      638, Lr: 0.000128\n",
      "2022-08-01 13:37:46,101 - INFO - joeynmt.training - Epoch   3, Step:     5000, Batch Loss:     0.440939, Batch Acc: 0.006723, Tokens per Sec:      490, Lr: 0.000126\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 92.61ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 8379.42ex/s]\n",
      "2022-08-01 13:37:48,042 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=5000\n",
      "2022-08-01 13:37:48,043 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 13:38:52,595 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 13:38:52,595 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.36, loss:   2.22, ppl:   9.21, acc:   0.59, generation: 64.4706[sec], evaluation: 0.0719[sec]\n",
      "2022-08-01 13:38:52,596 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 13:38:53,919 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/2000.ckpt\n",
      "2022-08-01 13:38:53,965 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 13:38:53,968 - INFO - joeynmt.training - \tSource:     Is it far from here?\n",
      "2022-08-01 13:38:53,968 - INFO - joeynmt.training - \tReference:  Fica longe daqui?\n",
      "2022-08-01 13:38:53,968 - INFO - joeynmt.training - \tHypothesis: √â longe daqui?\n",
      "2022-08-01 13:38:53,968 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 13:38:53,970 - INFO - joeynmt.training - \tSource:     I learned to live without her.\n",
      "2022-08-01 13:38:53,970 - INFO - joeynmt.training - \tReference:  Eu aprendi a viver sem ela.\n",
      "2022-08-01 13:38:53,970 - INFO - joeynmt.training - \tHypothesis: Eu aprendi a viver sem ela.\n",
      "2022-08-01 13:38:53,970 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 13:38:53,972 - INFO - joeynmt.training - \tSource:     What other options do I have?\n",
      "2022-08-01 13:38:53,972 - INFO - joeynmt.training - \tReference:  Que outras op√ß√µes eu tenho?\n",
      "2022-08-01 13:38:53,973 - INFO - joeynmt.training - \tHypothesis: Que outro tempo eu tenho?\n",
      "2022-08-01 13:38:53,973 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 13:38:53,975 - INFO - joeynmt.training - \tSource:     I never liked biology.\n",
      "2022-08-01 13:38:53,975 - INFO - joeynmt.training - \tReference:  Nunca curti biologia.\n",
      "2022-08-01 13:38:53,975 - INFO - joeynmt.training - \tHypothesis: Eu nunca gostei de matem√°tica.\n",
      "2022-08-01 13:41:20,772 - INFO - joeynmt.training - Epoch   3, Step:     5100, Batch Loss:     0.445751, Batch Acc: 0.005971, Tokens per Sec:      619, Lr: 0.000125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 13:43:42,894 - INFO - joeynmt.training - Epoch   3, Step:     5200, Batch Loss:     0.353230, Batch Acc: 0.006974, Tokens per Sec:      671, Lr: 0.000124\n",
      "2022-08-01 13:46:07,872 - INFO - joeynmt.training - Epoch   3, Step:     5300, Batch Loss:     0.452040, Batch Acc: 0.005981, Tokens per Sec:      655, Lr: 0.000123\n",
      "2022-08-01 13:48:34,816 - INFO - joeynmt.training - Epoch   3, Step:     5400, Batch Loss:     0.344018, Batch Acc: 0.008686, Tokens per Sec:      640, Lr: 0.000122\n",
      "2022-08-01 13:50:58,321 - INFO - joeynmt.training - Epoch   3, Step:     5500, Batch Loss:     0.396004, Batch Acc: 0.006378, Tokens per Sec:      659, Lr: 0.000121\n",
      "2022-08-01 13:53:25,711 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     0.418149, Batch Acc: 0.006702, Tokens per Sec:      644, Lr: 0.000120\n",
      "2022-08-01 13:55:49,337 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     0.402390, Batch Acc: 0.007443, Tokens per Sec:      658, Lr: 0.000118\n",
      "2022-08-01 13:57:49,453 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     0.376988, Batch Acc: 0.007991, Tokens per Sec:      787, Lr: 0.000117\n",
      "2022-08-01 13:59:24,222 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     0.354388, Batch Acc: 0.006372, Tokens per Sec:      994, Lr: 0.000116\n",
      "2022-08-01 14:01:06,678 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     0.398298, Batch Acc: 0.006723, Tokens per Sec:      928, Lr: 0.000115\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 184.20ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 17298.31ex/s]\n",
      "2022-08-01 14:01:07,310 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=6000\n",
      "2022-08-01 14:01:07,310 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 14:01:48,061 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 14:01:48,063 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  23.48, loss:   2.08, ppl:   7.99, acc:   0.60, generation: 40.7064[sec], evaluation: 0.0391[sec]\n",
      "2022-08-01 14:01:48,064 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 14:01:48,539 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/3000.ckpt\n",
      "2022-08-01 14:01:48,549 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 14:01:48,551 - INFO - joeynmt.training - \tSource:     I don't know what you mean.\n",
      "2022-08-01 14:01:48,551 - INFO - joeynmt.training - \tReference:  N√£o sei o que o senhor est√° querendo dizer.\n",
      "2022-08-01 14:01:48,551 - INFO - joeynmt.training - \tHypothesis: N√£o sei o que voc√™ quer dizer.\n",
      "2022-08-01 14:01:48,551 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 14:01:48,553 - INFO - joeynmt.training - \tSource:     When I grow up, I want to be a king.\n",
      "2022-08-01 14:01:48,553 - INFO - joeynmt.training - \tReference:  Quando eu crescer eu quero ser rei.\n",
      "2022-08-01 14:01:48,553 - INFO - joeynmt.training - \tHypothesis: Quando eu vou me dar um rei.\n",
      "2022-08-01 14:01:48,553 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 14:01:48,554 - INFO - joeynmt.training - \tSource:     I hate those spiders. They're always there to freak me out when I'm cleaning.\n",
      "2022-08-01 14:01:48,554 - INFO - joeynmt.training - \tReference:  Odeio aquelas aranhas. Elas est√£o sempre l√° para me assustarem quando estou limpando.\n",
      "2022-08-01 14:01:48,554 - INFO - joeynmt.training - \tHypothesis: Eu odeio essas aranhas. Eles sempre est√£o em l√° em mim quando eu estou me limpar.\n",
      "2022-08-01 14:01:48,554 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 14:01:48,555 - INFO - joeynmt.training - \tSource:     What do you want?\n",
      "2022-08-01 14:01:48,556 - INFO - joeynmt.training - \tReference:  Que √© que deseja?\n",
      "2022-08-01 14:01:48,556 - INFO - joeynmt.training - \tHypothesis: O que voc√™ quer?\n",
      "2022-08-01 14:02:34,346 - INFO - joeynmt.training - Epoch   3: total training loss 855.38\n",
      "2022-08-01 14:02:34,346 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-08-01 14:04:15,702 - INFO - joeynmt.training - Epoch   4, Step:     6100, Batch Loss:     0.320667, Batch Acc: 0.014432, Tokens per Sec:      494, Lr: 0.000115\n",
      "2022-08-01 14:11:40,641 - INFO - joeynmt.training - Epoch   4, Step:     6200, Batch Loss:     0.383267, Batch Acc: 0.007079, Tokens per Sec:      212, Lr: 0.000114\n",
      "2022-08-01 14:16:50,403 - INFO - joeynmt.training - Epoch   4, Step:     6300, Batch Loss:     0.416928, Batch Acc: 0.007233, Tokens per Sec:      296, Lr: 0.000113\n",
      "2022-08-01 14:18:30,347 - INFO - joeynmt.training - Epoch   4, Step:     6400, Batch Loss:     0.385368, Batch Acc: 0.006504, Tokens per Sec:      958, Lr: 0.000112\n",
      "2022-08-01 14:20:09,647 - INFO - joeynmt.training - Epoch   4, Step:     6500, Batch Loss:     0.351653, Batch Acc: 0.006634, Tokens per Sec:      964, Lr: 0.000111\n",
      "2022-08-01 14:21:46,132 - INFO - joeynmt.training - Epoch   4, Step:     6600, Batch Loss:     0.384987, Batch Acc: 0.006529, Tokens per Sec:      972, Lr: 0.000110\n",
      "2022-08-01 14:23:22,436 - INFO - joeynmt.training - Epoch   4, Step:     6700, Batch Loss:     0.301329, Batch Acc: 0.008073, Tokens per Sec:      989, Lr: 0.000109\n",
      "2022-08-01 14:24:58,803 - INFO - joeynmt.training - Epoch   4, Step:     6800, Batch Loss:     0.339708, Batch Acc: 0.007086, Tokens per Sec:      994, Lr: 0.000108\n",
      "2022-08-01 14:26:35,790 - INFO - joeynmt.training - Epoch   4, Step:     6900, Batch Loss:     0.353832, Batch Acc: 0.007396, Tokens per Sec:      962, Lr: 0.000108\n",
      "2022-08-01 14:28:11,639 - INFO - joeynmt.training - Epoch   4, Step:     7000, Batch Loss:     0.323537, Batch Acc: 0.008309, Tokens per Sec:      993, Lr: 0.000107\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 188.41ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 17393.72ex/s]\n",
      "2022-08-01 14:28:12,266 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=7000\n",
      "2022-08-01 14:28:12,267 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 14:28:37,010 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 14:28:37,014 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  29.66, loss:   1.93, ppl:   6.89, acc:   0.63, generation: 24.7141[sec], evaluation: 0.0276[sec]\n",
      "2022-08-01 14:28:37,015 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 14:28:37,403 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/4000.ckpt\n",
      "2022-08-01 14:28:37,413 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 14:28:37,415 - INFO - joeynmt.training - \tSource:     Why do you ask?\n",
      "2022-08-01 14:28:37,415 - INFO - joeynmt.training - \tReference:  Por que voc√™s perguntam?\n",
      "2022-08-01 14:28:37,415 - INFO - joeynmt.training - \tHypothesis: Por que voc√™ pediu?\n",
      "2022-08-01 14:28:37,415 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 14:28:37,416 - INFO - joeynmt.training - \tSource:     When I ask people what they regret most about high school, they nearly all say the same thing: that they wasted so much time.\n",
      "2022-08-01 14:28:37,416 - INFO - joeynmt.training - \tReference:  Quando pergunto √†s pessoas o que elas mais lamentam sobre a escola secund√°ria, quase todos dizem a mesma coisa: que perderam muito tempo.\n",
      "2022-08-01 14:28:37,416 - INFO - joeynmt.training - \tHypothesis: Quando eu pedir pessoas o que se arrepender de mais no ensino m√©dio, eles quase quase todos eles dizem que todos os tempo.\n",
      "2022-08-01 14:28:37,416 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 14:28:37,417 - INFO - joeynmt.training - \tSource:     Why don't you come visit us?\n",
      "2022-08-01 14:28:37,418 - INFO - joeynmt.training - \tReference:  Por que n√£o vem nos visitar?\n",
      "2022-08-01 14:28:37,418 - INFO - joeynmt.training - \tHypothesis: Por que voc√™ n√£o vem a gente?\n",
      "2022-08-01 14:28:37,418 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 14:28:37,419 - INFO - joeynmt.training - \tSource:     I'd like to stay for one night.\n",
      "2022-08-01 14:28:37,419 - INFO - joeynmt.training - \tReference:  Eu gostaria de ficar por uma noite.\n",
      "2022-08-01 14:28:37,419 - INFO - joeynmt.training - \tHypothesis: Eu gostaria de ficar por uma noite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 14:30:12,787 - INFO - joeynmt.training - Epoch   4, Step:     7100, Batch Loss:     0.332396, Batch Acc: 0.005975, Tokens per Sec:      977, Lr: 0.000106\n",
      "2022-08-01 14:31:48,179 - INFO - joeynmt.training - Epoch   4, Step:     7200, Batch Loss:     0.367908, Batch Acc: 0.007055, Tokens per Sec:      982, Lr: 0.000105\n",
      "2022-08-01 14:33:24,281 - INFO - joeynmt.training - Epoch   4, Step:     7300, Batch Loss:     0.303412, Batch Acc: 0.008551, Tokens per Sec:      985, Lr: 0.000105\n",
      "2022-08-01 14:37:54,865 - INFO - joeynmt.training - Epoch   4, Step:     7400, Batch Loss:     0.366174, Batch Acc: 0.007413, Tokens per Sec:      352, Lr: 0.000104\n",
      "2022-08-01 14:39:31,064 - INFO - joeynmt.training - Epoch   4, Step:     7500, Batch Loss:     0.330252, Batch Acc: 0.007389, Tokens per Sec:      985, Lr: 0.000103\n",
      "2022-08-01 14:41:05,626 - INFO - joeynmt.training - Epoch   4, Step:     7600, Batch Loss:     0.381350, Batch Acc: 0.006794, Tokens per Sec:      995, Lr: 0.000103\n",
      "2022-08-01 14:42:40,428 - INFO - joeynmt.training - Epoch   4, Step:     7700, Batch Loss:     0.347142, Batch Acc: 0.005842, Tokens per Sec:      993, Lr: 0.000102\n",
      "2022-08-01 14:44:16,472 - INFO - joeynmt.training - Epoch   4, Step:     7800, Batch Loss:     0.313382, Batch Acc: 0.007624, Tokens per Sec:      993, Lr: 0.000101\n",
      "2022-08-01 14:45:51,827 - INFO - joeynmt.training - Epoch   4, Step:     7900, Batch Loss:     0.334884, Batch Acc: 0.006286, Tokens per Sec:     1001, Lr: 0.000101\n",
      "2022-08-01 14:47:26,855 - INFO - joeynmt.training - Epoch   4, Step:     8000, Batch Loss:     0.329835, Batch Acc: 0.007788, Tokens per Sec:     1009, Lr: 0.000100\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 189.70ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 17252.13ex/s]\n",
      "2022-08-01 14:47:27,484 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=8000\n",
      "2022-08-01 14:47:27,484 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 14:48:02,901 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 14:48:02,901 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  30.88, loss:   1.85, ppl:   6.36, acc:   0.64, generation: 35.3484[sec], evaluation: 0.0616[sec]\n",
      "2022-08-01 14:48:02,902 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 14:48:03,297 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/5000.ckpt\n",
      "2022-08-01 14:48:03,308 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 14:48:03,309 - INFO - joeynmt.training - \tSource:     Then there is a problem...\n",
      "2022-08-01 14:48:03,310 - INFO - joeynmt.training - \tReference:  Ent√£o h√° um problema...\n",
      "2022-08-01 14:48:03,310 - INFO - joeynmt.training - \tHypothesis: Ent√£o h√° um problema.\n",
      "2022-08-01 14:48:03,310 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 14:48:03,311 - INFO - joeynmt.training - \tSource:     Whenever I find something I like, it's too expensive.\n",
      "2022-08-01 14:48:03,311 - INFO - joeynmt.training - \tReference:  Sempre quando eu acho algo que me agrade, este algo √© caro demais.\n",
      "2022-08-01 14:48:03,311 - INFO - joeynmt.training - \tHypothesis: Sempre que eu encontrei algo que gosto, √© caro demais.\n",
      "2022-08-01 14:48:03,311 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 14:48:03,312 - INFO - joeynmt.training - \tSource:     Don't expect others to think for you!\n",
      "2022-08-01 14:48:03,312 - INFO - joeynmt.training - \tReference:  N√£o espere que as outras pessoas pensem por voc√™.\n",
      "2022-08-01 14:48:03,312 - INFO - joeynmt.training - \tHypothesis: N√£o espere os outros que penses para voc√™!\n",
      "2022-08-01 14:48:03,312 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 14:48:03,313 - INFO - joeynmt.training - \tSource:     I miss you.\n",
      "2022-08-01 14:48:03,313 - INFO - joeynmt.training - \tReference:  Tenho saudades tuas.\n",
      "2022-08-01 14:48:03,313 - INFO - joeynmt.training - \tHypothesis: Sinto saudades de voc√™.\n",
      "2022-08-01 14:49:07,852 - INFO - joeynmt.training - Epoch   4: total training loss 717.44\n",
      "2022-08-01 14:49:07,853 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-08-01 14:49:39,611 - INFO - joeynmt.training - Epoch   5, Step:     8100, Batch Loss:     0.402492, Batch Acc: 0.019241, Tokens per Sec:      985, Lr: 0.000099\n",
      "2022-08-01 14:51:15,731 - INFO - joeynmt.training - Epoch   5, Step:     8200, Batch Loss:     0.288994, Batch Acc: 0.008187, Tokens per Sec:      979, Lr: 0.000099\n",
      "2022-08-01 14:52:50,269 - INFO - joeynmt.training - Epoch   5, Step:     8300, Batch Loss:     0.300038, Batch Acc: 0.007602, Tokens per Sec:      993, Lr: 0.000098\n",
      "2022-08-01 14:54:26,327 - INFO - joeynmt.training - Epoch   5, Step:     8400, Batch Loss:     0.305980, Batch Acc: 0.007486, Tokens per Sec:      993, Lr: 0.000098\n",
      "2022-08-01 14:56:00,896 - INFO - joeynmt.training - Epoch   5, Step:     8500, Batch Loss:     0.395077, Batch Acc: 0.007641, Tokens per Sec:      996, Lr: 0.000097\n",
      "2022-08-01 14:57:37,113 - INFO - joeynmt.training - Epoch   5, Step:     8600, Batch Loss:     0.313633, Batch Acc: 0.007045, Tokens per Sec:      994, Lr: 0.000096\n",
      "2022-08-01 14:59:11,990 - INFO - joeynmt.training - Epoch   5, Step:     8700, Batch Loss:     0.305919, Batch Acc: 0.007797, Tokens per Sec:      995, Lr: 0.000096\n",
      "2022-08-01 15:00:47,447 - INFO - joeynmt.training - Epoch   5, Step:     8800, Batch Loss:     0.313894, Batch Acc: 0.007611, Tokens per Sec:      991, Lr: 0.000095\n",
      "2022-08-01 15:02:24,504 - INFO - joeynmt.training - Epoch   5, Step:     8900, Batch Loss:     0.356923, Batch Acc: 0.006609, Tokens per Sec:      978, Lr: 0.000095\n",
      "2022-08-01 15:04:00,453 - INFO - joeynmt.training - Epoch   5, Step:     9000, Batch Loss:     0.286514, Batch Acc: 0.007314, Tokens per Sec:      986, Lr: 0.000094\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 194.44ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 17335.99ex/s]\n",
      "2022-08-01 15:04:01,076 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=9000\n",
      "2022-08-01 15:04:01,076 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 15:04:24,163 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 15:04:24,174 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  30.74, loss:   1.83, ppl:   6.25, acc:   0.65, generation: 23.0580[sec], evaluation: 0.0336[sec]\n",
      "2022-08-01 15:04:24,548 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/6000.ckpt\n",
      "2022-08-01 15:04:24,559 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 15:04:24,561 - INFO - joeynmt.training - \tSource:     This is my friend Rachel. We went to high school together.\n",
      "2022-08-01 15:04:24,561 - INFO - joeynmt.training - \tReference:  Essa √© minha amiga Rachel, n√≥s fomos juntos ao col√©gio.\n",
      "2022-08-01 15:04:24,561 - INFO - joeynmt.training - \tHypothesis: Este √© o meu amigo de Jesus. N√≥s fomos √† escola de ensino juntos.\n",
      "2022-08-01 15:04:24,561 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 15:04:24,562 - INFO - joeynmt.training - \tSource:     Where are the showers?\n",
      "2022-08-01 15:04:24,562 - INFO - joeynmt.training - \tReference:  Onde est√£o as duchas ?\n",
      "2022-08-01 15:04:24,562 - INFO - joeynmt.training - \tHypothesis: Onde est√£o os banho?\n",
      "2022-08-01 15:04:24,562 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 15:04:24,563 - INFO - joeynmt.training - \tSource:     There's a problem there that you don't see.\n",
      "2022-08-01 15:04:24,564 - INFO - joeynmt.training - \tReference:  A√≠ h√° um problema que voc√™ n√£o est√° vendo.\n",
      "2022-08-01 15:04:24,564 - INFO - joeynmt.training - \tHypothesis: H√° um problema que voc√™ n√£o est√° vendo.\n",
      "2022-08-01 15:04:24,564 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 15:04:24,565 - INFO - joeynmt.training - \tSource:     I thought you liked to learn new things.\n",
      "2022-08-01 15:04:24,565 - INFO - joeynmt.training - \tReference:  Eu pensei que o senhor gostasse de aprender coisas novas.\n",
      "2022-08-01 15:04:24,565 - INFO - joeynmt.training - \tHypothesis: Eu pensei que voc√™ gostava de aprender coisas novas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 15:05:59,921 - INFO - joeynmt.training - Epoch   5, Step:     9100, Batch Loss:     0.321830, Batch Acc: 0.007476, Tokens per Sec:      991, Lr: 0.000094\n",
      "2022-08-01 15:07:35,293 - INFO - joeynmt.training - Epoch   5, Step:     9200, Batch Loss:     0.280348, Batch Acc: 0.008095, Tokens per Sec:      975, Lr: 0.000093\n",
      "2022-08-01 15:09:12,679 - INFO - joeynmt.training - Epoch   5, Step:     9300, Batch Loss:     0.266997, Batch Acc: 0.007042, Tokens per Sec:      978, Lr: 0.000093\n",
      "2022-08-01 15:10:48,266 - INFO - joeynmt.training - Epoch   5, Step:     9400, Batch Loss:     0.305801, Batch Acc: 0.006992, Tokens per Sec:      988, Lr: 0.000092\n",
      "2022-08-01 15:12:23,063 - INFO - joeynmt.training - Epoch   5, Step:     9500, Batch Loss:     0.283467, Batch Acc: 0.008758, Tokens per Sec:     1005, Lr: 0.000092\n",
      "2022-08-01 15:13:58,635 - INFO - joeynmt.training - Epoch   5, Step:     9600, Batch Loss:     0.320564, Batch Acc: 0.007887, Tokens per Sec:      987, Lr: 0.000091\n",
      "2022-08-01 15:15:35,136 - INFO - joeynmt.training - Epoch   5, Step:     9700, Batch Loss:     0.248521, Batch Acc: 0.008373, Tokens per Sec:      991, Lr: 0.000091\n",
      "2022-08-01 15:17:10,501 - INFO - joeynmt.training - Epoch   5, Step:     9800, Batch Loss:     0.343227, Batch Acc: 0.006505, Tokens per Sec:      983, Lr: 0.000090\n",
      "2022-08-01 15:18:45,006 - INFO - joeynmt.training - Epoch   5, Step:     9900, Batch Loss:     0.276096, Batch Acc: 0.007857, Tokens per Sec:     1007, Lr: 0.000090\n",
      "2022-08-01 15:20:21,491 - INFO - joeynmt.training - Epoch   5, Step:    10000, Batch Loss:     0.308125, Batch Acc: 0.007065, Tokens per Sec:      999, Lr: 0.000089\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 186.41ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 17212.77ex/s]\n",
      "2022-08-01 15:20:22,134 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=10000\n",
      "2022-08-01 15:20:22,134 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 15:20:56,697 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 15:20:56,697 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  33.11, loss:   1.71, ppl:   5.53, acc:   0.66, generation: 34.4645[sec], evaluation: 0.0739[sec]\n",
      "2022-08-01 15:20:56,697 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 15:20:57,028 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/7000.ckpt\n",
      "2022-08-01 15:20:57,039 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 15:20:57,041 - INFO - joeynmt.training - \tSource:     Sometimes he can be a strange guy.\n",
      "2022-08-01 15:20:57,041 - INFO - joeynmt.training - \tReference:  √Äs vezes ele pode ser um tipo estranho.\n",
      "2022-08-01 15:20:57,041 - INFO - joeynmt.training - \tHypothesis: √Äs vezes ele pode ser um cara estranho.\n",
      "2022-08-01 15:20:57,041 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 15:20:57,042 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-01 15:20:57,042 - INFO - joeynmt.training - \tReference:  V√≥s me olvidareis um dia.\n",
      "2022-08-01 15:20:57,042 - INFO - joeynmt.training - \tHypothesis: Voc√™ vai esquecer de mim um dia.\n",
      "2022-08-01 15:20:57,042 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 15:20:57,043 - INFO - joeynmt.training - \tSource:     Would you like something to drink?\n",
      "2022-08-01 15:20:57,043 - INFO - joeynmt.training - \tReference:  Voc√™ gostaria de algo para beber?\n",
      "2022-08-01 15:20:57,043 - INFO - joeynmt.training - \tHypothesis: Voc√™ gostaria de alguma coisa para beber?\n",
      "2022-08-01 15:20:57,043 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 15:20:57,044 - INFO - joeynmt.training - \tSource:     I have a headache.\n",
      "2022-08-01 15:20:57,045 - INFO - joeynmt.training - \tReference:  Eu estou com dor de cabe√ßa.\n",
      "2022-08-01 15:20:57,045 - INFO - joeynmt.training - \tHypothesis: Estou com dor de cabe√ßa.\n",
      "2022-08-01 15:22:17,492 - INFO - joeynmt.training - Epoch   5: total training loss 633.43\n",
      "2022-08-01 15:22:17,492 - INFO - joeynmt.training - EPOCH 6\n",
      "2022-08-01 15:22:33,484 - INFO - joeynmt.training - Epoch   6, Step:    10100, Batch Loss:     0.285349, Batch Acc: 0.044590, Tokens per Sec:      990, Lr: 0.000089\n",
      "2022-08-01 15:24:09,350 - INFO - joeynmt.training - Epoch   6, Step:    10200, Batch Loss:     0.315366, Batch Acc: 0.005732, Tokens per Sec:      995, Lr: 0.000089\n",
      "2022-08-01 15:25:47,609 - INFO - joeynmt.training - Epoch   6, Step:    10300, Batch Loss:     0.255507, Batch Acc: 0.007567, Tokens per Sec:      966, Lr: 0.000088\n",
      "2022-08-01 15:27:23,749 - INFO - joeynmt.training - Epoch   6, Step:    10400, Batch Loss:     0.258162, Batch Acc: 0.007974, Tokens per Sec:      986, Lr: 0.000088\n",
      "2022-08-01 15:28:58,470 - INFO - joeynmt.training - Epoch   6, Step:    10500, Batch Loss:     0.275090, Batch Acc: 0.007998, Tokens per Sec:     1007, Lr: 0.000087\n",
      "2022-08-01 15:30:33,930 - INFO - joeynmt.training - Epoch   6, Step:    10600, Batch Loss:     0.246907, Batch Acc: 0.007879, Tokens per Sec:      992, Lr: 0.000087\n",
      "2022-08-01 15:32:10,472 - INFO - joeynmt.training - Epoch   6, Step:    10700, Batch Loss:     0.269028, Batch Acc: 0.008094, Tokens per Sec:      993, Lr: 0.000086\n",
      "2022-08-01 15:33:45,609 - INFO - joeynmt.training - Epoch   6, Step:    10800, Batch Loss:     0.270683, Batch Acc: 0.008232, Tokens per Sec:      997, Lr: 0.000086\n",
      "2022-08-01 15:35:22,513 - INFO - joeynmt.training - Epoch   6, Step:    10900, Batch Loss:     0.271776, Batch Acc: 0.007270, Tokens per Sec:      977, Lr: 0.000086\n",
      "2022-08-01 15:36:58,336 - INFO - joeynmt.training - Epoch   6, Step:    11000, Batch Loss:     0.280497, Batch Acc: 0.008832, Tokens per Sec:      990, Lr: 0.000085\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 170.02ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 15197.41ex/s]\n",
      "2022-08-01 15:36:59,009 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=11000\n",
      "2022-08-01 15:36:59,009 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 15:37:20,973 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 15:37:20,976 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  31.67, loss:   1.61, ppl:   5.02, acc:   0.67, generation: 21.9332[sec], evaluation: 0.0271[sec]\n",
      "2022-08-01 15:37:21,366 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/9000.ckpt\n",
      "2022-08-01 15:37:21,377 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 15:37:21,378 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-01 15:37:21,378 - INFO - joeynmt.training - \tReference:  Voc√™ vai me esquecer um dia.\n",
      "2022-08-01 15:37:21,378 - INFO - joeynmt.training - \tHypothesis: Voc√™ vai esquecer de mim um dia.\n",
      "2022-08-01 15:37:21,378 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 15:37:21,379 - INFO - joeynmt.training - \tSource:     Why don't you come visit us?\n",
      "2022-08-01 15:37:21,379 - INFO - joeynmt.training - \tReference:  Por que voc√™ n√£o nos faz uma visita?\n",
      "2022-08-01 15:37:21,380 - INFO - joeynmt.training - \tHypothesis: Por que voc√™ n√£o vem nos visitar?\n",
      "2022-08-01 15:37:21,380 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 15:37:21,381 - INFO - joeynmt.training - \tSource:     You are in my way.\n",
      "2022-08-01 15:37:21,381 - INFO - joeynmt.training - \tReference:  A senhora est√° a impedir-me a passagem.\n",
      "2022-08-01 15:37:21,381 - INFO - joeynmt.training - \tHypothesis: Voc√™ est√° no meu caminho.\n",
      "2022-08-01 15:37:21,381 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 15:37:21,382 - INFO - joeynmt.training - \tSource:     I may give up soon and just nap instead.\n",
      "2022-08-01 15:37:21,382 - INFO - joeynmt.training - \tReference:  Pode ser que eu desista em breve e, em vez disso, tire uma soneca.\n",
      "2022-08-01 15:37:21,382 - INFO - joeynmt.training - \tHypothesis: Eu posso desistir de breve e apenas de soneca em vez.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 15:38:58,828 - INFO - joeynmt.training - Epoch   6, Step:    11100, Batch Loss:     0.274397, Batch Acc: 0.007073, Tokens per Sec:      967, Lr: 0.000085\n",
      "2022-08-01 15:40:34,746 - INFO - joeynmt.training - Epoch   6, Step:    11200, Batch Loss:     0.293030, Batch Acc: 0.006856, Tokens per Sec:      995, Lr: 0.000085\n",
      "2022-08-01 15:42:13,376 - INFO - joeynmt.training - Epoch   6, Step:    11300, Batch Loss:     0.260951, Batch Acc: 0.006553, Tokens per Sec:      966, Lr: 0.000084\n",
      "2022-08-01 15:43:52,027 - INFO - joeynmt.training - Epoch   6, Step:    11400, Batch Loss:     0.281529, Batch Acc: 0.008095, Tokens per Sec:      960, Lr: 0.000084\n",
      "2022-08-01 15:45:27,945 - INFO - joeynmt.training - Epoch   6, Step:    11500, Batch Loss:     0.330304, Batch Acc: 0.007825, Tokens per Sec:      987, Lr: 0.000083\n",
      "2022-08-01 15:47:03,477 - INFO - joeynmt.training - Epoch   6, Step:    11600, Batch Loss:     0.300006, Batch Acc: 0.007041, Tokens per Sec:      986, Lr: 0.000083\n",
      "2022-08-01 15:48:38,338 - INFO - joeynmt.training - Epoch   6, Step:    11700, Batch Loss:     0.277751, Batch Acc: 0.008237, Tokens per Sec:      994, Lr: 0.000083\n",
      "2022-08-01 15:50:16,610 - INFO - joeynmt.training - Epoch   6, Step:    11800, Batch Loss:     0.262483, Batch Acc: 0.006986, Tokens per Sec:      947, Lr: 0.000082\n",
      "2022-08-01 15:51:54,934 - INFO - joeynmt.training - Epoch   6, Step:    11900, Batch Loss:     0.255144, Batch Acc: 0.008032, Tokens per Sec:      962, Lr: 0.000082\n",
      "2022-08-01 15:53:33,397 - INFO - joeynmt.training - Epoch   6, Step:    12000, Batch Loss:     0.300256, Batch Acc: 0.007593, Tokens per Sec:      960, Lr: 0.000082\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 181.74ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 16381.63ex/s]\n",
      "2022-08-01 15:53:34,059 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=12000\n",
      "2022-08-01 15:53:34,059 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 15:54:06,464 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 15:54:06,465 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.06, loss:   1.74, ppl:   5.67, acc:   0.66, generation: 32.3728[sec], evaluation: 0.0253[sec]\n",
      "2022-08-01 15:54:06,465 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-01 15:54:06,853 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/model/8000.ckpt\n",
      "2022-08-01 15:54:06,864 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 15:54:06,866 - INFO - joeynmt.training - \tSource:     But the universe is infinite.\n",
      "2022-08-01 15:54:06,866 - INFO - joeynmt.training - \tReference:  Mas o Universo √© infinito.\n",
      "2022-08-01 15:54:06,866 - INFO - joeynmt.training - \tHypothesis: Mas o universo √© natural.\n",
      "2022-08-01 15:54:06,866 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 15:54:06,867 - INFO - joeynmt.training - \tSource:     It's a pity when somebody dies.\n",
      "2022-08-01 15:54:06,867 - INFO - joeynmt.training - \tReference:  √â uma pena quando algu√©m morre.\n",
      "2022-08-01 15:54:06,867 - INFO - joeynmt.training - \tHypothesis: √â uma pena quando algu√©m morrer.\n",
      "2022-08-01 15:54:06,867 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 15:54:06,868 - INFO - joeynmt.training - \tSource:     What do you want?\n",
      "2022-08-01 15:54:06,868 - INFO - joeynmt.training - \tReference:  Que deseja o senhor?\n",
      "2022-08-01 15:54:06,868 - INFO - joeynmt.training - \tHypothesis: O que voc√™ quer?\n",
      "2022-08-01 15:54:06,868 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 15:54:06,869 - INFO - joeynmt.training - \tSource:     I thought you liked to learn new things.\n",
      "2022-08-01 15:54:06,869 - INFO - joeynmt.training - \tReference:  Eu pensava que tu gostavas de aprender coisas novas.\n",
      "2022-08-01 15:54:06,869 - INFO - joeynmt.training - \tHypothesis: Pensei que voc√™ gostava de aprender coisas novas.\n",
      "2022-08-01 15:55:43,227 - INFO - joeynmt.training - Epoch   6: total training loss 576.04\n",
      "2022-08-01 15:55:43,227 - INFO - joeynmt.training - EPOCH 7\n",
      "2022-08-01 15:55:45,114 - INFO - joeynmt.training - Epoch   7, Step:    12100, Batch Loss:     0.263732, Batch Acc: 0.347597, Tokens per Sec:      905, Lr: 0.000081\n",
      "2022-08-01 15:57:24,215 - INFO - joeynmt.training - Epoch   7, Step:    12200, Batch Loss:     0.257596, Batch Acc: 0.008541, Tokens per Sec:      956, Lr: 0.000081\n",
      "2022-08-01 15:59:02,518 - INFO - joeynmt.training - Epoch   7, Step:    12300, Batch Loss:     0.279305, Batch Acc: 0.007509, Tokens per Sec:      971, Lr: 0.000081\n",
      "2022-08-01 16:00:40,578 - INFO - joeynmt.training - Epoch   7, Step:    12400, Batch Loss:     0.260886, Batch Acc: 0.007225, Tokens per Sec:      965, Lr: 0.000080\n",
      "2022-08-01 16:02:18,909 - INFO - joeynmt.training - Epoch   7, Step:    12500, Batch Loss:     0.231328, Batch Acc: 0.007892, Tokens per Sec:      968, Lr: 0.000080\n",
      "2022-08-01 16:03:57,015 - INFO - joeynmt.training - Epoch   7, Step:    12600, Batch Loss:     0.248601, Batch Acc: 0.008583, Tokens per Sec:      967, Lr: 0.000080\n",
      "2022-08-01 16:05:32,524 - INFO - joeynmt.training - Epoch   7, Step:    12700, Batch Loss:     0.247403, Batch Acc: 0.006766, Tokens per Sec:      984, Lr: 0.000079\n",
      "2022-08-01 16:07:08,605 - INFO - joeynmt.training - Epoch   7, Step:    12800, Batch Loss:     0.249114, Batch Acc: 0.007148, Tokens per Sec:      995, Lr: 0.000079\n",
      "2022-08-01 16:08:44,951 - INFO - joeynmt.training - Epoch   7, Step:    12900, Batch Loss:     0.253116, Batch Acc: 0.007833, Tokens per Sec:      995, Lr: 0.000079\n",
      "2022-08-01 16:10:21,450 - INFO - joeynmt.training - Epoch   7, Step:    13000, Batch Loss:     0.227871, Batch Acc: 0.009140, Tokens per Sec:      987, Lr: 0.000078\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 190.34ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 17065.56ex/s]\n",
      "2022-08-01 16:10:22,074 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=13000\n",
      "2022-08-01 16:10:22,074 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-01 16:10:55,447 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-01 16:10:55,451 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  31.01, loss:   1.76, ppl:   5.82, acc:   0.65, generation: 33.3437[sec], evaluation: 0.0266[sec]\n",
      "2022-08-01 16:10:55,452 - INFO - joeynmt.training - Example #0\n",
      "2022-08-01 16:10:55,455 - INFO - joeynmt.training - \tSource:     Rye was called the grain of poverty.\n",
      "2022-08-01 16:10:55,457 - INFO - joeynmt.training - \tReference:  O centeio foi chamado de gr√£o da pobreza.\n",
      "2022-08-01 16:10:55,458 - INFO - joeynmt.training - \tHypothesis: Carlos foi chamado dos problemas da pobreza.\n",
      "2022-08-01 16:10:55,458 - INFO - joeynmt.training - Example #1\n",
      "2022-08-01 16:10:55,460 - INFO - joeynmt.training - \tSource:     You're so impatient with me.\n",
      "2022-08-01 16:10:55,464 - INFO - joeynmt.training - \tReference:  Voc√™ √© t√£o impaciente comigo.\n",
      "2022-08-01 16:10:55,464 - INFO - joeynmt.training - \tHypothesis: Voc√™ est√° t√£o impaciente comigo.\n",
      "2022-08-01 16:10:55,464 - INFO - joeynmt.training - Example #2\n",
      "2022-08-01 16:10:55,466 - INFO - joeynmt.training - \tSource:     How could I be a robot? Robots don't dream.\n",
      "2022-08-01 16:10:55,470 - INFO - joeynmt.training - \tReference:  Como eu poderia ser um rob√¥? Rob√¥s n√£o sonham.\n",
      "2022-08-01 16:10:55,470 - INFO - joeynmt.training - \tHypothesis: Como eu poderia ser um rob√¥? Os rob√¥s n√£o sonhom.\n",
      "2022-08-01 16:10:55,470 - INFO - joeynmt.training - Example #3\n",
      "2022-08-01 16:10:55,472 - INFO - joeynmt.training - \tSource:     You're not fast enough.\n",
      "2022-08-01 16:10:55,476 - INFO - joeynmt.training - \tReference:  Voc√™ n√£o √© r√°pido o bastante.\n",
      "2022-08-01 16:10:55,476 - INFO - joeynmt.training - \tHypothesis: Voc√™ n√£o √© r√°pido o suficiente.\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/config_normal.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7qULQu0B5Yl"
   },
   "source": [
    "### Continue training after interruption\n",
    "To continue after an interruption, the configuration needs to be modified in the following places:\n",
    "\n",
    "- `load_model` to point to the checkpoint to load.\n",
    "- `reset_*` options (must be False) to resume the previous session.\n",
    "- `model_dir` to create a new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr3mEDga2BiJ"
   },
   "outputs": [],
   "source": [
    "resume_config = config\\\n",
    "  .replace('#load_model:', 'load_model:')\\\n",
    "  .replace('#reset_best_ckpt: False', 'reset_best_ckpt: False')\\\n",
    "  .replace('#reset_scheduler: False', 'reset_scheduler: False')\\\n",
    "  .replace('#reset_optimizer: False', 'reset_optimizer: False')\\\n",
    "  .replace('#reset_iter_state: False', 'reset_iter_state: False')\\\n",
    "  .replace(f'model_dir: \"{model_dir}\"', f'model_dir: \"{model_dir}_resume\"')\n",
    "\n",
    "with (Path(data_dir) / \"resume_config.yaml\").open('w') as f:\n",
    "    f.write(resume_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgH1vAsV2Bkw",
    "outputId": "56439036-1e0d-4fb9-cffd-257f09903d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/site-packages/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/site-packages/joeynmt/__main__.py\", line 44, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/site-packages/joeynmt/training.py\", line 804, in train\n",
      "    cfg = load_config(Path(cfg_file))\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/site-packages/joeynmt/helpers.py\", line 193, in load_config\n",
      "    with path.open(\"r\", encoding=\"utf-8\") as ymlfile:\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/pathlib.py\", line 1252, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline,\n",
      "  File \"/home/lconti/anaconda3/envs/jnmt/lib/python3.9/pathlib.py\", line 1120, in _opener\n",
      "    return self._accessor.open(self, flags, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/lconti/en-pt_tatoeba/data/resume_config.yaml'\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/resume_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVv1ja0eCk66"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The `test` mode can be used to translate (and evaluate on) the test set specified in the configuration. We usually do this only once after we've tuned hyperparameters on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5T0OEp22BnX",
    "outputId": "327025da-0e8a-4b54-d602-019e1289c2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-03 18:38:06,199 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-03 18:38:06,200 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-03 18:38:06,327 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-03 18:38:06,327 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-03 18:38:06,327 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-03 18:38:17,789 - INFO - joeynmt.data - Loading dev set...\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 86.76ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 16241.76ex/s]\n",
      "2022-08-03 18:38:18,943 - INFO - joeynmt.data - Loading test set...\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 192.22ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 16572.38ex/s]\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.helpers - Train dataset: None\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ‚ñÅTom (6) ' (7) ‚ñÅI (8) ? (9) ‚ñÅa\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ‚ñÅTom (6) ' (7) ‚ñÅI (8) ? (9) ‚ñÅa\n",
      "2022-08-03 18:38:19,591 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-03 18:38:19,592 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-03 18:38:19,592 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-03 18:38:19,929 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-03 18:38:19,934 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-03 18:38:22,322 - INFO - joeynmt.helpers - Load model from /home/lconti/en-pt_tatoeba/models/normal_tf/18000.ckpt.\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 181.93ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 16452.96ex/s]\n",
      "2022-08-03 18:38:23,036 - INFO - joeynmt.prediction - Decoding on dev set...\n",
      "2022-08-03 18:38:23,036 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-03 18:40:58,624 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-03 18:40:58,624 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  35.70, generation: 155.4227[sec], evaluation: 0.1452[sec]\n",
      "Dropping NaN...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 177.98ba/s]\n",
      "Preprocessing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 16482.12ex/s]\n",
      "2022-08-03 18:40:59,260 - INFO - joeynmt.prediction - Decoding on test set...\n",
      "2022-08-03 18:40:59,260 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-03 18:44:01,661 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-03 18:44:01,661 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  39.07, generation: 182.2405[sec], evaluation: 0.1399[sec]\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt test {data_dir}/config_normal.yaml --ckpt {model_dir}/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy5Y4Qr_3p3I"
   },
   "source": [
    "> ‚ö† In beam search, the batch size is expanded {beam_size} times. For instance, if batch_size=10, batch_type=sentence and beam_size=5, joeynmt internally creates a batch of length 10*5=50. It may cause an out-of-memory error. Please specify the batch_size in `testing` section of config.yaml by taking this into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ripZMg6kCqzd"
   },
   "source": [
    "The `translate` mode is more interactive and takes prompts to translate interactively.\n",
    "\n",
    "Let's Translate a few examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEcyEwpS1Pvi",
    "outputId": "2259e7d7-ca22-409c-f6ea-593fa3b5de83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:43:59,643 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:44:17,067 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:44:17,429 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:44:21,857 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle √úbersetzung macht Spa√ü!\n",
      "2022-06-04 22:46:22,281 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:46:22,379 - INFO - joeynmt.prediction - Generation took 0.0963[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle √úbersetzung Sinn?\n",
      "2022-06-04 22:47:21,390 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:47:21,489 - INFO - joeynmt.prediction - Generation took 0.0975[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\n",
      "Please enter a source sentence:\n",
      "\n",
      "Bye.\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 2037, in shutdown\n",
      "    h.close()\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1103, in close\n",
      "    stream.close()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH5QwafwIYZu"
   },
   "source": [
    "You can also get the n-best hypotheses (up to the size of the beam, in our example 5), not only the highest scoring one. The better your model gets, the more interesting should the alternatives be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "GzpnG_3IJwkj"
   },
   "outputs": [],
   "source": [
    "nbest_config = config.replace('n_best: 1', 'n_best: 5')\\\n",
    "  .replace('#return_prob: \"hyp\"', 'return_prob: \"hyp\"')\n",
    "\n",
    "with (Path(data_dir) / \"nbest_config.yaml\").open('w') as f:\n",
    "    f.write(nbest_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nya4YXTGRurr",
    "outputId": "b879c6e2-2d12-4d22-d1ad-d441f87eaae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:48:11,849 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:48:29,174 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:48:29,557 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:48:34,476 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle √úbersetzung macht Spa√ü!\n",
      "2022-06-04 22:49:28,668 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:28,766 - INFO - joeynmt.prediction - Generation took 0.0962[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\ttokens: ['‚ñÅDon', \"'\", 't', '‚ñÅdrink', '‚ñÅtranslation', '‚ñÅis', '‚ñÅfun', '.', '</s>']\n",
      "\tsequence score: -3.362283706665039\n",
      "#2: Don't make any translation fun.\n",
      "\ttokens: ['‚ñÅDon', \"'\", 't', '‚ñÅmake', '‚ñÅany', '‚ñÅtranslation', '‚ñÅfun', '.', '</s>']\n",
      "\tsequence score: -3.677445411682129\n",
      "#3: Don't make sense to the machine.\n",
      "\ttokens: ['‚ñÅDon', \"'\", 't', '‚ñÅmake', '‚ñÅsense', '‚ñÅto', '‚ñÅthe', '‚ñÅmachine', '.', '</s>']\n",
      "\tsequence score: -3.9387967586517334\n",
      "#4: Don't drink translation is fun!\n",
      "\ttokens: ['‚ñÅDon', \"'\", 't', '‚ñÅdrink', '‚ñÅtranslation', '‚ñÅis', '‚ñÅfun', '!', '</s>']\n",
      "\tsequence score: -3.9769201278686523\n",
      "#5: Don't make a good translation.\n",
      "\ttokens: ['‚ñÅDon', \"'\", 't', '‚ñÅmake', '‚ñÅa', '‚ñÅgood', '‚ñÅtranslation', '.', '</s>']\n",
      "\tsequence score: -4.1912617683410645\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle √úbersetzung Sinn?\n",
      "2022-06-04 22:49:47,934 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:48,022 - INFO - joeynmt.prediction - Generation took 0.0862[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\ttokens: ['‚ñÅWhen', '‚ñÅdoes', '‚ñÅmachine', '‚ñÅmake', '‚ñÅsense', '?', '</s>']\n",
      "\tsequence score: -2.5040993690490723\n",
      "#2: When does the translation make sense?\n",
      "\ttokens: ['‚ñÅWhen', '‚ñÅdoes', '‚ñÅthe', '‚ñÅtranslation', '‚ñÅmake', '‚ñÅsense', '?', '</s>']\n",
      "\tsequence score: -2.5658979415893555\n",
      "#3: When does machine make sense to sense?\n",
      "\ttokens: ['‚ñÅWhen', '‚ñÅdoes', '‚ñÅmachine', '‚ñÅmake', '‚ñÅsense', '‚ñÅto', '‚ñÅsense', '?', '</s>']\n",
      "\tsequence score: -3.325543165206909\n",
      "#4: When does machine make sense to make?\n",
      "\ttokens: ['‚ñÅWhen', '‚ñÅdoes', '‚ñÅmachine', '‚ñÅmake', '‚ñÅsense', '‚ñÅto', '‚ñÅmake', '?', '</s>']\n",
      "\tsequence score: -3.422945976257324\n",
      "#5: When does the translation use?\n",
      "\ttokens: ['‚ñÅWhen', '‚ñÅdoes', '‚ñÅthe', '‚ñÅtranslation', '‚ñÅuse', '?', '</s>']\n",
      "\tsequence score: -3.6186904907226562\n",
      "\n",
      "Please enter a source sentence:\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 57, in main\n",
      "    output_path=args.output_path,\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/nbest_config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28sXN0GNIYAM"
   },
   "source": [
    "> üí° In BPE decoding, there are multiple ways to tokenize one sequence. That is, the same output string sequence might appear multiple times in the n best list, because they have different tokenization and thus different sequence in the generation.\n",
    "> For instance, say 3-best generation were:\n",
    "> ```\n",
    "> #1 best ['‚ñÅ', 'N', 'e', 'w', '‚ñÅYork']\n",
    "> #2 best ['‚ñÅ', 'New', '‚ñÅYork']\n",
    "> #3 best ['‚ñÅ', 'New', '‚ñÅY', 'o', 'r', 'k']\n",
    "> ````\n",
    "All three were different in next-token prediction, but ended up the same string sequence `New York` after being un-bpe-ed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "\n",
    "### plotting learning curves\n",
    "\n",
    "`plot_validations.py` script will generate validation learning curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/lconti/joeynmt/scripts/plot_validations.py {model_dir} --output_path /home/lconti/results/normal_learning_curve.png"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "quick-start-with-joeynmt2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jnmt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8f15f545f960394c2b715929c50f53ca25bb3ac30999d2a9b000ae440e403d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1795a2b1d6354fd2b1c69b12c9487f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27e4365db7d644d486745b9f334d06db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ff1b97d66404e488862d694facdfc23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fdb10ecb2040f8bd5e07834b422ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a00af231d8a44e9972f6916948418fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79a7716ac54b4f50981033df15f06f8e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c26c414246154062bbd0682340613e40",
      "value": 1
     }
    },
    "4a5a5a13229f404f9e4085751b0e2104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c447e15d9354c3c9a47c3ff843e74ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51afb334205f4a26b43fd6ddd0036cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52fc941d087447aa88ca1336e89d1d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6505b17c75e64c709fc6a91cc362444d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a841a34dd549f4bbe9fed60fc03853",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_31fdb10ecb2040f8bd5e07834b422ca0",
      "value": "Generating train split: "
     }
    },
    "65a841a34dd549f4bbe9fed60fc03853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6690085804c940ccb8dbd87e3f563678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6505b17c75e64c709fc6a91cc362444d",
       "IPY_MODEL_3a00af231d8a44e9972f6916948418fd",
       "IPY_MODEL_e79f7e89c05d4ce4935f53929b8aaaa1"
      ],
      "layout": "IPY_MODEL_4c447e15d9354c3c9a47c3ff843e74ce"
     }
    },
    "6d393ab342cc46109d1c723d736320ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52fc941d087447aa88ca1336e89d1d0f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1795a2b1d6354fd2b1c69b12c9487f0b",
      "value": " 1/1 [00:00&lt;00:00, 24.21ba/s]"
     }
    },
    "79a7716ac54b4f50981033df15f06f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9c838f660f7a46da92cfdb54c25fcb58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e9312e5e6646739ae3e6738de0e90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff1b97d66404e488862d694facdfc23",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_27e4365db7d644d486745b9f334d06db",
      "value": "Flattening the indices: 100%"
     }
    },
    "b669da1ab44a4e8cbfa6674f2dac1e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14c2320cc944c469a52a5579ee471bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c838f660f7a46da92cfdb54c25fcb58",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51afb334205f4a26b43fd6ddd0036cbe",
      "value": 1
     }
    },
    "c26c414246154062bbd0682340613e40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c951a5fc9f1540249dbb03a0ecf2f2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e9312e5e6646739ae3e6738de0e90c",
       "IPY_MODEL_c14c2320cc944c469a52a5579ee471bb",
       "IPY_MODEL_6d393ab342cc46109d1c723d736320ad"
      ],
      "layout": "IPY_MODEL_b669da1ab44a4e8cbfa6674f2dac1e5b"
     }
    },
    "e73400a7748241d29340d21f94784a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e79f7e89c05d4ce4935f53929b8aaaa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e73400a7748241d29340d21f94784a2e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4a5a5a13229f404f9e4085751b0e2104",
      "value": " 304551/0 [00:13&lt;00:00, 21566.82 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
