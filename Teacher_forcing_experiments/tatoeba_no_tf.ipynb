{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k051L-Y3rTu"
   },
   "source": [
    "\n",
    "# Training a toy NMT model en-pt with no teacher forcing and using the tatoeba corpus and JoeyNMT 2.0\n",
    "\n",
    "This notebook is based on [this demo](https://github.com/joeynmt/joeynmt/blob/main/notebooks/quick-start-with-joeynmt2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un8VWHfq5a-T"
   },
   "source": [
    "> âš  **Important:** Before you start, set runtime type to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_s7s4uevEtx",
    "outputId": "c58870bf-7812-4013-b93e-e323e84dbc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  2 13:43:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 26%   26C    P0    52W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 29%   27C    P0    52W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 30%   28C    P0    59W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:D9:00.0 Off |                  N/A |\n",
      "| 10%   25C    P0    57W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "242SJA2q5dRr"
   },
   "source": [
    "Make sure that you have a compatible PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pQwoOS-OvMLf",
    "outputId": "0bd93e90-3ff0-4812-f5a1-7eb255a5116e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rHdT56SF1J3"
   },
   "source": [
    "Install joeynmt (it's important to clone it from my fork, so teacher forcing can be deactivated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxQUAeHKR7Im",
    "outputId": "eb9697ec-8159-4cf5-a68c-3e2768d6d203"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/lina-conti/joeynmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/lconti/joeynmt\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-9.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.19.5\n",
      "  Downloading numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in ./my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (62.3.2)\n",
      "Collecting torch>=1.10.0\n",
      "  Downloading torch-1.12.0-cp39-cp39-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf==3.20.1\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard>=1.15\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sacrebleu>=2.0.0\n",
      "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m661.8/661.8 kB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12 in ./my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.16.0)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.14.5-py3-none-any.whl (488 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.2/488.2 kB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yapf\n",
      "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.2/190.2 kB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flake8\n",
      "  Downloading flake8-5.0.3-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2022.7.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m765.1/765.1 kB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lxml\n",
      "  Downloading lxml-4.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.4/232.4 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard>=1.15\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wheel>=0.26\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.2/141.2 kB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (21.3)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyflakes<2.6.0,>=2.5.0\n",
      "  Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycodestyle<2.10.0,>=2.9.0\n",
      "  Downloading pycodestyle-2.9.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mccabe<0.8.0,>=0.7.0\n",
      "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m944.1/944.1 kB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (3.0.9)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs>=2.2.0\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting tomlkit>=0.10.1\n",
      "  Downloading tomlkit-0.11.1-py3-none-any.whl (34 kB)\n",
      "Collecting astroid<=2.12.0-dev0,>=2.11.6\n",
      "  Downloading astroid-2.11.7-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=19.2.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting scipy>=1.0\n",
      "  Downloading scipy-1.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mock\n",
      "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m503.5/503.5 kB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.2/156.2 kB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing legacy 'setup.py install' for future, since package 'wheel' is not installed.\n",
      "Installing collected packages: yapf, tensorboard-plugin-wit, sentencepiece, pytz, pyasn1, iniconfig, zipp, xxhash, wrapt, wheel, urllib3, typing-extensions, tqdm, tomlkit, tomli, tensorboard-data-server, tabulate, rsa, regex, pyyaml, pyflakes, pycodestyle, pyasn1-modules, py, protobuf, portalocker, pluggy, platformdirs, pillow, oauthlib, numpy, multidict, mock, mccabe, MarkupSafe, lxml, lazy-object-proxy, kiwisolver, isort, idna, grpcio, future, fsspec, frozenlist, fonttools, filelock, dill, cycler, colorama, charset-normalizer, certifi, cachetools, attrs, async-timeout, absl-py, yarl, werkzeug, torch, subword-nmt, scipy, sacrebleu, requests, pytest, pyarrow, pandas, multiprocess, matplotlib, importlib-metadata, google-auth, flake8, astroid, aiosignal, seaborn, responses, requests-oauthlib, pylint, markdown, huggingface-hub, aiohttp, google-auth-oauthlib, tensorboard, datasets, joeynmt\n",
      "  Running setup.py install for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py develop for joeynmt\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 astroid-2.11.7 async-timeout-4.0.2 attrs-22.1.0 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.0 colorama-0.4.5 cycler-0.11.0 datasets-2.4.0 dill-0.3.5.1 filelock-3.7.1 flake8-5.0.3 fonttools-4.34.4 frozenlist-1.3.0 fsspec-2022.7.1 future-0.18.2 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 huggingface-hub-0.8.1 idna-3.3 importlib-metadata-4.12.0 iniconfig-1.1.1 isort-5.10.1 joeynmt-2.0.0 kiwisolver-1.4.4 lazy-object-proxy-1.7.1 lxml-4.9.1 markdown-3.4.1 matplotlib-3.5.2 mccabe-0.7.0 mock-4.0.3 multidict-6.0.2 multiprocess-0.70.13 numpy-1.23.1 oauthlib-3.2.0 pandas-1.4.3 pillow-9.2.0 platformdirs-2.5.2 pluggy-1.0.0 portalocker-2.5.1 protobuf-3.20.1 py-1.11.0 pyarrow-8.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycodestyle-2.9.0 pyflakes-2.5.0 pylint-2.14.5 pytest-7.1.2 pytz-2022.1 pyyaml-6.0 regex-2022.7.25 requests-2.28.1 requests-oauthlib-1.3.1 responses-0.18.0 rsa-4.9 sacrebleu-2.2.0 scipy-1.9.0 seaborn-0.11.2 sentencepiece-0.1.96 subword-nmt-0.3.8 tabulate-0.8.10 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tomli-2.0.1 tomlkit-0.11.1 torch-1.12.0 tqdm-4.64.0 typing-extensions-4.3.0 urllib3-1.26.11 werkzeug-2.2.1 wheel-0.37.1 wrapt-1.14.1 xxhash-3.0.0 yapf-0.32.0 yarl-1.8.1 zipp-3.8.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -e ./joeynmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJBa6lx26Hdx"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Download\n",
    "We'll use English - Portuguese translations from the [Tatoeba](https://tatoeba.org/) collection ([CC-BY 2.0 FR](https://creativecommons.org/licenses/by/2.0/fr/)).\n",
    "\n",
    "[Tatoeba](https://huggingface.co/datasets/tatoeba) corpus is available in Huggingface's datasets library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/lconti/en-pt_tatoeba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUvuySKGzyfP",
    "outputId": "188199bc-d145-4475-fa54-168108b4727c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-01 11:07:17--  https://raw.githubusercontent.com/may-/datasets/master/datasets/tatoeba/tatoeba.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4425 (4.3K) [text/plain]\n",
      "Saving to: '/home/lconti/en-pt_tatoeba/data/get_tatoeba.py'\n",
      "\n",
      "/home/lconti/en-pt_ 100%[===================>]   4.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-01 11:07:18 (40.1 MB/s) - '/home/lconti/en-pt_tatoeba/data/get_tatoeba.py' saved [4425/4425]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir {data_dir}\n",
    "!wget -O {data_dir}/get_tatoeba.py https://raw.githubusercontent.com/may-/datasets/master/datasets/tatoeba/tatoeba.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NjaFsxw6IFU"
   },
   "source": [
    "The Tatoeba dataset on HuggingFace Hub doesn't have dev and test split, but train split only. So let's split the data manually and save it locally.\n",
    "\n",
    "> ğŸ“ Note that most of the dataset loading scripts in Huggingface have pre-defined train-dev-test splits, e.g. [wmt17](https://huggingface.co/datasets/wmt17). In that case, you can skip this step, please go to the Vocabulary generation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "6690085804c940ccb8dbd87e3f563678",
      "6505b17c75e64c709fc6a91cc362444d",
      "3a00af231d8a44e9972f6916948418fd",
      "e79f7e89c05d4ce4935f53929b8aaaa1",
      "4c447e15d9354c3c9a47c3ff843e74ce",
      "65a841a34dd549f4bbe9fed60fc03853",
      "31fdb10ecb2040f8bd5e07834b422ca0",
      "79a7716ac54b4f50981033df15f06f8e",
      "c26c414246154062bbd0682340613e40",
      "e73400a7748241d29340d21f94784a2e",
      "4a5a5a13229f404f9e4085751b0e2104"
     ]
    },
    "id": "eTPhy7M8vWw0",
    "outputId": "77d1d31c-edb9-40d5-c64e-62d54b81db35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-pt-lang1=en,lang2=pt\n",
      "Reusing dataset get_tatoeba (/tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267)\n",
      "Using custom data configuration en-pt-lang1=en,lang2=pt\n",
      "Reusing dataset get_tatoeba (/tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267)\n",
      "Using custom data configuration en-pt-lang1=en,lang2=pt\n",
      "Reusing dataset get_tatoeba (/tmp/.cache/huggingface/get_tatoeba/en-pt-lang1=en,lang2=pt/0.0.0/336de120b2cb1a268f4eb9ebc7969075ccfabb978716d834a58a7889dbb5f267)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'translation'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'translation'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'translation'],\n",
       "     num_rows: 215647\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tatoeba_kwargs = {\n",
    "  \"path\": f\"{data_dir}/get_tatoeba.py\",\n",
    "  \"lang1\": \"en\",\n",
    "  \"lang2\": \"pt\",\n",
    "  \"ignore_verifications\": True,\n",
    "  \"cache_dir\": \"/tmp/.cache/huggingface\"\n",
    "}\n",
    "\n",
    "tatoeba_dev = load_dataset(split=\"train[:1000]\", **tatoeba_kwargs)\n",
    "tatoeba_test = load_dataset(split=\"train[1000:2000]\", **tatoeba_kwargs)\n",
    "tatoeba_train = load_dataset(split=\"train[2000:]\", **tatoeba_kwargs)\n",
    "\n",
    "tatoeba_dev, tatoeba_test, tatoeba_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mibraaK86JP0"
   },
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUFSJnWYvWzq",
    "outputId": "4a7ed79c-c7bb-4035-a0e8-38f7e515a734"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': \"Let's try something.\", 'pt': 'Vamos tentar alguma coisa!'},\n",
       " {'en': \"Let's try something.\", 'pt': 'Vamos tentar algo!'},\n",
       " {'en': \"Let's try something.\", 'pt': 'Vamos tentar algo.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba_dev['translation'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcBH2XbsvW2U",
    "outputId": "eae2a167-91a6-4038-fb33-0177bb7d1fc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': \"You're my type.\", 'pt': 'VocÃª Ã© o meu tipo.'},\n",
       " {'en': \"You're irresistible.\", 'pt': 'VocÃª Ã© irresistÃ­vel.'},\n",
       " {'en': 'Could you call again later, please?',\n",
       "  'pt': 'VocÃª poderia telefonar de novo mais tarde, por favor?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba_test['translation'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwGxfTTGvW44",
    "outputId": "f256020a-8672-4c8e-a2c0-35a6467cbbc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'What do you want now?', 'pt': 'O que vocÃª deseja agora?'},\n",
       " {'en': 'Do you love music?', 'pt': 'VocÃª ama mÃºsica?'},\n",
       " {'en': 'Do you love music?', 'pt': 'VocÃª aprecia a mÃºsica?'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba_train['translation'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlCBTXOH6KSw"
   },
   "source": [
    "Save the train-dev-test splits in local dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jz97rpCkvW7w"
   },
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "dataset_dict = DatasetDict({ \n",
    "  \"train\": tatoeba_train,\n",
    "  \"validation\": tatoeba_dev,\n",
    "  \"test\": tatoeba_test\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1RMfXeT-V1m"
   },
   "source": [
    "### Vocabulary\n",
    "\n",
    "We will use the [sentencepiece](https://github.com/google/sentencepiece) library to split words into subwords (BPE) according to their frequency in the training corpus.\n",
    "\n",
    "`build_vocab.py` script will train the BPE model and creates joint vocabulary. It takes the same config file as the joeynmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fJML2jYR1PlG"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"tatoeba_enpt_no_tf_sp\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"{data_dir}/train\"\n",
    "    dev: \"{data_dir}/validation\"\n",
    "    test: \"{data_dir}/test\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    #dataset_cfg:           # not necessary for manually saved pyarray daraset\n",
    "    #    name: \"en-pt\"\n",
    "    sample_dev_subset: 200\n",
    "    src:\n",
    "        lang: \"en\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "    trg:\n",
    "        lang: \"pt\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "\"\"\".format(data_dir=data_dir)\n",
    "with (Path(data_dir) / \"config_no_tf.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhZ19OVyAEQH"
   },
   "source": [
    "Call the script with `--joint` flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v3cW4WdDPpXT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-02 13:45:25--  https://raw.githubusercontent.com/joeynmt/joeynmt/main/scripts/build_vocab.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10277 (10K) [text/plain]\n",
      "Saving to: '/home/lconti/en-pt_no_tf/data/build_vocab.py'\n",
      "\n",
      "/home/lconti/en-pt_ 100%[===================>]  10.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-02 13:45:25 (53.3 MB/s) - '/home/lconti/en-pt_no_tf/data/build_vocab.py' saved [10277/10277]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O {data_dir}/build_vocab.py https://raw.githubusercontent.com/joeynmt/joeynmt/main/scripts/build_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMkbAmPz1Pnx",
    "outputId": "6d50d281-47f4-4fa0-bcc5-b3d65bccbd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 216/216 [00:01<00:00, 203.73ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 215642/215642 [00:12<00:00, 16708.24ex/s]\n",
      "### Training sentencepiece...\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/tmp/sentencepiece_4q60qtkm.txt --model_prefix=/home/lconti/en-pt_no_tf/data/sp --model_type=unigram --vocab_size=32000 --character_coverage=1.0 --accept_language=en,pt --unk_piece=<unk> --bos_piece=<s> --eos_piece=</s> --pad_piece=<pad> --unk_id=0 --bos_id=2 --eos_id=3 --pad_id=1 --vocabulary_output_piece_score=false\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /tmp/sentencepiece_4q60qtkm.txt\n",
      "  input_format: \n",
      "  model_prefix: /home/lconti/en-pt_no_tf/data/sp\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  accept_language: en\n",
      "  accept_language: pt\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 0\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /tmp/sentencepiece_4q60qtkm.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 431284 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=14706124\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=170\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 431284 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 157962 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 431284\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 101842\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 101842 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=54102 obj=10.9792 num_tokens=201010 num_tokens/piece=3.71539\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=43250 obj=8.65434 num_tokens=201758 num_tokens/piece=4.66492\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35194 obj=8.60258 num_tokens=209558 num_tokens/piece=5.95437\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35161 obj=8.59272 num_tokens=209737 num_tokens/piece=5.96505\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: /home/lconti/en-pt_no_tf/data/sp.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: /home/lconti/en-pt_no_tf/data/sp.vocab\n",
      "### Copying /home/lconti/en-pt_no_tf/data/sp.vocab to /home/lconti/en-pt_no_tf/data/vocab.txt ...\n",
      "### Done.\n"
     ]
    }
   ],
   "source": [
    "!python {data_dir}/build_vocab.py {data_dir}/config.yaml --joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gmLvwo9AO2i"
   },
   "source": [
    "The generated vocabulary looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HN1n1gTKARR7",
    "outputId": "afcfb609-8435-4de6-d91b-d9f3ee8ab0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "<s>\n",
      "</s>\n",
      ".\n",
      "â–Tom\n",
      "'\n",
      "â–I\n",
      "?\n",
      "â–a\n",
      "â–to\n",
      "â–que\n",
      "s\n",
      ",\n",
      "â–the\n",
      "â–de\n",
      "â–you\n",
      "t\n",
      "â–o\n",
      "â–nÃ£o\n"
     ]
    }
   ],
   "source": [
    "!head -20 {data_dir}/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6cN9CPtAaPl"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Joey NMT reads model and training hyperparameters from a configuration file. We're generating this now to configure paths in the appropriate places.\n",
    "\n",
    "The configuration below builds a small Transformer model with shared embeddings between source and target language on the base of the subword vocabularies created above.\n",
    "\n",
    "Note the \"teacher_forcing\" configuration in \"model\" â€” this is specific to my fork of joeynmt. It is where you can choose between \"on\", \"off\" or \"alternating\" (default is on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/lconti/en-pt_tatoeba/models/no_tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OJcLOr_S2BTD"
   },
   "outputs": [],
   "source": [
    "config += \"\"\"\n",
    "testing:\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 256\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 100\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    #return_prob: \"hyp\"\n",
    "    #return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "\n",
    "training:\n",
    "    #load_model: \"{model_dir}/latest.ckpt\"\n",
    "    #reset_best_ckpt: False\n",
    "    #reset_scheduler: False\n",
    "    #reset_optimizer: False\n",
    "    #reset_iter_state: False\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999]\n",
    "    scheduling: \"warmupinversesquareroot\"\n",
    "    learning_rate_warmup: 2000\n",
    "    learning_rate: 0.0002\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    loss: \"crossentropy\"\n",
    "    batch_size: 512\n",
    "    batch_type: \"token\"\n",
    "    batch_multiplier: 4\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 10\n",
    "    updates: 20000\n",
    "    validation_freq: 1000\n",
    "    logging_freq: 100\n",
    "    model_dir: \"{model_dir}\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    teacher_forcing: \"off\"\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\".format(model_dir=model_dir)\n",
    "with (Path(data_dir) / \"config_no_tf.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w45HbBfeMW38"
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH0-HBLLBLmR"
   },
   "source": [
    "### Run training\n",
    "â³ This will take a while. Model parameters will be stored on mounted google drive. The log reports the training process, look out for the prints of example translations and the BLEU evaluation scores to get an impression of the current quality.\n",
    "\n",
    "> â›” If you execute this twice, you might get an error that the model directory already exists. You can specify in the configuration to overwrite it, or delete it manually (`!rm -r {model_dir}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTbfgVOq2BfB",
    "outputId": "b667e5d9-4587-4f92-f8a5-308b7826ac51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-03 23:04:04,500 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -                           cfg.name : tatoeba_enpt_no_tf_sp\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -                     cfg.data.train : /home/lconti/en-pt_tatoeba/train\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/lconti/en-pt_tatoeba/validation\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -                      cfg.data.test : /home/lconti/en-pt_tatoeba/test\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-08-03 23:04:04,500 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : 200\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/lconti/en-pt_tatoeba/vocab.txt\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : sentencepiece\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/sp.model\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : pt\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/lconti/en-pt_tatoeba/vocab.txt\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : sentencepiece\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/sp.model\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 256\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-08-03 23:04:04,501 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 2000\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0002\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -            cfg.training.batch_size : 512\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -                cfg.training.epochs : 10\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -               cfg.training.updates : 20000\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/lconti/en-pt_tatoeba/models/no_tf\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -             cfg.training.overwrite : False\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -          cfg.model.teacher_forcing : off\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-08-03 23:04:04,502 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\n",
      "2022-08-03 23:04:04,503 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-08-03 23:04:04,504 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-03 23:04:04,589 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-03 23:04:04,590 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-03 23:04:04,590 - INFO - joeynmt.data - Loading train set...\n",
      "2022-08-03 23:04:05,012 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/train/cache-4be94cce75c187c1.arrow\n",
      "2022-08-03 23:04:05,027 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/train/cache-0066646d166ee7b1.arrow\n",
      "2022-08-03 23:04:05,030 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-03 23:04:17,270 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-03 23:04:17,603 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-e3360f65f1f28706.arrow\n",
      "2022-08-03 23:04:17,938 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-bbcf864065c98515.arrow\n",
      "2022-08-03 23:04:17,940 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-03 23:04:18,274 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-0099eb081810535f.arrow\n",
      "2022-08-03 23:04:18,598 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-7c540055e789733b.arrow\n",
      "2022-08-03 23:04:18,599 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-03 23:04:18,599 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=215642, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=train, path=/home/lconti/en-pt_tatoeba/train)\n",
      "2022-08-03 23:04:18,600 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-03 23:04:18,600 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-03 23:04:18,600 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] â–What â–do â–you â–want â–now ?\n",
      "\t[TRG] â–O â–que â–vocÃª â–deseja â–agora ?\n",
      "2022-08-03 23:04:18,600 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) â–Tom (6) ' (7) â–I (8) ? (9) â–a\n",
      "2022-08-03 23:04:18,600 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) â–Tom (6) ' (7) â–I (8) ? (9) â–a\n",
      "2022-08-03 23:04:18,601 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-03 23:04:18,601 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-03 23:04:18,615 - WARNING - joeynmt.tokenizers - /home/lconti/en-pt_tatoeba/models/no_tf/sp.model already exists. Stop copying.\n",
      "2022-08-03 23:04:18,616 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-03 23:04:18,921 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-03 23:04:18,925 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-03 23:04:18,927 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-08-03 23:04:21,106 - INFO - joeynmt.builders - Adam(lr=0.0002, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-08-03 23:04:21,106 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=2000, decay_rate=0.008944, peak_rate=0.0002, min_rate=1e-08)\n",
      "2022-08-03 23:04:21,106 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 4\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 128\n",
      "\teffective batch size (w. parallel & accumulation): 2048\n",
      "2022-08-03 23:04:21,106 - INFO - joeynmt.training - EPOCH 1\n",
      "/home/lconti/my_jnmt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2022-08-03 23:08:03,364 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     2.153384, Batch Acc: 0.001007, Tokens per Sec:      429, Lr: 0.000010\n",
      "2022-08-03 23:11:43,978 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.034605, Batch Acc: 0.001336, Tokens per Sec:      424, Lr: 0.000020\n",
      "2022-08-03 23:15:25,060 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     1.858821, Batch Acc: 0.001026, Tokens per Sec:      428, Lr: 0.000030\n",
      "2022-08-03 23:19:02,429 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     1.642029, Batch Acc: 0.001623, Tokens per Sec:      434, Lr: 0.000040\n",
      "2022-08-03 23:22:37,526 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     1.456401, Batch Acc: 0.001603, Tokens per Sec:      444, Lr: 0.000050\n",
      "2022-08-03 23:26:09,386 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     1.362663, Batch Acc: 0.001706, Tokens per Sec:      445, Lr: 0.000060\n",
      "2022-08-03 23:29:41,914 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     1.275981, Batch Acc: 0.001444, Tokens per Sec:      446, Lr: 0.000070\n",
      "2022-08-03 23:33:13,863 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     1.227097, Batch Acc: 0.001793, Tokens per Sec:      450, Lr: 0.000080\n",
      "2022-08-03 23:36:49,049 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     1.246886, Batch Acc: 0.001474, Tokens per Sec:      441, Lr: 0.000090\n",
      "2022-08-03 23:40:20,674 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     1.172321, Batch Acc: 0.002603, Tokens per Sec:      448, Lr: 0.000100\n",
      "2022-08-03 23:40:20,999 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-06617e88cb272dab.arrow\n",
      "2022-08-03 23:40:21,323 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-3d931698fbe82046.arrow\n",
      "2022-08-03 23:40:21,328 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=1000\n",
      "2022-08-03 23:40:21,329 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-03 23:40:41,775 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-03 23:40:41,775 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.33, loss:   4.87, ppl: 130.29, acc:   0.20, generation: 20.4249[sec], evaluation: 0.0167[sec]\n",
      "2022-08-03 23:40:41,776 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-03 23:40:42,078 - INFO - joeynmt.training - Example #0\n",
      "2022-08-03 23:40:42,079 - INFO - joeynmt.training - \tSource:     This is my friend Rachel. We went to high school together.\n",
      "2022-08-03 23:40:42,079 - INFO - joeynmt.training - \tReference:  Essa Ã© minha amiga Rachel, nÃ³s fomos juntos ao colÃ©gio.\n",
      "2022-08-03 23:40:42,080 - INFO - joeynmt.training - \tHypothesis: O Ã© Ã© de de de de de de...\n",
      "2022-08-03 23:40:42,080 - INFO - joeynmt.training - Example #1\n",
      "2022-08-03 23:40:42,081 - INFO - joeynmt.training - \tSource:     \"Yes, orange juice please,\" says Mike.\n",
      "2022-08-03 23:40:42,081 - INFO - joeynmt.training - \tReference:  \"Sim, suco de laranja, por favor\", diz Mike.\n",
      "2022-08-03 23:40:42,081 - INFO - joeynmt.training - \tHypothesis: O Ã© de de de de de de...\n",
      "2022-08-03 23:40:42,081 - INFO - joeynmt.training - Example #2\n",
      "2022-08-03 23:40:42,082 - INFO - joeynmt.training - \tSource:     \"This is what I was looking for!\" he exclaimed.\n",
      "2022-08-03 23:40:42,082 - INFO - joeynmt.training - \tReference:  \"Era isso que eu estava procurando!\" Ele exclamou.\n",
      "2022-08-03 23:40:42,082 - INFO - joeynmt.training - \tHypothesis: Tom nÃ£o que que que que que que que..\n",
      "2022-08-03 23:40:42,082 - INFO - joeynmt.training - Example #3\n",
      "2022-08-03 23:40:42,083 - INFO - joeynmt.training - \tSource:     I have to go shopping. I'll be back in an hour.\n",
      "2022-08-03 23:40:42,083 - INFO - joeynmt.training - \tReference:  Eu tenho que fazer compras. Voltarei em uma hora.\n",
      "2022-08-03 23:40:42,083 - INFO - joeynmt.training - \tHypothesis: Eu nÃ£o que que que de de.....\n",
      "2022-08-03 23:44:24,588 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     1.187402, Batch Acc: 0.002110, Tokens per Sec:      422, Lr: 0.000110\n",
      "2022-08-03 23:47:54,572 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     1.136091, Batch Acc: 0.002378, Tokens per Sec:      451, Lr: 0.000120\n",
      "2022-08-03 23:51:34,661 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     1.117036, Batch Acc: 0.002389, Tokens per Sec:      428, Lr: 0.000130\n",
      "2022-08-03 23:55:04,796 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     1.036078, Batch Acc: 0.002800, Tokens per Sec:      455, Lr: 0.000140\n",
      "2022-08-03 23:58:37,206 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     1.081824, Batch Acc: 0.002756, Tokens per Sec:      446, Lr: 0.000150\n",
      "2022-08-04 00:02:10,914 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     1.021089, Batch Acc: 0.002748, Tokens per Sec:      443, Lr: 0.000160\n",
      "2022-08-04 00:05:47,101 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     1.005889, Batch Acc: 0.002967, Tokens per Sec:      438, Lr: 0.000170\n",
      "2022-08-04 00:09:26,765 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     0.978776, Batch Acc: 0.003169, Tokens per Sec:      435, Lr: 0.000180\n",
      "2022-08-04 00:13:04,847 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     0.977176, Batch Acc: 0.003201, Tokens per Sec:      431, Lr: 0.000190\n",
      "2022-08-04 00:16:41,415 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     0.936413, Batch Acc: 0.003221, Tokens per Sec:      440, Lr: 0.000200\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 95.62ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17276.22ex/s]\n",
      "2022-08-04 00:16:42,142 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=2000\n",
      "2022-08-04 00:16:42,142 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 00:16:57,382 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 00:16:57,382 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   1.91, loss:   4.24, ppl:  69.37, acc:   0.25, generation: 15.1425[sec], evaluation: 0.0882[sec]\n",
      "2022-08-04 00:16:57,382 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 00:16:57,684 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 00:16:57,686 - INFO - joeynmt.training - \tSource:     \"Haven't we met somewhere before?\" asked the student.\n",
      "2022-08-04 00:16:57,686 - INFO - joeynmt.training - \tReference:  \"JÃ¡ nÃ£o nos conhecemos de algum lugar?\" perguntou o estudante.\n",
      "2022-08-04 00:16:57,686 - INFO - joeynmt.training - \tHypothesis: \" nÃ£o nÃ£o a a a a a a..\n",
      "2022-08-04 00:16:57,686 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 00:16:57,687 - INFO - joeynmt.training - \tSource:     You are saying you intentionally hide your good looks?\n",
      "2022-08-04 00:16:57,687 - INFO - joeynmt.training - \tReference:  VocÃª estÃ¡ dizendo que esconde sua beleza intencionalmente?\n",
      "2022-08-04 00:16:57,687 - INFO - joeynmt.training - \tHypothesis: VocÃª Ã© que que que o o o estÃ¡ estÃ¡?\n",
      "2022-08-04 00:16:57,687 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 00:16:57,689 - INFO - joeynmt.training - \tSource:     Did you miss me?\n",
      "2022-08-04 00:16:57,689 - INFO - joeynmt.training - \tReference:  A senhora sentiu saudades minhas?\n",
      "2022-08-04 00:16:57,689 - INFO - joeynmt.training - \tHypothesis: VocÃª me me de?\n",
      "2022-08-04 00:16:57,689 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 00:16:57,690 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-04 00:16:57,690 - INFO - joeynmt.training - \tReference:  O senhor vai me esquecer um dia.\n",
      "2022-08-04 00:16:57,690 - INFO - joeynmt.training - \tHypothesis: VocÃª vai me me me de..\n",
      "2022-08-04 00:17:31,911 - INFO - joeynmt.training - Epoch   1: total training loss 2663.48\n",
      "2022-08-04 00:17:31,911 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-08-04 00:20:29,355 - INFO - joeynmt.training - Epoch   2, Step:     2100, Batch Loss:     0.939052, Batch Acc: 0.003579, Tokens per Sec:      446, Lr: 0.000195\n",
      "2022-08-04 00:24:00,807 - INFO - joeynmt.training - Epoch   2, Step:     2200, Batch Loss:     0.875511, Batch Acc: 0.003401, Tokens per Sec:      453, Lr: 0.000191\n",
      "2022-08-04 00:27:31,472 - INFO - joeynmt.training - Epoch   2, Step:     2300, Batch Loss:     0.843270, Batch Acc: 0.003416, Tokens per Sec:      456, Lr: 0.000187\n",
      "2022-08-04 00:31:06,399 - INFO - joeynmt.training - Epoch   2, Step:     2400, Batch Loss:     0.890733, Batch Acc: 0.003616, Tokens per Sec:      440, Lr: 0.000183\n",
      "2022-08-04 00:34:41,472 - INFO - joeynmt.training - Epoch   2, Step:     2500, Batch Loss:     0.824513, Batch Acc: 0.003534, Tokens per Sec:      437, Lr: 0.000179\n",
      "2022-08-04 00:46:22,260 - INFO - joeynmt.training - Epoch   2, Step:     2600, Batch Loss:     0.835773, Batch Acc: 0.003441, Tokens per Sec:      136, Lr: 0.000175\n",
      "2022-08-04 01:11:57,594 - INFO - joeynmt.training - Epoch   2, Step:     2700, Batch Loss:     0.779367, Batch Acc: 0.003990, Tokens per Sec:       62, Lr: 0.000172\n",
      "2022-08-04 01:38:10,503 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     0.780242, Batch Acc: 0.003676, Tokens per Sec:       60, Lr: 0.000169\n",
      "2022-08-04 02:03:59,838 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     0.752105, Batch Acc: 0.004089, Tokens per Sec:       61, Lr: 0.000166\n",
      "2022-08-04 02:28:36,797 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     0.832371, Batch Acc: 0.003737, Tokens per Sec:       64, Lr: 0.000163\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.76ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 589.79ex/s]\n",
      "2022-08-04 02:28:55,275 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=3000\n",
      "2022-08-04 02:28:55,275 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 02:30:41,790 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 02:30:41,791 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.67, loss:   3.60, ppl:  36.60, acc:   0.32, generation: 106.3940[sec], evaluation: 0.1067[sec]\n",
      "2022-08-04 02:30:41,792 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 02:30:44,537 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 02:30:44,540 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-04 02:30:44,540 - INFO - joeynmt.training - \tReference:  As senhoras me olvidarÃ£o um dia.\n",
      "2022-08-04 02:30:44,540 - INFO - joeynmt.training - \tHypothesis: VocÃª vai se se para mim uma dia.\n",
      "2022-08-04 02:30:44,540 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 02:30:44,542 - INFO - joeynmt.training - \tSource:     Are they all the same?\n",
      "2022-08-04 02:30:44,542 - INFO - joeynmt.training - \tReference:  SÃ£o todos do mesmo tipo?\n",
      "2022-08-04 02:30:44,542 - INFO - joeynmt.training - \tHypothesis: Eles sÃ£o os os??\n",
      "2022-08-04 02:30:44,542 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 02:30:44,544 - INFO - joeynmt.training - \tSource:     The wind calmed down.\n",
      "2022-08-04 02:30:44,544 - INFO - joeynmt.training - \tReference:  O vento se acalmou.\n",
      "2022-08-04 02:30:44,544 - INFO - joeynmt.training - \tHypothesis: O vento se..\n",
      "2022-08-04 02:30:44,544 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 02:30:44,546 - INFO - joeynmt.training - \tSource:     How do you spell \"pretty\"?\n",
      "2022-08-04 02:30:44,547 - INFO - joeynmt.training - \tReference:  Como se escreve \"pretty\"?\n",
      "2022-08-04 02:30:44,548 - INFO - joeynmt.training - \tHypothesis: Como vocÃª \" \" \"?\n",
      "2022-08-04 02:53:44,223 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     0.732109, Batch Acc: 0.004097, Tokens per Sec:       68, Lr: 0.000161\n",
      "2022-08-04 03:18:00,838 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     0.786322, Batch Acc: 0.003569, Tokens per Sec:       66, Lr: 0.000158\n",
      "2022-08-04 03:43:32,480 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     0.833439, Batch Acc: 0.003443, Tokens per Sec:       61, Lr: 0.000156\n",
      "2022-08-04 04:07:35,665 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     0.797631, Batch Acc: 0.003596, Tokens per Sec:       66, Lr: 0.000153\n",
      "2022-08-04 04:14:08,058 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     0.739343, Batch Acc: 0.004174, Tokens per Sec:      244, Lr: 0.000151\n",
      "2022-08-04 04:22:38,871 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     0.798664, Batch Acc: 0.003481, Tokens per Sec:      187, Lr: 0.000149\n",
      "2022-08-04 04:42:39,237 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     0.739708, Batch Acc: 0.003633, Tokens per Sec:       79, Lr: 0.000147\n",
      "2022-08-04 05:06:06,888 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     0.772976, Batch Acc: 0.003400, Tokens per Sec:       68, Lr: 0.000145\n",
      "2022-08-04 05:30:02,335 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     0.751700, Batch Acc: 0.003457, Tokens per Sec:       66, Lr: 0.000143\n",
      "2022-08-04 05:54:04,609 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     0.737424, Batch Acc: 0.003610, Tokens per Sec:       65, Lr: 0.000141\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 94.89ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1499.08ex/s]\n",
      "2022-08-04 05:54:12,728 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=4000\n",
      "2022-08-04 05:54:12,737 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 05:55:44,333 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 05:55:44,333 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.76, loss:   3.48, ppl:  32.60, acc:   0.32, generation: 91.3517[sec], evaluation: 0.2352[sec]\n",
      "2022-08-04 05:55:44,334 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 05:55:47,441 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/1000.ckpt\n",
      "2022-08-04 05:55:47,605 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 05:55:47,619 - INFO - joeynmt.training - \tSource:     \"Why aren't you going?\" \"Because I don't want to.\"\n",
      "2022-08-04 05:55:47,634 - INFO - joeynmt.training - \tReference:  \"Por que vocÃª nÃ£o vai?\" \"Porque nÃ£o estou a fim de ir.\"\n",
      "2022-08-04 05:55:47,636 - INFO - joeynmt.training - \tHypothesis: \" \" que nÃ£o nÃ£o vocÃª?\"?\"?\" eu quero quero\n",
      "2022-08-04 05:55:47,636 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 05:55:47,640 - INFO - joeynmt.training - \tSource:     It almost scared me not to see you online for a whole day.\n",
      "2022-08-04 05:55:47,653 - INFO - joeynmt.training - \tReference:  Quase me assustou nÃ£o te ver online por um dia inteiro.\n",
      "2022-08-04 05:55:47,654 - INFO - joeynmt.training - \tHypothesis: EstÃ¡ quase de eu nÃ£o ver ver ver um um dia..\n",
      "2022-08-04 05:55:47,656 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 05:55:47,662 - INFO - joeynmt.training - \tSource:     Most people think I'm crazy.\n",
      "2022-08-04 05:55:47,675 - INFO - joeynmt.training - \tReference:  A maioria das pessoas acha que eu estou louco.\n",
      "2022-08-04 05:55:47,676 - INFO - joeynmt.training - \tHypothesis: A maioria maioria pessoas que que eu..\n",
      "2022-08-04 05:55:47,677 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 05:55:47,682 - INFO - joeynmt.training - \tSource:     This is not important.\n",
      "2022-08-04 05:55:47,696 - INFO - joeynmt.training - \tReference:  Isto nÃ£o Ã© importante.\n",
      "2022-08-04 05:55:47,698 - INFO - joeynmt.training - \tHypothesis: Isto nÃ£o Ã© importante.\n",
      "2022-08-04 06:03:20,389 - INFO - joeynmt.training - Epoch   2: total training loss 1613.71\n",
      "2022-08-04 06:03:20,389 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-08-04 06:20:24,427 - INFO - joeynmt.training - Epoch   3, Step:     4100, Batch Loss:     0.667778, Batch Acc: 0.006373, Tokens per Sec:       64, Lr: 0.000140\n",
      "2022-08-04 06:43:18,232 - INFO - joeynmt.training - Epoch   3, Step:     4200, Batch Loss:     0.714573, Batch Acc: 0.004557, Tokens per Sec:       69, Lr: 0.000138\n",
      "2022-08-04 07:06:06,014 - INFO - joeynmt.training - Epoch   3, Step:     4300, Batch Loss:     0.695665, Batch Acc: 0.004053, Tokens per Sec:       70, Lr: 0.000136\n",
      "2022-08-04 07:28:16,815 - INFO - joeynmt.training - Epoch   3, Step:     4400, Batch Loss:     0.682062, Batch Acc: 0.004234, Tokens per Sec:       71, Lr: 0.000135\n",
      "2022-08-04 07:52:41,554 - INFO - joeynmt.training - Epoch   3, Step:     4500, Batch Loss:     0.710002, Batch Acc: 0.004042, Tokens per Sec:       65, Lr: 0.000133\n",
      "2022-08-04 08:03:23,115 - INFO - joeynmt.training - Epoch   3, Step:     4600, Batch Loss:     0.663782, Batch Acc: 0.004478, Tokens per Sec:      148, Lr: 0.000132\n",
      "2022-08-04 08:11:33,507 - INFO - joeynmt.training - Epoch   3, Step:     4700, Batch Loss:     0.811341, Batch Acc: 0.002966, Tokens per Sec:      194, Lr: 0.000130\n",
      "2022-08-04 08:26:31,526 - INFO - joeynmt.training - Epoch   3, Step:     4800, Batch Loss:     0.646380, Batch Acc: 0.004447, Tokens per Sec:      106, Lr: 0.000129\n",
      "2022-08-04 08:50:15,907 - INFO - joeynmt.training - Epoch   3, Step:     4900, Batch Loss:     0.674708, Batch Acc: 0.004448, Tokens per Sec:       67, Lr: 0.000128\n",
      "2022-08-04 09:13:19,617 - INFO - joeynmt.training - Epoch   3, Step:     5000, Batch Loss:     0.697092, Batch Acc: 0.003931, Tokens per Sec:       69, Lr: 0.000126\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 81.79ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5849.26ex/s]\n",
      "2022-08-04 09:13:24,216 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=5000\n",
      "2022-08-04 09:13:24,254 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 09:16:14,856 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 09:16:14,856 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.80, loss:   3.34, ppl:  28.19, acc:   0.34, generation: 170.3985[sec], evaluation: 0.1947[sec]\n",
      "2022-08-04 09:16:14,856 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 09:16:17,299 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/2000.ckpt\n",
      "2022-08-04 09:16:17,589 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 09:16:17,592 - INFO - joeynmt.training - \tSource:     Is it far from here?\n",
      "2022-08-04 09:16:17,592 - INFO - joeynmt.training - \tReference:  Fica longe daqui?\n",
      "2022-08-04 09:16:17,592 - INFO - joeynmt.training - \tHypothesis: Ã‰ longe daqui?\n",
      "2022-08-04 09:16:17,592 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 09:16:17,599 - INFO - joeynmt.training - \tSource:     I learned to live without her.\n",
      "2022-08-04 09:16:17,603 - INFO - joeynmt.training - \tReference:  Eu aprendi a viver sem ela.\n",
      "2022-08-04 09:16:17,605 - INFO - joeynmt.training - \tHypothesis: Eu aprendi a sem sem.\n",
      "2022-08-04 09:16:17,606 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 09:16:17,611 - INFO - joeynmt.training - \tSource:     What other options do I have?\n",
      "2022-08-04 09:16:17,641 - INFO - joeynmt.training - \tReference:  Que outras opÃ§Ãµes eu tenho?\n",
      "2022-08-04 09:16:17,644 - INFO - joeynmt.training - \tHypothesis: Que coisas coisas que eu tenho?\n",
      "2022-08-04 09:16:17,646 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 09:16:17,651 - INFO - joeynmt.training - \tSource:     I never liked biology.\n",
      "2022-08-04 09:16:17,680 - INFO - joeynmt.training - \tReference:  Nunca curti biologia.\n",
      "2022-08-04 09:16:17,683 - INFO - joeynmt.training - \tHypothesis: Eu nunca gostei de.\n",
      "2022-08-04 09:39:17,651 - INFO - joeynmt.training - Epoch   3, Step:     5100, Batch Loss:     0.679124, Batch Acc: 0.003895, Tokens per Sec:       67, Lr: 0.000125\n",
      "2022-08-04 10:02:24,333 - INFO - joeynmt.training - Epoch   3, Step:     5200, Batch Loss:     0.582367, Batch Acc: 0.004604, Tokens per Sec:       69, Lr: 0.000124\n",
      "2022-08-04 10:24:28,801 - INFO - joeynmt.training - Epoch   3, Step:     5300, Batch Loss:     0.678969, Batch Acc: 0.003685, Tokens per Sec:       72, Lr: 0.000123\n",
      "2022-08-04 10:49:28,479 - INFO - joeynmt.training - Epoch   3, Step:     5400, Batch Loss:     0.599346, Batch Acc: 0.005401, Tokens per Sec:       63, Lr: 0.000122\n",
      "2022-08-04 11:13:54,637 - INFO - joeynmt.training - Epoch   3, Step:     5500, Batch Loss:     0.621922, Batch Acc: 0.004115, Tokens per Sec:       64, Lr: 0.000121\n",
      "2022-08-04 11:35:12,884 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     0.653459, Batch Acc: 0.004299, Tokens per Sec:       74, Lr: 0.000120\n",
      "2022-08-04 11:53:06,886 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     0.657566, Batch Acc: 0.004584, Tokens per Sec:       88, Lr: 0.000118\n",
      "2022-08-04 12:01:08,973 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     0.607324, Batch Acc: 0.004900, Tokens per Sec:      196, Lr: 0.000117\n",
      "2022-08-04 12:12:58,735 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     0.610906, Batch Acc: 0.004099, Tokens per Sec:      133, Lr: 0.000116\n",
      "2022-08-04 12:36:49,987 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     0.616584, Batch Acc: 0.004650, Tokens per Sec:       66, Lr: 0.000115\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.81ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1696.19ex/s]\n",
      "2022-08-04 12:36:58,564 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=6000\n",
      "2022-08-04 12:36:58,565 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 12:39:03,039 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 12:39:03,039 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.71, loss:   3.31, ppl:  27.31, acc:   0.32, generation: 124.2290[sec], evaluation: 0.2353[sec]\n",
      "2022-08-04 12:39:08,293 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/3000.ckpt\n",
      "2022-08-04 12:39:08,749 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 12:39:08,752 - INFO - joeynmt.training - \tSource:     I don't know what you mean.\n",
      "2022-08-04 12:39:08,752 - INFO - joeynmt.training - \tReference:  NÃ£o sei o que o senhor estÃ¡ querendo dizer.\n",
      "2022-08-04 12:39:08,752 - INFO - joeynmt.training - \tHypothesis: NÃ£o nÃ£o sei o que vocÃª quer.\n",
      "2022-08-04 12:39:08,752 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 12:39:08,754 - INFO - joeynmt.training - \tSource:     When I grow up, I want to be a king.\n",
      "2022-08-04 12:39:08,754 - INFO - joeynmt.training - \tReference:  Quando eu crescer eu quero ser rei.\n",
      "2022-08-04 12:39:08,754 - INFO - joeynmt.training - \tHypothesis: Quando eu,, quero ser um..\n",
      "2022-08-04 12:39:08,754 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 12:39:08,756 - INFO - joeynmt.training - \tSource:     I hate those spiders. They're always there to freak me out when I'm cleaning.\n",
      "2022-08-04 12:39:08,756 - INFO - joeynmt.training - \tReference:  Odeio aquelas aranhas. Elas estÃ£o sempre lÃ¡ para me assustarem quando estou limpando.\n",
      "2022-08-04 12:39:08,756 - INFO - joeynmt.training - \tHypothesis: Eu odeio essas.. Eles sempre sempre para para me quando quando quando eu eu..\n",
      "2022-08-04 12:39:08,757 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 12:39:08,896 - INFO - joeynmt.training - \tSource:     What do you want?\n",
      "2022-08-04 12:39:08,897 - INFO - joeynmt.training - \tReference:  Que Ã© que deseja?\n",
      "2022-08-04 12:39:08,897 - INFO - joeynmt.training - \tHypothesis: O que vocÃª quer?\n",
      "2022-08-04 12:50:21,369 - INFO - joeynmt.training - Epoch   3: total training loss 1330.21\n",
      "2022-08-04 12:50:21,369 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-08-04 13:02:44,337 - INFO - joeynmt.training - Epoch   4, Step:     6100, Batch Loss:     0.529244, Batch Acc: 0.009941, Tokens per Sec:       67, Lr: 0.000115\n",
      "2022-08-04 13:27:06,493 - INFO - joeynmt.training - Epoch   4, Step:     6200, Batch Loss:     0.620765, Batch Acc: 0.004635, Tokens per Sec:       65, Lr: 0.000114\n",
      "2022-08-04 13:52:08,378 - INFO - joeynmt.training - Epoch   4, Step:     6300, Batch Loss:     0.648014, Batch Acc: 0.004728, Tokens per Sec:       61, Lr: 0.000113\n",
      "2022-08-04 14:13:44,167 - INFO - joeynmt.training - Epoch   4, Step:     6400, Batch Loss:     0.622375, Batch Acc: 0.004050, Tokens per Sec:       74, Lr: 0.000112\n",
      "2022-08-04 14:37:13,328 - INFO - joeynmt.training - Epoch   4, Step:     6500, Batch Loss:     0.597178, Batch Acc: 0.004095, Tokens per Sec:       68, Lr: 0.000111\n",
      "2022-08-04 15:02:17,067 - INFO - joeynmt.training - Epoch   4, Step:     6600, Batch Loss:     0.592959, Batch Acc: 0.004140, Tokens per Sec:       62, Lr: 0.000110\n",
      "2022-08-04 15:24:51,648 - INFO - joeynmt.training - Epoch   4, Step:     6700, Batch Loss:     0.538296, Batch Acc: 0.005176, Tokens per Sec:       70, Lr: 0.000109\n",
      "2022-08-04 15:43:34,573 - INFO - joeynmt.training - Epoch   4, Step:     6800, Batch Loss:     0.581911, Batch Acc: 0.004675, Tokens per Sec:       85, Lr: 0.000108\n",
      "2022-08-04 15:50:55,536 - INFO - joeynmt.training - Epoch   4, Step:     6900, Batch Loss:     0.593695, Batch Acc: 0.004405, Tokens per Sec:      212, Lr: 0.000108\n",
      "2022-08-04 15:59:31,365 - INFO - joeynmt.training - Epoch   4, Step:     7000, Batch Loss:     0.560212, Batch Acc: 0.005263, Tokens per Sec:      185, Lr: 0.000107\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 44.15ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2490.77ex/s]\n",
      "2022-08-04 15:59:36,450 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=7000\n",
      "2022-08-04 15:59:36,451 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 16:00:18,099 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 16:00:18,100 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  13.80, loss:   3.22, ppl:  25.10, acc:   0.35, generation: 41.6082[sec], evaluation: 0.0325[sec]\n",
      "2022-08-04 16:00:18,100 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 16:00:18,555 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/4000.ckpt\n",
      "2022-08-04 16:00:18,576 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 16:00:18,578 - INFO - joeynmt.training - \tSource:     Why do you ask?\n",
      "2022-08-04 16:00:18,578 - INFO - joeynmt.training - \tReference:  Por que vocÃªs perguntam?\n",
      "2022-08-04 16:00:18,578 - INFO - joeynmt.training - \tHypothesis: Por que vocÃª faz?\n",
      "2022-08-04 16:00:18,578 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 16:00:18,580 - INFO - joeynmt.training - \tSource:     When I ask people what they regret most about high school, they nearly all say the same thing: that they wasted so much time.\n",
      "2022-08-04 16:00:18,580 - INFO - joeynmt.training - \tReference:  Quando pergunto Ã s pessoas o que elas mais lamentam sobre a escola secundÃ¡ria, quase todos dizem a mesma coisa: que perderam muito tempo.\n",
      "2022-08-04 16:00:18,580 - INFO - joeynmt.training - \tHypothesis: Quando eu algumas que pessoas que se se se mais mais mais,,,, sempre sempre sempre sempre mesma mesma que que que que que que que..\n",
      "2022-08-04 16:00:18,580 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 16:00:18,582 - INFO - joeynmt.training - \tSource:     Why don't you come visit us?\n",
      "2022-08-04 16:00:18,582 - INFO - joeynmt.training - \tReference:  Por que nÃ£o vem nos visitar?\n",
      "2022-08-04 16:00:18,582 - INFO - joeynmt.training - \tHypothesis: Por que vocÃª nÃ£o nos visitar?\n",
      "2022-08-04 16:00:18,582 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 16:00:18,584 - INFO - joeynmt.training - \tSource:     I'd like to stay for one night.\n",
      "2022-08-04 16:00:18,584 - INFO - joeynmt.training - \tReference:  Eu gostaria de ficar por uma noite.\n",
      "2022-08-04 16:00:18,584 - INFO - joeynmt.training - \tHypothesis: Gostaria de ficar ficar uma noite.\n",
      "2022-08-04 16:24:48,004 - INFO - joeynmt.training - Epoch   4, Step:     7100, Batch Loss:     0.551623, Batch Acc: 0.004022, Tokens per Sec:       64, Lr: 0.000106\n",
      "2022-08-04 16:48:46,489 - INFO - joeynmt.training - Epoch   4, Step:     7200, Batch Loss:     0.617481, Batch Acc: 0.004504, Tokens per Sec:       65, Lr: 0.000105\n",
      "2022-08-04 17:11:54,944 - INFO - joeynmt.training - Epoch   4, Step:     7300, Batch Loss:     0.556354, Batch Acc: 0.005316, Tokens per Sec:       68, Lr: 0.000105\n",
      "2022-08-04 17:36:27,407 - INFO - joeynmt.training - Epoch   4, Step:     7400, Batch Loss:     0.614268, Batch Acc: 0.004441, Tokens per Sec:       65, Lr: 0.000104\n",
      "2022-08-04 17:59:29,873 - INFO - joeynmt.training - Epoch   4, Step:     7500, Batch Loss:     0.547685, Batch Acc: 0.004866, Tokens per Sec:       69, Lr: 0.000103\n",
      "2022-08-04 18:22:53,950 - INFO - joeynmt.training - Epoch   4, Step:     7600, Batch Loss:     0.615513, Batch Acc: 0.004370, Tokens per Sec:       67, Lr: 0.000103\n",
      "2022-08-04 18:44:12,230 - INFO - joeynmt.training - Epoch   4, Step:     7700, Batch Loss:     0.602778, Batch Acc: 0.003410, Tokens per Sec:       74, Lr: 0.000102\n",
      "2022-08-04 19:10:35,491 - INFO - joeynmt.training - Epoch   4, Step:     7800, Batch Loss:     0.544417, Batch Acc: 0.005086, Tokens per Sec:       60, Lr: 0.000101\n",
      "2022-08-04 19:33:02,390 - INFO - joeynmt.training - Epoch   4, Step:     7900, Batch Loss:     0.560565, Batch Acc: 0.004128, Tokens per Sec:       71, Lr: 0.000101\n",
      "2022-08-04 19:38:49,701 - INFO - joeynmt.training - Epoch   4, Step:     8000, Batch Loss:     0.564635, Batch Acc: 0.005056, Tokens per Sec:      276, Lr: 0.000100\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 86.09ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 9660.76ex/s]\n",
      "2022-08-04 19:38:51,844 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=8000\n",
      "2022-08-04 19:38:51,844 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 19:39:57,001 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 19:39:57,001 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  14.58, loss:   3.02, ppl:  20.48, acc:   0.36, generation: 65.0899[sec], evaluation: 0.0600[sec]\n",
      "2022-08-04 19:39:57,002 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 19:39:58,650 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/6000.ckpt\n",
      "2022-08-04 19:39:58,739 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 19:39:58,746 - INFO - joeynmt.training - \tSource:     Then there is a problem...\n",
      "2022-08-04 19:39:58,763 - INFO - joeynmt.training - \tReference:  EntÃ£o hÃ¡ um problema...\n",
      "2022-08-04 19:39:58,763 - INFO - joeynmt.training - \tHypothesis: EntÃ£o hÃ¡ Ã© um problema.\n",
      "2022-08-04 19:39:58,763 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 19:39:58,770 - INFO - joeynmt.training - \tSource:     Whenever I find something I like, it's too expensive.\n",
      "2022-08-04 19:39:58,774 - INFO - joeynmt.training - \tReference:  Sempre quando eu acho algo que me agrade, este algo Ã© caro demais.\n",
      "2022-08-04 19:39:58,775 - INFO - joeynmt.training - \tHypothesis: Sempre eu que algo algo eu eu Ã© caro caro caro\n",
      "2022-08-04 19:39:58,775 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 19:39:58,778 - INFO - joeynmt.training - \tSource:     Don't expect others to think for you!\n",
      "2022-08-04 19:39:58,785 - INFO - joeynmt.training - \tReference:  NÃ£o espere que as outras pessoas pensem por vocÃª.\n",
      "2022-08-04 19:39:58,785 - INFO - joeynmt.training - \tHypothesis: NÃ£o espere todos outros que que vocÃª!\n",
      "2022-08-04 19:39:58,785 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 19:39:58,822 - INFO - joeynmt.training - \tSource:     I miss you.\n",
      "2022-08-04 19:39:58,822 - INFO - joeynmt.training - \tReference:  Tenho saudades tuas.\n",
      "2022-08-04 19:39:58,822 - INFO - joeynmt.training - \tHypothesis: Eu sinto saudade de..\n",
      "2022-08-04 19:45:40,303 - INFO - joeynmt.training - Epoch   4: total training loss 1196.93\n",
      "2022-08-04 19:45:40,303 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-08-04 19:47:41,949 - INFO - joeynmt.training - Epoch   5, Step:     8100, Batch Loss:     0.657506, Batch Acc: 0.012561, Tokens per Sec:      257, Lr: 0.000099\n",
      "2022-08-04 20:08:32,696 - INFO - joeynmt.training - Epoch   5, Step:     8200, Batch Loss:     0.495564, Batch Acc: 0.005635, Tokens per Sec:       75, Lr: 0.000099\n",
      "2022-08-04 20:33:16,581 - INFO - joeynmt.training - Epoch   5, Step:     8300, Batch Loss:     0.533684, Batch Acc: 0.004940, Tokens per Sec:       63, Lr: 0.000098\n",
      "2022-08-04 20:56:27,414 - INFO - joeynmt.training - Epoch   5, Step:     8400, Batch Loss:     0.553876, Batch Acc: 0.004708, Tokens per Sec:       69, Lr: 0.000098\n",
      "2022-08-04 21:06:34,400 - INFO - joeynmt.training - Epoch   5, Step:     8500, Batch Loss:     0.639673, Batch Acc: 0.004712, Tokens per Sec:      155, Lr: 0.000097\n",
      "2022-08-04 21:10:10,138 - INFO - joeynmt.training - Epoch   5, Step:     8600, Batch Loss:     0.554682, Batch Acc: 0.004463, Tokens per Sec:      443, Lr: 0.000096\n",
      "2022-08-04 21:13:45,945 - INFO - joeynmt.training - Epoch   5, Step:     8700, Batch Loss:     0.547049, Batch Acc: 0.005075, Tokens per Sec:      437, Lr: 0.000096\n",
      "2022-08-04 21:17:21,966 - INFO - joeynmt.training - Epoch   5, Step:     8800, Batch Loss:     0.544800, Batch Acc: 0.004873, Tokens per Sec:      438, Lr: 0.000095\n",
      "2022-08-04 21:20:58,633 - INFO - joeynmt.training - Epoch   5, Step:     8900, Batch Loss:     0.589179, Batch Acc: 0.004406, Tokens per Sec:      438, Lr: 0.000095\n",
      "2022-08-04 21:24:38,499 - INFO - joeynmt.training - Epoch   5, Step:     9000, Batch Loss:     0.515007, Batch Acc: 0.004587, Tokens per Sec:      430, Lr: 0.000094\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 174.59ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 16916.88ex/s]\n",
      "2022-08-04 21:24:39,221 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=9000\n",
      "2022-08-04 21:24:39,221 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 21:24:57,089 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 21:24:57,089 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  14.89, loss:   3.08, ppl:  21.79, acc:   0.35, generation: 17.8462[sec], evaluation: 0.0167[sec]\n",
      "2022-08-04 21:24:57,089 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 21:24:57,398 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/5000.ckpt\n",
      "2022-08-04 21:24:57,415 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 21:24:57,416 - INFO - joeynmt.training - \tSource:     This is my friend Rachel. We went to high school together.\n",
      "2022-08-04 21:24:57,416 - INFO - joeynmt.training - \tReference:  Essa Ã© minha amiga Rachel, nÃ³s fomos juntos ao colÃ©gio.\n",
      "2022-08-04 21:24:57,416 - INFO - joeynmt.training - \tHypothesis: Este Ã© meu amigo amigo.. NÃ³s a a a escola..\n",
      "2022-08-04 21:24:57,416 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 21:24:57,418 - INFO - joeynmt.training - \tSource:     Where are the showers?\n",
      "2022-08-04 21:24:57,418 - INFO - joeynmt.training - \tReference:  Onde estÃ£o as duchas ?\n",
      "2022-08-04 21:24:57,418 - INFO - joeynmt.training - \tHypothesis: Onde estÃ£o os banho?\n",
      "2022-08-04 21:24:57,418 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 21:24:57,419 - INFO - joeynmt.training - \tSource:     There's a problem there that you don't see.\n",
      "2022-08-04 21:24:57,419 - INFO - joeynmt.training - \tReference:  AÃ­ hÃ¡ um problema que vocÃª nÃ£o estÃ¡ vendo.\n",
      "2022-08-04 21:24:57,419 - INFO - joeynmt.training - \tHypothesis: HÃ¡ um problema que que vocÃª nÃ£o vÃª.\n",
      "2022-08-04 21:24:57,419 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 21:24:57,420 - INFO - joeynmt.training - \tSource:     I thought you liked to learn new things.\n",
      "2022-08-04 21:24:57,421 - INFO - joeynmt.training - \tReference:  Eu pensei que o senhor gostasse de aprender coisas novas.\n",
      "2022-08-04 21:24:57,421 - INFO - joeynmt.training - \tHypothesis: Eu pensei que vocÃª gostava de aprender novas coisas.\n",
      "2022-08-04 21:28:37,552 - INFO - joeynmt.training - Epoch   5, Step:     9100, Batch Loss:     0.570200, Batch Acc: 0.004691, Tokens per Sec:      432, Lr: 0.000094\n",
      "2022-08-04 21:33:20,901 - INFO - joeynmt.training - Epoch   5, Step:     9200, Batch Loss:     0.480789, Batch Acc: 0.005429, Tokens per Sec:      328, Lr: 0.000093\n",
      "2022-08-04 21:36:53,623 - INFO - joeynmt.training - Epoch   5, Step:     9300, Batch Loss:     0.494128, Batch Acc: 0.004628, Tokens per Sec:      448, Lr: 0.000093\n",
      "2022-08-04 21:40:30,986 - INFO - joeynmt.training - Epoch   5, Step:     9400, Batch Loss:     0.543420, Batch Acc: 0.004544, Tokens per Sec:      434, Lr: 0.000092\n",
      "2022-08-04 21:44:09,436 - INFO - joeynmt.training - Epoch   5, Step:     9500, Batch Loss:     0.517272, Batch Acc: 0.005744, Tokens per Sec:      436, Lr: 0.000092\n",
      "2022-08-04 21:47:51,602 - INFO - joeynmt.training - Epoch   5, Step:     9600, Batch Loss:     0.563998, Batch Acc: 0.004845, Tokens per Sec:      425, Lr: 0.000091\n",
      "2022-08-04 21:51:27,573 - INFO - joeynmt.training - Epoch   5, Step:     9700, Batch Loss:     0.501229, Batch Acc: 0.005227, Tokens per Sec:      443, Lr: 0.000091\n",
      "2022-08-04 21:55:07,420 - INFO - joeynmt.training - Epoch   5, Step:     9800, Batch Loss:     0.593314, Batch Acc: 0.004223, Tokens per Sec:      427, Lr: 0.000090\n",
      "2022-08-04 21:58:43,419 - INFO - joeynmt.training - Epoch   5, Step:     9900, Batch Loss:     0.489767, Batch Acc: 0.005000, Tokens per Sec:      441, Lr: 0.000090\n",
      "2022-08-04 22:02:15,632 - INFO - joeynmt.training - Epoch   5, Step:    10000, Batch Loss:     0.564798, Batch Acc: 0.004285, Tokens per Sec:      454, Lr: 0.000089\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 172.90ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17475.25ex/s]\n",
      "2022-08-04 22:02:16,345 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=10000\n",
      "2022-08-04 22:02:16,345 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 22:02:34,206 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 22:02:34,206 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.29, loss:   2.94, ppl:  18.93, acc:   0.37, generation: 17.8344[sec], evaluation: 0.0173[sec]\n",
      "2022-08-04 22:02:34,206 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 22:02:34,517 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/7000.ckpt\n",
      "2022-08-04 22:02:34,532 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 22:02:34,534 - INFO - joeynmt.training - \tSource:     Sometimes he can be a strange guy.\n",
      "2022-08-04 22:02:34,534 - INFO - joeynmt.training - \tReference:  Ã€s vezes ele pode ser um tipo estranho.\n",
      "2022-08-04 22:02:34,534 - INFO - joeynmt.training - \tHypothesis: Ã€s vezes ele ser ser cara estranho.\n",
      "2022-08-04 22:02:34,534 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 22:02:34,535 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-04 22:02:34,535 - INFO - joeynmt.training - \tReference:  VÃ³s me olvidareis um dia.\n",
      "2022-08-04 22:02:34,535 - INFO - joeynmt.training - \tHypothesis: VocÃª vai esquecer de mim um dia dia.\n",
      "2022-08-04 22:02:34,535 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 22:02:34,537 - INFO - joeynmt.training - \tSource:     Would you like something to drink?\n",
      "2022-08-04 22:02:34,537 - INFO - joeynmt.training - \tReference:  VocÃª gostaria de algo para beber?\n",
      "2022-08-04 22:02:34,537 - INFO - joeynmt.training - \tHypothesis: VocÃª quer alguma para beber?\n",
      "2022-08-04 22:02:34,537 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 22:02:34,538 - INFO - joeynmt.training - \tSource:     I have a headache.\n",
      "2022-08-04 22:02:34,538 - INFO - joeynmt.training - \tReference:  Eu estou com dor de cabeÃ§a.\n",
      "2022-08-04 22:02:34,538 - INFO - joeynmt.training - \tHypothesis: Estou com dor dor de cabeÃ§a.\n",
      "2022-08-04 22:05:31,672 - INFO - joeynmt.training - Epoch   5: total training loss 1109.40\n",
      "2022-08-04 22:05:31,673 - INFO - joeynmt.training - EPOCH 6\n",
      "2022-08-04 22:06:08,506 - INFO - joeynmt.training - Epoch   6, Step:    10100, Batch Loss:     0.498072, Batch Acc: 0.030569, Tokens per Sec:      430, Lr: 0.000089\n",
      "2022-08-04 22:10:48,210 - INFO - joeynmt.training - Epoch   6, Step:    10200, Batch Loss:     0.576328, Batch Acc: 0.003511, Tokens per Sec:      341, Lr: 0.000089\n",
      "2022-08-04 22:16:04,596 - INFO - joeynmt.training - Epoch   6, Step:    10300, Batch Loss:     0.489578, Batch Acc: 0.004722, Tokens per Sec:      300, Lr: 0.000088\n",
      "2022-08-04 22:21:31,455 - INFO - joeynmt.training - Epoch   6, Step:    10400, Batch Loss:     0.478485, Batch Acc: 0.005210, Tokens per Sec:      290, Lr: 0.000088\n",
      "2022-08-04 22:26:12,270 - INFO - joeynmt.training - Epoch   6, Step:    10500, Batch Loss:     0.530199, Batch Acc: 0.004749, Tokens per Sec:      340, Lr: 0.000087\n",
      "2022-08-04 22:30:54,170 - INFO - joeynmt.training - Epoch   6, Step:    10600, Batch Loss:     0.463765, Batch Acc: 0.005302, Tokens per Sec:      336, Lr: 0.000087\n",
      "2022-08-04 22:35:16,443 - INFO - joeynmt.training - Epoch   6, Step:    10700, Batch Loss:     0.509667, Batch Acc: 0.005069, Tokens per Sec:      366, Lr: 0.000086\n",
      "2022-08-04 22:38:53,889 - INFO - joeynmt.training - Epoch   6, Step:    10800, Batch Loss:     0.496659, Batch Acc: 0.005502, Tokens per Sec:      436, Lr: 0.000086\n",
      "2022-08-04 22:42:32,377 - INFO - joeynmt.training - Epoch   6, Step:    10900, Batch Loss:     0.498840, Batch Acc: 0.004755, Tokens per Sec:      433, Lr: 0.000086\n",
      "2022-08-04 22:46:06,920 - INFO - joeynmt.training - Epoch   6, Step:    11000, Batch Loss:     0.545903, Batch Acc: 0.005280, Tokens per Sec:      442, Lr: 0.000085\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 164.52ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17160.02ex/s]\n",
      "2022-08-04 22:46:07,639 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=11000\n",
      "2022-08-04 22:46:07,640 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 22:46:25,056 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 22:46:25,057 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.34, loss:   2.90, ppl:  18.12, acc:   0.38, generation: 17.3957[sec], evaluation: 0.0166[sec]\n",
      "2022-08-04 22:46:25,057 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-04 22:46:25,364 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/8000.ckpt\n",
      "2022-08-04 22:46:25,391 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 22:46:25,393 - INFO - joeynmt.training - \tSource:     You'll forget about me someday.\n",
      "2022-08-04 22:46:25,393 - INFO - joeynmt.training - \tReference:  VocÃª vai me esquecer um dia.\n",
      "2022-08-04 22:46:25,393 - INFO - joeynmt.training - \tHypothesis: VocÃª vai esquecer de de um um dia.\n",
      "2022-08-04 22:46:25,393 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 22:46:25,394 - INFO - joeynmt.training - \tSource:     Why don't you come visit us?\n",
      "2022-08-04 22:46:25,394 - INFO - joeynmt.training - \tReference:  Por que vocÃª nÃ£o nos faz uma visita?\n",
      "2022-08-04 22:46:25,394 - INFO - joeynmt.training - \tHypothesis: Por que vocÃª nÃ£o nos visitar?\n",
      "2022-08-04 22:46:25,394 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 22:46:25,396 - INFO - joeynmt.training - \tSource:     You are in my way.\n",
      "2022-08-04 22:46:25,396 - INFO - joeynmt.training - \tReference:  A senhora estÃ¡ a impedir-me a passagem.\n",
      "2022-08-04 22:46:25,396 - INFO - joeynmt.training - \tHypothesis: VocÃª estÃ¡ no meu caminho.\n",
      "2022-08-04 22:46:25,396 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 22:46:25,397 - INFO - joeynmt.training - \tSource:     I may give up soon and just nap instead.\n",
      "2022-08-04 22:46:25,397 - INFO - joeynmt.training - \tReference:  Pode ser que eu desista em breve e, em vez disso, tire uma soneca.\n",
      "2022-08-04 22:46:25,397 - INFO - joeynmt.training - \tHypothesis: Posso posso dar logo e e de de vez vez.\n",
      "2022-08-04 22:51:36,698 - INFO - joeynmt.training - Epoch   6, Step:    11100, Batch Loss:     0.504556, Batch Acc: 0.004502, Tokens per Sec:      305, Lr: 0.000085\n",
      "2022-08-04 22:56:58,045 - INFO - joeynmt.training - Epoch   6, Step:    11200, Batch Loss:     0.529509, Batch Acc: 0.004539, Tokens per Sec:      297, Lr: 0.000085\n",
      "2022-08-04 23:01:43,565 - INFO - joeynmt.training - Epoch   6, Step:    11300, Batch Loss:     0.481832, Batch Acc: 0.004305, Tokens per Sec:      334, Lr: 0.000084\n",
      "2022-08-04 23:06:31,901 - INFO - joeynmt.training - Epoch   6, Step:    11400, Batch Loss:     0.507815, Batch Acc: 0.005404, Tokens per Sec:      329, Lr: 0.000084\n",
      "2022-08-04 23:11:16,712 - INFO - joeynmt.training - Epoch   6, Step:    11500, Batch Loss:     0.559940, Batch Acc: 0.004847, Tokens per Sec:      332, Lr: 0.000083\n",
      "2022-08-04 23:16:04,383 - INFO - joeynmt.training - Epoch   6, Step:    11600, Batch Loss:     0.524235, Batch Acc: 0.004662, Tokens per Sec:      327, Lr: 0.000083\n",
      "2022-08-04 23:19:50,348 - INFO - joeynmt.training - Epoch   6, Step:    11700, Batch Loss:     0.497820, Batch Acc: 0.005258, Tokens per Sec:      417, Lr: 0.000083\n",
      "2022-08-04 23:23:31,522 - INFO - joeynmt.training - Epoch   6, Step:    11800, Batch Loss:     0.510621, Batch Acc: 0.004332, Tokens per Sec:      421, Lr: 0.000082\n",
      "2022-08-04 23:27:12,848 - INFO - joeynmt.training - Epoch   6, Step:    11900, Batch Loss:     0.473866, Batch Acc: 0.005591, Tokens per Sec:      427, Lr: 0.000082\n",
      "2022-08-04 23:30:47,674 - INFO - joeynmt.training - Epoch   6, Step:    12000, Batch Loss:     0.545769, Batch Acc: 0.004685, Tokens per Sec:      440, Lr: 0.000082\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 167.54ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17351.19ex/s]\n",
      "2022-08-04 23:30:48,402 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=12000\n",
      "2022-08-04 23:30:48,402 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-04 23:31:09,825 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-04 23:31:09,825 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.01, loss:   3.02, ppl:  20.43, acc:   0.35, generation: 21.3986[sec], evaluation: 0.0182[sec]\n",
      "2022-08-04 23:31:10,131 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/9000.ckpt\n",
      "2022-08-04 23:31:10,147 - INFO - joeynmt.training - Example #0\n",
      "2022-08-04 23:31:10,148 - INFO - joeynmt.training - \tSource:     But the universe is infinite.\n",
      "2022-08-04 23:31:10,148 - INFO - joeynmt.training - \tReference:  Mas o Universo Ã© infinito.\n",
      "2022-08-04 23:31:10,148 - INFO - joeynmt.training - \tHypothesis: Mas o universo Ã© infinito.\n",
      "2022-08-04 23:31:10,148 - INFO - joeynmt.training - Example #1\n",
      "2022-08-04 23:31:10,150 - INFO - joeynmt.training - \tSource:     It's a pity when somebody dies.\n",
      "2022-08-04 23:31:10,150 - INFO - joeynmt.training - \tReference:  Ã‰ uma pena quando alguÃ©m morre.\n",
      "2022-08-04 23:31:10,150 - INFO - joeynmt.training - \tHypothesis: Ã‰ uma pena quando alguÃ©m..\n",
      "2022-08-04 23:31:10,150 - INFO - joeynmt.training - Example #2\n",
      "2022-08-04 23:31:10,151 - INFO - joeynmt.training - \tSource:     What do you want?\n",
      "2022-08-04 23:31:10,151 - INFO - joeynmt.training - \tReference:  Que deseja o senhor?\n",
      "2022-08-04 23:31:10,151 - INFO - joeynmt.training - \tHypothesis: O que vocÃª quer?\n",
      "2022-08-04 23:31:10,151 - INFO - joeynmt.training - Example #3\n",
      "2022-08-04 23:31:10,152 - INFO - joeynmt.training - \tSource:     I thought you liked to learn new things.\n",
      "2022-08-04 23:31:10,152 - INFO - joeynmt.training - \tReference:  Eu pensava que tu gostavas de aprender coisas novas.\n",
      "2022-08-04 23:31:10,152 - INFO - joeynmt.training - \tHypothesis: Pensei que vocÃª gostava de aprender coisas.\n",
      "2022-08-04 23:34:40,237 - INFO - joeynmt.training - Epoch   6: total training loss 1047.65\n",
      "2022-08-04 23:34:40,237 - INFO - joeynmt.training - EPOCH 7\n",
      "2022-08-04 23:34:44,798 - INFO - joeynmt.training - Epoch   7, Step:    12100, Batch Loss:     0.488425, Batch Acc: 0.223329, Tokens per Sec:      374, Lr: 0.000081\n",
      "2022-08-04 23:38:19,155 - INFO - joeynmt.training - Epoch   7, Step:    12200, Batch Loss:     0.483781, Batch Acc: 0.005469, Tokens per Sec:      442, Lr: 0.000081\n",
      "2022-08-04 23:41:52,400 - INFO - joeynmt.training - Epoch   7, Step:    12300, Batch Loss:     0.511114, Batch Acc: 0.004775, Tokens per Sec:      448, Lr: 0.000081\n",
      "2022-08-04 23:45:25,738 - INFO - joeynmt.training - Epoch   7, Step:    12400, Batch Loss:     0.486489, Batch Acc: 0.004774, Tokens per Sec:      444, Lr: 0.000080\n",
      "2022-08-04 23:49:04,046 - INFO - joeynmt.training - Epoch   7, Step:    12500, Batch Loss:     0.439249, Batch Acc: 0.005580, Tokens per Sec:      436, Lr: 0.000080\n",
      "2022-08-04 23:52:45,110 - INFO - joeynmt.training - Epoch   7, Step:    12600, Batch Loss:     0.481114, Batch Acc: 0.005652, Tokens per Sec:      429, Lr: 0.000080\n",
      "2022-08-04 23:56:28,041 - INFO - joeynmt.training - Epoch   7, Step:    12700, Batch Loss:     0.501722, Batch Acc: 0.004149, Tokens per Sec:      422, Lr: 0.000079\n",
      "2022-08-05 00:00:01,530 - INFO - joeynmt.training - Epoch   7, Step:    12800, Batch Loss:     0.490860, Batch Acc: 0.004280, Tokens per Sec:      448, Lr: 0.000079\n",
      "2022-08-05 00:03:36,627 - INFO - joeynmt.training - Epoch   7, Step:    12900, Batch Loss:     0.483938, Batch Acc: 0.005299, Tokens per Sec:      446, Lr: 0.000079\n",
      "2022-08-05 00:07:12,427 - INFO - joeynmt.training - Epoch   7, Step:    13000, Batch Loss:     0.439765, Batch Acc: 0.006181, Tokens per Sec:      442, Lr: 0.000078\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 152.86ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 16987.25ex/s]\n",
      "2022-08-05 00:07:13,155 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=13000\n",
      "2022-08-05 00:07:13,155 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 00:07:29,685 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 00:07:29,685 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.97, loss:   3.14, ppl:  23.22, acc:   0.35, generation: 16.5023[sec], evaluation: 0.0179[sec]\n",
      "2022-08-05 00:07:29,686 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 00:07:29,687 - INFO - joeynmt.training - \tSource:     Rye was called the grain of poverty.\n",
      "2022-08-05 00:07:29,687 - INFO - joeynmt.training - \tReference:  O centeio foi chamado de grÃ£o da pobreza.\n",
      "2022-08-05 00:07:29,687 - INFO - joeynmt.training - \tHypothesis: Oa foi o os de da.\n",
      "2022-08-05 00:07:29,687 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 00:07:29,689 - INFO - joeynmt.training - \tSource:     You're so impatient with me.\n",
      "2022-08-05 00:07:29,689 - INFO - joeynmt.training - \tReference:  VocÃª Ã© tÃ£o impaciente comigo.\n",
      "2022-08-05 00:07:29,689 - INFO - joeynmt.training - \tHypothesis: VocÃª estÃ¡ tÃ£o impaciente comigo.\n",
      "2022-08-05 00:07:29,689 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 00:07:29,690 - INFO - joeynmt.training - \tSource:     How could I be a robot? Robots don't dream.\n",
      "2022-08-05 00:07:29,690 - INFO - joeynmt.training - \tReference:  Como eu poderia ser um robÃ´? RobÃ´s nÃ£o sonham.\n",
      "2022-08-05 00:07:29,690 - INFO - joeynmt.training - \tHypothesis: Como eu poderia ser um??ss nÃ£o nÃ£o..\n",
      "2022-08-05 00:07:29,690 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 00:07:29,692 - INFO - joeynmt.training - \tSource:     You're not fast enough.\n",
      "2022-08-05 00:07:29,692 - INFO - joeynmt.training - \tReference:  VocÃª nÃ£o Ã© rÃ¡pido o bastante.\n",
      "2022-08-05 00:07:29,692 - INFO - joeynmt.training - \tHypothesis: VocÃª nÃ£o Ã© o suficiente.\n",
      "2022-08-05 00:11:11,679 - INFO - joeynmt.training - Epoch   7, Step:    13100, Batch Loss:     0.776711, Batch Acc: 0.002203, Tokens per Sec:      422, Lr: 0.000078\n",
      "2022-08-05 00:14:43,181 - INFO - joeynmt.training - Epoch   7, Step:    13200, Batch Loss:     0.546650, Batch Acc: 0.004672, Tokens per Sec:      452, Lr: 0.000078\n",
      "2022-08-05 00:18:23,474 - INFO - joeynmt.training - Epoch   7, Step:    13300, Batch Loss:     0.560419, Batch Acc: 0.003793, Tokens per Sec:      425, Lr: 0.000078\n",
      "2022-08-05 00:22:00,358 - INFO - joeynmt.training - Epoch   7, Step:    13400, Batch Loss:     0.485748, Batch Acc: 0.005250, Tokens per Sec:      438, Lr: 0.000077\n",
      "2022-08-05 00:25:42,899 - INFO - joeynmt.training - Epoch   7, Step:    13500, Batch Loss:     0.491097, Batch Acc: 0.005163, Tokens per Sec:      422, Lr: 0.000077\n",
      "2022-08-05 00:29:27,128 - INFO - joeynmt.training - Epoch   7, Step:    13600, Batch Loss:     0.478742, Batch Acc: 0.005413, Tokens per Sec:      424, Lr: 0.000077\n",
      "2022-08-05 00:33:08,873 - INFO - joeynmt.training - Epoch   7, Step:    13700, Batch Loss:     0.563320, Batch Acc: 0.004275, Tokens per Sec:      423, Lr: 0.000076\n",
      "2022-08-05 00:36:39,143 - INFO - joeynmt.training - Epoch   7, Step:    13800, Batch Loss:     0.545487, Batch Acc: 0.004228, Tokens per Sec:      453, Lr: 0.000076\n",
      "2022-08-05 00:40:16,857 - INFO - joeynmt.training - Epoch   7, Step:    13900, Batch Loss:     0.474827, Batch Acc: 0.005612, Tokens per Sec:      431, Lr: 0.000076\n",
      "2022-08-05 00:43:48,988 - INFO - joeynmt.training - Epoch   7, Step:    14000, Batch Loss:     0.446324, Batch Acc: 0.005354, Tokens per Sec:      450, Lr: 0.000076\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 169.47ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17489.38ex/s]\n",
      "2022-08-05 00:43:49,706 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=14000\n",
      "2022-08-05 00:43:49,706 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 00:44:07,383 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 00:44:07,384 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.32, loss:   2.98, ppl:  19.78, acc:   0.37, generation: 17.6535[sec], evaluation: 0.0173[sec]\n",
      "2022-08-05 00:44:07,691 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/10000.ckpt\n",
      "2022-08-05 00:44:07,708 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 00:44:07,709 - INFO - joeynmt.training - \tSource:     What do you think I've been doing?\n",
      "2022-08-05 00:44:07,709 - INFO - joeynmt.training - \tReference:  Que Ã© que tu achas que eu andava fazendo?\n",
      "2022-08-05 00:44:07,709 - INFO - joeynmt.training - \tHypothesis: O que vocÃª acha que eu estava fazendo?\n",
      "2022-08-05 00:44:07,710 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 00:44:07,711 - INFO - joeynmt.training - \tSource:     Nowadays we want our children to make their own decisions, but we expect those decisions to please us.\n",
      "2022-08-05 00:44:07,711 - INFO - joeynmt.training - \tReference:  Atualmente nÃ³s queremos que nossos filhos tomem suas prÃ³prias decisÃµes, mas nÃ³s esperamos que essas decisÃµes nos agradem.\n",
      "2022-08-05 00:44:07,711 - INFO - joeynmt.training - \tHypothesis: Hoje dia queremos que nossas nossas nossas nossas suas suas suas,,,,, todas as as as as as...\n",
      "2022-08-05 00:44:07,711 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 00:44:07,712 - INFO - joeynmt.training - \tSource:     Why do you ask?\n",
      "2022-08-05 00:44:07,712 - INFO - joeynmt.training - \tReference:  Por que vocÃª estÃ¡ perguntando?\n",
      "2022-08-05 00:44:07,712 - INFO - joeynmt.training - \tHypothesis: Por que vocÃª??\n",
      "2022-08-05 00:44:07,712 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 00:44:07,714 - INFO - joeynmt.training - \tSource:     How many close friends do you have?\n",
      "2022-08-05 00:44:07,714 - INFO - joeynmt.training - \tReference:  Quantos amigos prÃ³ximos vÃ³s tendes?\n",
      "2022-08-05 00:44:07,714 - INFO - joeynmt.training - \tHypothesis: Quantos amigos amigos vocÃª tem?\n",
      "2022-08-05 00:47:44,604 - INFO - joeynmt.training - Epoch   7, Step:    14100, Batch Loss:     0.436777, Batch Acc: 0.005666, Tokens per Sec:      438, Lr: 0.000075\n",
      "2022-08-05 00:48:15,917 - INFO - joeynmt.training - Epoch   7: total training loss 1001.56\n",
      "2022-08-05 00:48:15,918 - INFO - joeynmt.training - EPOCH 8\n",
      "2022-08-05 00:51:22,447 - INFO - joeynmt.training - Epoch   8, Step:    14200, Batch Loss:     0.459167, Batch Acc: 0.005650, Tokens per Sec:      436, Lr: 0.000075\n",
      "2022-08-05 00:55:06,831 - INFO - joeynmt.training - Epoch   8, Step:    14300, Batch Loss:     0.443082, Batch Acc: 0.005172, Tokens per Sec:      419, Lr: 0.000075\n",
      "2022-08-05 00:58:43,927 - INFO - joeynmt.training - Epoch   8, Step:    14400, Batch Loss:     0.454191, Batch Acc: 0.005690, Tokens per Sec:      436, Lr: 0.000075\n",
      "2022-08-05 01:02:11,358 - INFO - joeynmt.training - Epoch   8, Step:    14500, Batch Loss:     0.462184, Batch Acc: 0.005221, Tokens per Sec:      461, Lr: 0.000074\n",
      "2022-08-05 01:05:45,045 - INFO - joeynmt.training - Epoch   8, Step:    14600, Batch Loss:     0.464096, Batch Acc: 0.004874, Tokens per Sec:      447, Lr: 0.000074\n",
      "2022-08-05 01:09:19,919 - INFO - joeynmt.training - Epoch   8, Step:    14700, Batch Loss:     0.481783, Batch Acc: 0.005101, Tokens per Sec:      443, Lr: 0.000074\n",
      "2022-08-05 01:12:55,464 - INFO - joeynmt.training - Epoch   8, Step:    14800, Batch Loss:     0.453716, Batch Acc: 0.005859, Tokens per Sec:      444, Lr: 0.000074\n",
      "2022-08-05 01:16:35,660 - INFO - joeynmt.training - Epoch   8, Step:    14900, Batch Loss:     0.480411, Batch Acc: 0.004817, Tokens per Sec:      429, Lr: 0.000073\n",
      "2022-08-05 01:20:12,058 - INFO - joeynmt.training - Epoch   8, Step:    15000, Batch Loss:     0.443501, Batch Acc: 0.005617, Tokens per Sec:      440, Lr: 0.000073\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 172.65ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17381.39ex/s]\n",
      "2022-08-05 01:20:12,777 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=15000\n",
      "2022-08-05 01:20:12,777 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 01:20:30,456 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 01:20:30,457 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.05, loss:   2.94, ppl:  18.99, acc:   0.39, generation: 17.6543[sec], evaluation: 0.0177[sec]\n",
      "2022-08-05 01:20:30,768 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/14000.ckpt\n",
      "2022-08-05 01:20:30,783 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/14000.ckpt\n",
      "2022-08-05 01:20:30,783 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lconti/en-pt_tatoeba/models/no_tf/14000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lconti/en-pt_tatoeba/models/no_tf/14000.ckpt')\n",
      "2022-08-05 01:20:30,783 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 01:20:30,785 - INFO - joeynmt.training - \tSource:     What do you think I've been doing?\n",
      "2022-08-05 01:20:30,785 - INFO - joeynmt.training - \tReference:  O que vocÃª acha que eu ando fazendo?\n",
      "2022-08-05 01:20:30,785 - INFO - joeynmt.training - \tHypothesis: O que vocÃª acha que eu estava fazendo?\n",
      "2022-08-05 01:20:30,785 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 01:20:30,786 - INFO - joeynmt.training - \tSource:     My shoes are too small. I need new ones.\n",
      "2022-08-05 01:20:30,786 - INFO - joeynmt.training - \tReference:  Meus sapatos estÃ£o muito pequenos. Preciso de novos.\n",
      "2022-08-05 01:20:30,786 - INFO - joeynmt.training - \tHypothesis: Meus sapatos sÃ£o muito demais. Preciso preciso de novo.\n",
      "2022-08-05 01:20:30,786 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 01:20:30,787 - INFO - joeynmt.training - \tSource:     Uh, now it's really weird...\n",
      "2022-08-05 01:20:30,787 - INFO - joeynmt.training - \tReference:  Hmm, agora estÃ¡ realmente estranho...\n",
      "2022-08-05 01:20:30,788 - INFO - joeynmt.training - \tHypothesis: Entre, agora Ã© realmente estranho.\n",
      "2022-08-05 01:20:30,788 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 01:20:30,789 - INFO - joeynmt.training - \tSource:     That wasn't my intention.\n",
      "2022-08-05 01:20:30,789 - INFO - joeynmt.training - \tReference:  NÃ£o era essa a minha intenÃ§Ã£o.\n",
      "2022-08-05 01:20:30,789 - INFO - joeynmt.training - \tHypothesis: Isso nÃ£o foi minha minha..\n",
      "2022-08-05 01:24:07,713 - INFO - joeynmt.training - Epoch   8, Step:    15100, Batch Loss:     0.458117, Batch Acc: 0.005643, Tokens per Sec:      436, Lr: 0.000073\n",
      "2022-08-05 01:27:46,803 - INFO - joeynmt.training - Epoch   8, Step:    15200, Batch Loss:     0.482642, Batch Acc: 0.005721, Tokens per Sec:      433, Lr: 0.000073\n",
      "2022-08-05 01:31:23,313 - INFO - joeynmt.training - Epoch   8, Step:    15300, Batch Loss:     0.471464, Batch Acc: 0.005665, Tokens per Sec:      437, Lr: 0.000072\n",
      "2022-08-05 01:35:00,267 - INFO - joeynmt.training - Epoch   8, Step:    15400, Batch Loss:     0.516102, Batch Acc: 0.004693, Tokens per Sec:      437, Lr: 0.000072\n",
      "2022-08-05 01:38:40,174 - INFO - joeynmt.training - Epoch   8, Step:    15500, Batch Loss:     0.515447, Batch Acc: 0.004317, Tokens per Sec:      429, Lr: 0.000072\n",
      "2022-08-05 01:42:19,901 - INFO - joeynmt.training - Epoch   8, Step:    15600, Batch Loss:     0.471903, Batch Acc: 0.005259, Tokens per Sec:      428, Lr: 0.000072\n",
      "2022-08-05 01:45:58,329 - INFO - joeynmt.training - Epoch   8, Step:    15700, Batch Loss:     0.554751, Batch Acc: 0.004323, Tokens per Sec:      438, Lr: 0.000071\n",
      "2022-08-05 01:49:38,178 - INFO - joeynmt.training - Epoch   8, Step:    15800, Batch Loss:     0.474869, Batch Acc: 0.004371, Tokens per Sec:      425, Lr: 0.000071\n",
      "2022-08-05 01:53:12,933 - INFO - joeynmt.training - Epoch   8, Step:    15900, Batch Loss:     0.546561, Batch Acc: 0.004705, Tokens per Sec:      447, Lr: 0.000071\n",
      "2022-08-05 01:56:46,596 - INFO - joeynmt.training - Epoch   8, Step:    16000, Batch Loss:     0.486844, Batch Acc: 0.005067, Tokens per Sec:      449, Lr: 0.000071\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 166.55ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17338.71ex/s]\n",
      "2022-08-05 01:56:47,312 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=16000\n",
      "2022-08-05 01:56:47,313 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 01:57:10,520 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 01:57:10,520 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.73, loss:   3.12, ppl:  22.67, acc:   0.36, generation: 23.1846[sec], evaluation: 0.0175[sec]\n",
      "2022-08-05 01:57:10,521 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 01:57:10,522 - INFO - joeynmt.training - \tSource:     Are you referring to me?\n",
      "2022-08-05 01:57:10,523 - INFO - joeynmt.training - \tReference:  VocÃª estÃ¡ falando de mim?\n",
      "2022-08-05 01:57:10,523 - INFO - joeynmt.training - \tHypothesis: VocÃª se me me??\n",
      "2022-08-05 01:57:10,523 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 01:57:10,524 - INFO - joeynmt.training - \tSource:     The French government has launched an online game that challenges taxpayers to balance the national budget.\n",
      "2022-08-05 01:57:10,524 - INFO - joeynmt.training - \tReference:  O governo francÃªs lanÃ§ou um jogo online que desafia o contribuinte a balancear o orÃ§amento nacional.\n",
      "2022-08-05 01:57:10,524 - INFO - joeynmt.training - \tHypothesis: O governo do tem um um um de de de de dettss para o o.....\n",
      "2022-08-05 01:57:10,524 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 01:57:10,526 - INFO - joeynmt.training - \tSource:     Why are you sorry for something you haven't done?\n",
      "2022-08-05 01:57:10,526 - INFO - joeynmt.training - \tReference:  Por que te estÃ¡s desculpando de algo que nÃ£o fizeste?\n",
      "2022-08-05 01:57:10,526 - INFO - joeynmt.training - \tHypothesis: Por que vocÃª se por algo que nÃ£o nÃ£o??\n",
      "2022-08-05 01:57:10,526 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 01:57:10,527 - INFO - joeynmt.training - \tSource:     Don't expect others to think for you!\n",
      "2022-08-05 01:57:10,527 - INFO - joeynmt.training - \tReference:  NÃ£o espere que as outras pessoas pensem por vocÃª.\n",
      "2022-08-05 01:57:10,527 - INFO - joeynmt.training - \tHypothesis: NÃ£o espere os outros que pensar vocÃª!\n",
      "2022-08-05 02:00:51,220 - INFO - joeynmt.training - Epoch   8, Step:    16100, Batch Loss:     0.536142, Batch Acc: 0.004026, Tokens per Sec:      430, Lr: 0.000070\n",
      "2022-08-05 02:01:52,698 - INFO - joeynmt.training - Epoch   8: total training loss 961.80\n",
      "2022-08-05 02:01:52,698 - INFO - joeynmt.training - EPOCH 9\n",
      "2022-08-05 02:04:28,365 - INFO - joeynmt.training - Epoch   9, Step:    16200, Batch Loss:     0.497494, Batch Acc: 0.007097, Tokens per Sec:      444, Lr: 0.000070\n",
      "2022-08-05 02:08:05,079 - INFO - joeynmt.training - Epoch   9, Step:    16300, Batch Loss:     0.470487, Batch Acc: 0.005594, Tokens per Sec:      440, Lr: 0.000070\n",
      "2022-08-05 02:11:42,105 - INFO - joeynmt.training - Epoch   9, Step:    16400, Batch Loss:     0.479687, Batch Acc: 0.004764, Tokens per Sec:      435, Lr: 0.000070\n",
      "2022-08-05 02:15:20,016 - INFO - joeynmt.training - Epoch   9, Step:    16500, Batch Loss:     0.448936, Batch Acc: 0.005272, Tokens per Sec:      434, Lr: 0.000070\n",
      "2022-08-05 02:19:04,564 - INFO - joeynmt.training - Epoch   9, Step:    16600, Batch Loss:     0.460459, Batch Acc: 0.005846, Tokens per Sec:      421, Lr: 0.000069\n",
      "2022-08-05 02:22:45,306 - INFO - joeynmt.training - Epoch   9, Step:    16700, Batch Loss:     0.463237, Batch Acc: 0.005005, Tokens per Sec:      428, Lr: 0.000069\n",
      "2022-08-05 02:26:21,568 - INFO - joeynmt.training - Epoch   9, Step:    16800, Batch Loss:     0.474850, Batch Acc: 0.004909, Tokens per Sec:      439, Lr: 0.000069\n",
      "2022-08-05 02:29:50,487 - INFO - joeynmt.training - Epoch   9, Step:    16900, Batch Loss:     0.451508, Batch Acc: 0.005231, Tokens per Sec:      459, Lr: 0.000069\n",
      "2022-08-05 02:33:38,574 - INFO - joeynmt.training - Epoch   9, Step:    17000, Batch Loss:     0.447090, Batch Acc: 0.005261, Tokens per Sec:      410, Lr: 0.000069\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 163.64ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17406.06ex/s]\n",
      "2022-08-05 02:33:39,290 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=17000\n",
      "2022-08-05 02:33:39,290 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 02:33:57,364 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 02:33:57,364 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  16.80, loss:   3.02, ppl:  20.47, acc:   0.34, generation: 18.0495[sec], evaluation: 0.0183[sec]\n",
      "2022-08-05 02:33:57,365 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 02:33:57,367 - INFO - joeynmt.training - \tSource:     What do you think I've been doing?\n",
      "2022-08-05 02:33:57,367 - INFO - joeynmt.training - \tReference:  Que Ã© que achais que eu estive a fazer?\n",
      "2022-08-05 02:33:57,367 - INFO - joeynmt.training - \tHypothesis: O que vocÃª acha que eu estava fazendo?\n",
      "2022-08-05 02:33:57,367 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 02:33:57,368 - INFO - joeynmt.training - \tSource:     I love you.\n",
      "2022-08-05 02:33:57,368 - INFO - joeynmt.training - \tReference:  Amo-a.\n",
      "2022-08-05 02:33:57,368 - INFO - joeynmt.training - \tHypothesis: Eu te amo.\n",
      "2022-08-05 02:33:57,368 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 02:33:57,370 - INFO - joeynmt.training - \tSource:     What do you mean you don't know?!\n",
      "2022-08-05 02:33:57,370 - INFO - joeynmt.training - \tReference:  Que tens em mira dizendo que nÃ£o sabes?\n",
      "2022-08-05 02:33:57,370 - INFO - joeynmt.training - \tHypothesis: O que vocÃª quer que vocÃª nÃ£o??\n",
      "2022-08-05 02:33:57,370 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 02:33:57,371 - INFO - joeynmt.training - \tSource:     You didn't tell him anything?\n",
      "2022-08-05 02:33:57,371 - INFO - joeynmt.training - \tReference:  VocÃª nÃ£o disse nada para ele?\n",
      "2022-08-05 02:33:57,371 - INFO - joeynmt.training - \tHypothesis: VocÃª nÃ£o disse nada nada?\n",
      "2022-08-05 02:37:40,756 - INFO - joeynmt.training - Epoch   9, Step:    17100, Batch Loss:     0.430697, Batch Acc: 0.006143, Tokens per Sec:      422, Lr: 0.000068\n",
      "2022-08-05 02:41:19,008 - INFO - joeynmt.training - Epoch   9, Step:    17200, Batch Loss:     0.488639, Batch Acc: 0.004588, Tokens per Sec:      432, Lr: 0.000068\n",
      "2022-08-05 02:44:56,988 - INFO - joeynmt.training - Epoch   9, Step:    17300, Batch Loss:     0.446939, Batch Acc: 0.005955, Tokens per Sec:      436, Lr: 0.000068\n",
      "2022-08-05 02:48:28,648 - INFO - joeynmt.training - Epoch   9, Step:    17400, Batch Loss:     0.377860, Batch Acc: 0.006077, Tokens per Sec:      452, Lr: 0.000068\n",
      "2022-08-05 02:51:59,397 - INFO - joeynmt.training - Epoch   9, Step:    17500, Batch Loss:     0.452892, Batch Acc: 0.006015, Tokens per Sec:      451, Lr: 0.000068\n",
      "2022-08-05 02:55:29,525 - INFO - joeynmt.training - Epoch   9, Step:    17600, Batch Loss:     0.417048, Batch Acc: 0.005848, Tokens per Sec:      455, Lr: 0.000067\n",
      "2022-08-05 02:59:03,531 - INFO - joeynmt.training - Epoch   9, Step:    17700, Batch Loss:     0.459923, Batch Acc: 0.005535, Tokens per Sec:      442, Lr: 0.000067\n",
      "2022-08-05 03:02:36,789 - INFO - joeynmt.training - Epoch   9, Step:    17800, Batch Loss:     0.428663, Batch Acc: 0.005624, Tokens per Sec:      444, Lr: 0.000067\n",
      "2022-08-05 03:06:16,501 - INFO - joeynmt.training - Epoch   9, Step:    17900, Batch Loss:     0.445717, Batch Acc: 0.005861, Tokens per Sec:      427, Lr: 0.000067\n",
      "2022-08-05 03:09:51,941 - INFO - joeynmt.training - Epoch   9, Step:    18000, Batch Loss:     0.436213, Batch Acc: 0.005351, Tokens per Sec:      443, Lr: 0.000067\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 168.81ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17464.70ex/s]\n",
      "2022-08-05 03:09:52,651 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=18000\n",
      "2022-08-05 03:09:52,651 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 03:10:08,118 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 03:10:08,119 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.25, loss:   2.94, ppl:  18.83, acc:   0.37, generation: 15.4459[sec], evaluation: 0.0168[sec]\n",
      "2022-08-05 03:10:08,119 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-08-05 03:10:08,426 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/12000.ckpt\n",
      "2022-08-05 03:10:08,441 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 03:10:08,443 - INFO - joeynmt.training - \tSource:     Open your mouth!\n",
      "2022-08-05 03:10:08,443 - INFO - joeynmt.training - \tReference:  Abra a sua boca!\n",
      "2022-08-05 03:10:08,443 - INFO - joeynmt.training - \tHypothesis: Abra a boca!\n",
      "2022-08-05 03:10:08,443 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 03:10:08,444 - INFO - joeynmt.training - \tSource:     What do you think I've been doing?\n",
      "2022-08-05 03:10:08,444 - INFO - joeynmt.training - \tReference:  O que vocÃª acha que eu estive fazendo?\n",
      "2022-08-05 03:10:08,444 - INFO - joeynmt.training - \tHypothesis: O que vocÃª acha que eu estava fazendo?\n",
      "2022-08-05 03:10:08,444 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 03:10:08,445 - INFO - joeynmt.training - \tSource:     A cubic meter corresponds to 1000 liters.\n",
      "2022-08-05 03:10:08,445 - INFO - joeynmt.training - \tReference:  Um metro cÃºbico corresponde a 1000 litros.\n",
      "2022-08-05 03:10:08,445 - INFO - joeynmt.training - \tHypothesis: Um dia dettts para de de..\n",
      "2022-08-05 03:10:08,445 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 03:10:08,446 - INFO - joeynmt.training - \tSource:     He plays the piano very well.\n",
      "2022-08-05 03:10:08,447 - INFO - joeynmt.training - \tReference:  Ele toca muito bem piano.\n",
      "2022-08-05 03:10:08,447 - INFO - joeynmt.training - \tHypothesis: Ele toca piano muito bem.\n",
      "2022-08-05 03:13:40,703 - INFO - joeynmt.training - Epoch   9, Step:    18100, Batch Loss:     0.477251, Batch Acc: 0.004651, Tokens per Sec:      450, Lr: 0.000066\n",
      "2022-08-05 03:15:10,678 - INFO - joeynmt.training - Epoch   9: total training loss 929.23\n",
      "2022-08-05 03:15:10,678 - INFO - joeynmt.training - EPOCH 10\n",
      "2022-08-05 03:17:16,002 - INFO - joeynmt.training - Epoch  10, Step:    18200, Batch Loss:     0.383814, Batch Acc: 0.008937, Tokens per Sec:      448, Lr: 0.000066\n",
      "2022-08-05 03:20:55,624 - INFO - joeynmt.training - Epoch  10, Step:    18300, Batch Loss:     0.513410, Batch Acc: 0.004764, Tokens per Sec:      429, Lr: 0.000066\n",
      "2022-08-05 03:24:35,445 - INFO - joeynmt.training - Epoch  10, Step:    18400, Batch Loss:     0.509621, Batch Acc: 0.004700, Tokens per Sec:      429, Lr: 0.000066\n",
      "2022-08-05 03:28:10,362 - INFO - joeynmt.training - Epoch  10, Step:    18500, Batch Loss:     0.393834, Batch Acc: 0.005770, Tokens per Sec:      444, Lr: 0.000066\n",
      "2022-08-05 03:31:48,028 - INFO - joeynmt.training - Epoch  10, Step:    18600, Batch Loss:     0.410896, Batch Acc: 0.005022, Tokens per Sec:      436, Lr: 0.000066\n",
      "2022-08-05 03:35:33,238 - INFO - joeynmt.training - Epoch  10, Step:    18700, Batch Loss:     0.470734, Batch Acc: 0.004432, Tokens per Sec:      420, Lr: 0.000065\n",
      "2022-08-05 03:39:04,450 - INFO - joeynmt.training - Epoch  10, Step:    18800, Batch Loss:     0.524886, Batch Acc: 0.004805, Tokens per Sec:      455, Lr: 0.000065\n",
      "2022-08-05 03:42:38,834 - INFO - joeynmt.training - Epoch  10, Step:    18900, Batch Loss:     0.488335, Batch Acc: 0.004148, Tokens per Sec:      445, Lr: 0.000065\n",
      "2022-08-05 03:46:10,715 - INFO - joeynmt.training - Epoch  10, Step:    19000, Batch Loss:     0.468760, Batch Acc: 0.004914, Tokens per Sec:      452, Lr: 0.000065\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 170.88ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17393.79ex/s]\n",
      "2022-08-05 03:46:11,430 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=19000\n",
      "2022-08-05 03:46:11,431 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 03:46:30,033 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 03:46:30,033 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.21, loss:   2.93, ppl:  18.72, acc:   0.36, generation: 18.5782[sec], evaluation: 0.0175[sec]\n",
      "2022-08-05 03:46:30,341 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/15000.ckpt\n",
      "2022-08-05 03:46:30,357 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 03:46:30,358 - INFO - joeynmt.training - \tSource:     Except that here, it's not so simple.\n",
      "2022-08-05 03:46:30,358 - INFO - joeynmt.training - \tReference:  Exceto que aqui nÃ£o Ã© tÃ£o simples assim.\n",
      "2022-08-05 03:46:30,358 - INFO - joeynmt.training - \tHypothesis: Exceto que aqui, nÃ£o tÃ£o simples simples.\n",
      "2022-08-05 03:46:30,358 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 03:46:30,360 - INFO - joeynmt.training - \tSource:     I can't tell her now. It's not that simple anymore.\n",
      "2022-08-05 03:46:30,360 - INFO - joeynmt.training - \tReference:  Eu nÃ£o posso contar a ela agora. NÃ£o Ã© mais tÃ£o simples.\n",
      "2022-08-05 03:46:30,360 - INFO - joeynmt.training - \tHypothesis: NÃ£o posso posso contar ela agora. NÃ£o nÃ£o simples simples simples simples\n",
      "2022-08-05 03:46:30,360 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 03:46:30,361 - INFO - joeynmt.training - \tSource:     Nothing is achieved without effort.\n",
      "2022-08-05 03:46:30,361 - INFO - joeynmt.training - \tReference:  Nada se consegue sem esforÃ§o.\n",
      "2022-08-05 03:46:30,361 - INFO - joeynmt.training - \tHypothesis: Nada Ã© em sem esforÃ§o.\n",
      "2022-08-05 03:46:30,361 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 03:46:30,362 - INFO - joeynmt.training - \tSource:     I love you.\n",
      "2022-08-05 03:46:30,362 - INFO - joeynmt.training - \tReference:  Te amo.\n",
      "2022-08-05 03:46:30,362 - INFO - joeynmt.training - \tHypothesis: Eu te amo.\n",
      "2022-08-05 03:50:03,906 - INFO - joeynmt.training - Epoch  10, Step:    19100, Batch Loss:     0.462854, Batch Acc: 0.005201, Tokens per Sec:      442, Lr: 0.000065\n",
      "2022-08-05 03:53:40,087 - INFO - joeynmt.training - Epoch  10, Step:    19200, Batch Loss:     0.461998, Batch Acc: 0.004576, Tokens per Sec:      438, Lr: 0.000065\n",
      "2022-08-05 03:57:15,852 - INFO - joeynmt.training - Epoch  10, Step:    19300, Batch Loss:     0.395270, Batch Acc: 0.005569, Tokens per Sec:      442, Lr: 0.000064\n",
      "2022-08-05 04:00:44,794 - INFO - joeynmt.training - Epoch  10, Step:    19400, Batch Loss:     0.439898, Batch Acc: 0.004767, Tokens per Sec:      455, Lr: 0.000064\n",
      "2022-08-05 04:04:21,745 - INFO - joeynmt.training - Epoch  10, Step:    19500, Batch Loss:     0.445078, Batch Acc: 0.005403, Tokens per Sec:      433, Lr: 0.000064\n",
      "2022-08-05 04:08:04,274 - INFO - joeynmt.training - Epoch  10, Step:    19600, Batch Loss:     0.534704, Batch Acc: 0.004472, Tokens per Sec:      424, Lr: 0.000064\n",
      "2022-08-05 04:11:48,743 - INFO - joeynmt.training - Epoch  10, Step:    19700, Batch Loss:     0.485396, Batch Acc: 0.006020, Tokens per Sec:      423, Lr: 0.000064\n",
      "2022-08-05 04:15:25,695 - INFO - joeynmt.training - Epoch  10, Step:    19800, Batch Loss:     0.450318, Batch Acc: 0.005387, Tokens per Sec:      437, Lr: 0.000064\n",
      "2022-08-05 04:19:06,481 - INFO - joeynmt.training - Epoch  10, Step:    19900, Batch Loss:     0.454304, Batch Acc: 0.004960, Tokens per Sec:      426, Lr: 0.000063\n",
      "2022-08-05 04:22:45,041 - INFO - joeynmt.training - Epoch  10, Step:    20000, Batch Loss:     0.448819, Batch Acc: 0.005288, Tokens per Sec:      433, Lr: 0.000063\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 168.46ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17242.13ex/s]\n",
      "2022-08-05 04:22:45,757 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=20000\n",
      "2022-08-05 04:22:45,757 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 04:23:02,578 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 04:23:02,578 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.51, loss:   3.07, ppl:  21.60, acc:   0.35, generation: 16.7994[sec], evaluation: 0.0167[sec]\n",
      "2022-08-05 04:23:02,885 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/no_tf/11000.ckpt\n",
      "2022-08-05 04:23:02,900 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 04:23:02,902 - INFO - joeynmt.training - \tSource:     I don't know what you mean.\n",
      "2022-08-05 04:23:02,902 - INFO - joeynmt.training - \tReference:  Eu nÃ£o sei o que vocÃª quis dizer.\n",
      "2022-08-05 04:23:02,902 - INFO - joeynmt.training - \tHypothesis: Eu sei o que vocÃª quer quer dizer.\n",
      "2022-08-05 04:23:02,902 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 04:23:02,903 - INFO - joeynmt.training - \tSource:     I always liked mysterious characters more.\n",
      "2022-08-05 04:23:02,903 - INFO - joeynmt.training - \tReference:  Eu sempre gostei mais de personagens misteriosos.\n",
      "2022-08-05 04:23:02,903 - INFO - joeynmt.training - \tHypothesis: Eu sempre gostei mais personagens..\n",
      "2022-08-05 04:23:02,903 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 04:23:02,905 - INFO - joeynmt.training - \tSource:     They were left speechless.\n",
      "2022-08-05 04:23:02,905 - INFO - joeynmt.training - \tReference:  As deixaram sem palavras.\n",
      "2022-08-05 04:23:02,905 - INFO - joeynmt.training - \tHypothesis: Eles foram foram sem palavras.\n",
      "2022-08-05 04:23:02,905 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 04:23:02,906 - INFO - joeynmt.training - \tSource:     If you don't want to put on sunscreen, that's your problem. Just don't come complaining to me when you get a sunburn.\n",
      "2022-08-05 04:23:02,906 - INFO - joeynmt.training - \tReference:  Se vocÃª nÃ£o quiser colocar protetor solar, isso Ã© problema seu. SÃ³ nÃ£o venha reclamar para mim quando vocÃª ficar com uma queimadura por causa do sol.\n",
      "2022-08-05 04:23:02,906 - INFO - joeynmt.training - \tHypothesis: Se vocÃª nÃ£o quer tocar protetor solar, esse esse problema.. nÃ£o nÃ£o nÃ£o quando quando quando.\n",
      "2022-08-05 04:23:02,908 - INFO - joeynmt.training - Training ended since maximum num. of updates 20000 was reached.\n",
      "2022-08-05 04:23:02,908 - INFO - joeynmt.training - Best validation result (greedy) at step    18000:  20.25 bleu.\n",
      "2022-08-05 04:23:02,928 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-05 04:23:03,205 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-05 04:23:03,207 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-05 04:23:03,405 - INFO - joeynmt.helpers - Load model from /home/lconti/en-pt_tatoeba/models/no_tf/18000.ckpt.\n",
      "Dropping NaN...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 175.90ba/s]\n",
      "Preprocessing...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17612.17ex/s]\n",
      "2022-08-05 04:23:04,190 - INFO - joeynmt.prediction - Decoding on dev set...\n",
      "2022-08-05 04:23:04,190 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 04:24:33,844 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 04:24:33,844 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  21.30, generation: 89.5684[sec], evaluation: 0.0702[sec]\n",
      "2022-08-05 04:24:33,845 - INFO - joeynmt.prediction - Translations saved to: /home/lconti/en-pt_tatoeba/models/no_tf/00018000.hyps.dev.\n",
      "2022-08-05 04:24:34,170 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-60b0c2cf34a85431.arrow\n",
      "2022-08-05 04:24:34,492 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-087e226f4078b277.arrow\n",
      "2022-08-05 04:24:34,493 - INFO - joeynmt.prediction - Decoding on test set...\n",
      "2022-08-05 04:24:34,493 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 04:26:07,883 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 04:26:07,883 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  26.05, generation: 93.2087[sec], evaluation: 0.1633[sec]\n",
      "2022-08-05 04:26:07,884 - INFO - joeynmt.prediction - Translations saved to: /home/lconti/en-pt_tatoeba/models/no_tf/00018000.hyps.test.\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/config_no_tf.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7qULQu0B5Yl"
   },
   "source": [
    "### Continue training after interruption\n",
    "To continue after an interruption, the configuration needs to be modified in the following places:\n",
    "\n",
    "- `load_model` to point to the checkpoint to load.\n",
    "- `reset_*` options (must be False) to resume the previous session.\n",
    "- `model_dir` to create a new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr3mEDga2BiJ"
   },
   "outputs": [],
   "source": [
    "resume_config = config\\\n",
    "  .replace('#load_model:', 'load_model:')\\\n",
    "  .replace('#reset_best_ckpt: False', 'reset_best_ckpt: False')\\\n",
    "  .replace('#reset_scheduler: False', 'reset_scheduler: False')\\\n",
    "  .replace('#reset_optimizer: False', 'reset_optimizer: False')\\\n",
    "  .replace('#reset_iter_state: False', 'reset_iter_state: False')\\\n",
    "  .replace(f'model_dir: \"{model_dir}\"', f'model_dir: \"{model_dir}_resume\"')\n",
    "\n",
    "with (Path(data_dir) / \"resume_config.yaml\").open('w') as f:\n",
    "    f.write(resume_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgH1vAsV2Bkw",
    "outputId": "56439036-1e0d-4fb9-cffd-257f09903d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 20:53:16,099 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 20:53:16,100 - INFO - joeynmt.helpers -                           cfg.name : tatoeba_deen_sp\n",
      "2022-06-04 20:53:16,100 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-06-04 20:53:16,100 - INFO - joeynmt.helpers -                     cfg.data.train : /content/drive/MyDrive/tatoeba_deen/train\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -                       cfg.data.dev : /content/drive/MyDrive/tatoeba_deen/validation\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -                      cfg.data.test : /content/drive/MyDrive/tatoeba_deen/test\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : 200\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /content/drive/MyDrive/tatoeba_deen/vocab.txt\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : sentencepiece\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.model_file : /content/drive/MyDrive/tatoeba_deen/sp.model\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /content/drive/MyDrive/tatoeba_deen/vocab.txt\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : sentencepiece\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.model_file : /content/drive/MyDrive/tatoeba_deen/sp.model\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 256\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -            cfg.training.load_model : /content/drive/MyDrive/models/tatoeba_deen/latest.ckpt\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 4000\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0002\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -            cfg.training.batch_size : 512\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -                cfg.training.epochs : 10\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -             cfg.training.model_dir : /content/drive/MyDrive/models/tatoeba_deen_resume\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 5\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-06-04 20:53:16,154 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-06-04 20:53:16,280 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 20:53:16,281 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 20:53:16,281 - INFO - joeynmt.data - Loading train set...\n",
      "2022-06-04 20:53:16,350 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n",
      "2022-06-04 20:53:18,870 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/train/cache-e9418b4482dd4017.arrow\n",
      "2022-06-04 20:53:18,903 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/train/cache-50210139b427e8ae.arrow\n",
      "2022-06-04 20:53:18,951 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-06-04 20:53:34,005 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-06-04 20:53:34,363 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-ce681e2c839418f9.arrow\n",
      "2022-06-04 20:53:34,702 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-0fdab69a63fd22e6.arrow\n",
      "2022-06-04 20:53:34,705 - INFO - joeynmt.data - Loading test set...\n",
      "2022-06-04 20:53:35,048 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/test/cache-4f5eef71bf1d5346.arrow\n",
      "2022-06-04 20:53:35,382 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/test/cache-a5f9783fc69136ac.arrow\n",
      "2022-06-04 20:53:35,385 - INFO - joeynmt.data - Data loaded.\n",
      "2022-06-04 20:53:35,385 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=305367, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, split=train, path=/content/drive/MyDrive/tatoeba_deen/train)\n",
      "2022-06-04 20:53:35,386 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=de, trg_lang=en, has_trg=True, random_subset=200, split=validation, path=/content/drive/MyDrive/tatoeba_deen/validation)\n",
      "2022-06-04 20:53:35,386 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, split=test, path=/content/drive/MyDrive/tatoeba_deen/test)\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] â–Sie â–kÃ¶nnen â–nicht â–reiten .\n",
      "\t[TRG] â–You â–can ' t â–ride â–a â–horse .\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) â–Tom (7) ' (8) ? (9) â–I\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) â–Tom (7) ' (8) ? (9) â–I\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-06-04 20:53:35,388 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-06-04 20:53:35,437 - WARNING - joeynmt.tokenizers - /content/drive/MyDrive/models/tatoeba_deen_resume/sp.model already exists. Stop copying.\n",
      "2022-06-04 20:53:35,447 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 20:53:35,800 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 20:53:35,812 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-06-04 20:53:35,813 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-06-04 20:53:40,214 - INFO - joeynmt.builders - Adam(lr=0.0002, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-06-04 20:53:40,214 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=4000, decay_rate=0.012649, peak_rate=0.0002, min_rate=1e-08)\n",
      "2022-06-04 20:53:40,215 - INFO - joeynmt.training - Loading model from /content/drive/MyDrive/models/tatoeba_deen/latest.ckpt\n",
      "2022-06-04 20:53:40,758 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen/4161.ckpt.\n",
      "2022-06-04 20:53:40,844 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 512\n",
      "\teffective batch size (w. parallel & accumulation): 2048\n",
      "2022-06-04 20:53:40,845 - INFO - joeynmt.training - EPOCH 1\n",
      "2022-06-04 20:53:51,110 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.288305, Batch Acc: 0.014659, Tokens per Sec:     3569, Lr: 0.000195\n",
      "2022-06-04 20:54:18,849 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.190759, Batch Acc: 0.006044, Tokens per Sec:     3472, Lr: 0.000193\n",
      "2022-06-04 20:54:45,568 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.643943, Batch Acc: 0.004332, Tokens per Sec:     3534, Lr: 0.000191\n",
      "2022-06-04 20:55:12,250 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.348576, Batch Acc: 0.005454, Tokens per Sec:     3546, Lr: 0.000189\n",
      "2022-06-04 20:55:39,271 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.120830, Batch Acc: 0.006304, Tokens per Sec:     3481, Lr: 0.000187\n",
      "2022-06-04 20:56:05,869 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     2.034187, Batch Acc: 0.005908, Tokens per Sec:     3589, Lr: 0.000185\n",
      "2022-06-04 20:56:32,403 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     2.197401, Batch Acc: 0.006395, Tokens per Sec:     3589, Lr: 0.000183\n",
      "2022-06-04 20:56:59,968 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     2.014929, Batch Acc: 0.006460, Tokens per Sec:     3431, Lr: 0.000181\n",
      "2022-06-04 20:57:26,724 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     2.061943, Batch Acc: 0.006459, Tokens per Sec:     3495, Lr: 0.000179\n",
      "2022-06-04 20:57:27,075 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-d2b87a894115655b.arrow\n",
      "2022-06-04 20:57:27,413 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-f6dc53774c4904a4.arrow\n",
      "2022-06-04 20:57:27,431 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=5000\n",
      "2022-06-04 20:57:27,432 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 20:57:37,835 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 20:57:37,835 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.44, loss:   2.44, ppl:  11.53, acc:   0.55, generation: 10.3712[sec], evaluation: 0.0263[sec]\n",
      "2022-06-04 20:57:37,836 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 20:57:38,886 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 20:57:38,890 - INFO - joeynmt.training - \tSource:     Die neulichen Skandale, in die Messdiener und religiÃ¶se OberhÃ¤upter verwickelt waren, haben den Glauben, den die Leute in die Kirche haben, unterminiert.\n",
      "2022-06-04 20:57:38,891 - INFO - joeynmt.training - \tReference:  The recent scandals involving altar boys and religious leaders have undermined the faith people have in the Church.\n",
      "2022-06-04 20:57:38,891 - INFO - joeynmt.training - \tHypothesis: The other day, the ttttsss and played out of the most tttttsssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "2022-06-04 20:57:38,891 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 20:57:38,893 - INFO - joeynmt.training - \tSource:     Wie bist du auf diese verrÃ¼ckte Idee gekommen?\n",
      "2022-06-04 20:57:38,894 - INFO - joeynmt.training - \tReference:  How did you come up with this crazy idea?\n",
      "2022-06-04 20:57:38,894 - INFO - joeynmt.training - \tHypothesis: How did you come out of this idea?\n",
      "2022-06-04 20:57:38,894 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - \tSource:     Ein Leben ohne Liebe hat Ã¼berhaupt keinen Sinn.\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - \tReference:  Life without love has no meaning at all.\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - \tHypothesis: A life without not a good way.\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 20:57:38,899 - INFO - joeynmt.training - \tSource:     Das wÃ¤re etwas, das ich programmieren sollte.\n",
      "2022-06-04 20:57:38,900 - INFO - joeynmt.training - \tReference:  It would be something I'd have to program.\n",
      "2022-06-04 20:57:38,900 - INFO - joeynmt.training - \tHypothesis: That would be something I should make.\n",
      "2022-06-04 20:58:06,718 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     2.013755, Batch Acc: 0.006844, Tokens per Sec:     3180, Lr: 0.000177\n",
      "2022-06-04 20:58:33,871 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     1.729190, Batch Acc: 0.006564, Tokens per Sec:     3479, Lr: 0.000175\n",
      "2022-06-04 20:59:00,603 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     1.810303, Batch Acc: 0.006589, Tokens per Sec:     3509, Lr: 0.000174\n",
      "2022-06-04 20:59:28,238 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     1.856463, Batch Acc: 0.006088, Tokens per Sec:     3376, Lr: 0.000172\n",
      "2022-06-04 20:59:55,245 - INFO - joeynmt.training - Epoch   1, Step:     5500, Batch Loss:     1.824483, Batch Acc: 0.006628, Tokens per Sec:     3553, Lr: 0.000171\n",
      "2022-06-04 21:00:21,641 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     2.152315, Batch Acc: 0.005813, Tokens per Sec:     3591, Lr: 0.000169\n",
      "2022-06-04 21:00:48,463 - INFO - joeynmt.training - Epoch   1, Step:     5700, Batch Loss:     1.766821, Batch Acc: 0.007215, Tokens per Sec:     3529, Lr: 0.000168\n",
      "2022-06-04 21:01:15,064 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     1.598522, Batch Acc: 0.006001, Tokens per Sec:     3558, Lr: 0.000166\n",
      "2022-06-04 21:01:41,877 - INFO - joeynmt.training - Epoch   1, Step:     5900, Batch Loss:     1.886486, Batch Acc: 0.006554, Tokens per Sec:     3534, Lr: 0.000165\n",
      "2022-06-04 21:02:09,422 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     1.926557, Batch Acc: 0.006216, Tokens per Sec:     3417, Lr: 0.000163\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 72.90ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11569.76ex/s]\n",
      "2022-06-04 21:02:10,261 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=6000\n",
      "2022-06-04 21:02:10,262 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:02:13,849 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:02:13,849 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  23.15, loss:   2.18, ppl:   8.83, acc:   0.59, generation: 3.5618[sec], evaluation: 0.0203[sec]\n",
      "2022-06-04 21:02:13,850 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:02:14,848 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:02:14,852 - INFO - joeynmt.training - \tSource:     Das fÃ¤ngt nicht vor acht Uhr dreiÃŸig an.\n",
      "2022-06-04 21:02:14,852 - INFO - joeynmt.training - \tReference:  It won't start before eight-thirty.\n",
      "2022-06-04 21:02:14,852 - INFO - joeynmt.training - \tHypothesis: That won't be thirty o'clock.\n",
      "2022-06-04 21:02:14,853 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - \tSource:     Sie fragt, wie das mÃ¶glich ist.\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - \tReference:  She's asking how that's possible.\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - \tHypothesis: She will ask how it is possible.\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:02:14,857 - INFO - joeynmt.training - \tSource:     Bill Clinton sprach eine mehrdeutige Sprache, als er gebeten wurde, sein VerhÃ¤ltnis mit Monika Lewinsky zu beschreiben.\n",
      "2022-06-04 21:02:14,858 - INFO - joeynmt.training - \tReference:  Bill Clinton spoke in ambiguous language when asked to describe his relationship with Monica Lewinsky.\n",
      "2022-06-04 21:02:14,858 - INFO - joeynmt.training - \tHypothesis: Bill was a more-ttttt of the language when he asked him to because of the 's-meet.\n",
      "2022-06-04 21:02:14,858 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:02:14,860 - INFO - joeynmt.training - \tSource:     Sie will nicht darÃ¼ber sprechen.\n",
      "2022-06-04 21:02:14,860 - INFO - joeynmt.training - \tReference:  She doesn't want to talk about it.\n",
      "2022-06-04 21:02:14,860 - INFO - joeynmt.training - \tHypothesis: She doesn't want to talk about it.\n",
      "2022-06-04 21:02:42,777 - INFO - joeynmt.training - Epoch   1, Step:     6100, Batch Loss:     2.206654, Batch Acc: 0.005183, Tokens per Sec:     3209, Lr: 0.000162\n",
      "2022-06-04 21:03:09,323 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     2.004051, Batch Acc: 0.005673, Tokens per Sec:     3592, Lr: 0.000161\n",
      "2022-06-04 21:03:36,067 - INFO - joeynmt.training - Epoch   1, Step:     6300, Batch Loss:     2.067890, Batch Acc: 0.005431, Tokens per Sec:     3552, Lr: 0.000159\n",
      "2022-06-04 21:04:02,975 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     1.810111, Batch Acc: 0.007169, Tokens per Sec:     3520, Lr: 0.000158\n",
      "2022-06-04 21:04:30,526 - INFO - joeynmt.training - Epoch   1, Step:     6500, Batch Loss:     1.852140, Batch Acc: 0.006594, Tokens per Sec:     3435, Lr: 0.000157\n",
      "2022-06-04 21:04:57,488 - INFO - joeynmt.training - Epoch   1, Step:     6600, Batch Loss:     1.497860, Batch Acc: 0.008099, Tokens per Sec:     3471, Lr: 0.000156\n",
      "2022-06-04 21:05:24,082 - INFO - joeynmt.training - Epoch   1, Step:     6700, Batch Loss:     1.597221, Batch Acc: 0.007165, Tokens per Sec:     3542, Lr: 0.000155\n",
      "2022-06-04 21:05:50,676 - INFO - joeynmt.training - Epoch   1, Step:     6800, Batch Loss:     1.645426, Batch Acc: 0.006132, Tokens per Sec:     3551, Lr: 0.000153\n",
      "2022-06-04 21:06:17,313 - INFO - joeynmt.training - Epoch   1, Step:     6900, Batch Loss:     1.655118, Batch Acc: 0.006630, Tokens per Sec:     3522, Lr: 0.000152\n",
      "2022-06-04 21:06:44,486 - INFO - joeynmt.training - Epoch   1, Step:     7000, Batch Loss:     1.632364, Batch Acc: 0.006844, Tokens per Sec:     3469, Lr: 0.000151\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.26ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11684.14ex/s]\n",
      "2022-06-04 21:06:45,309 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=7000\n",
      "2022-06-04 21:06:45,310 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:06:54,551 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:06:54,552 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.88, loss:   2.20, ppl:   9.03, acc:   0.58, generation: 9.2057[sec], evaluation: 0.0277[sec]\n",
      "2022-06-04 21:06:55,521 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - \tSource:     Ich wollte nicht, dass das passiert.\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - \tReference:  I didn't want this to happen.\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - \tHypothesis: I didn't want this happened.\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:06:55,527 - INFO - joeynmt.training - \tSource:     Eins, zwei, drei, vier, fÃ¼nf, sechs, sieben, acht, neun, zehn.\n",
      "2022-06-04 21:06:55,527 - INFO - joeynmt.training - \tReference:  One, two, three, four, five, six, seven, eight, nine, ten.\n",
      "2022-06-04 21:06:55,527 - INFO - joeynmt.training - \tHypothesis: A one, two, three, four, five, five, ten, seven, seven, seven, ten, seven, ten.\n",
      "2022-06-04 21:06:55,528 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:06:55,529 - INFO - joeynmt.training - \tSource:     Sprichst du Italienisch?\n",
      "2022-06-04 21:06:55,530 - INFO - joeynmt.training - \tReference:  Do you speak Italian?\n",
      "2022-06-04 21:06:55,530 - INFO - joeynmt.training - \tHypothesis: Do you speak Italian?\n",
      "2022-06-04 21:06:55,530 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:06:55,532 - INFO - joeynmt.training - \tSource:     Es kommt darauf an, was du mit an Gott \"glauben\" meinst.\n",
      "2022-06-04 21:06:55,532 - INFO - joeynmt.training - \tReference:  It depends what you mean by \"believe\" in God.\n",
      "2022-06-04 21:06:55,532 - INFO - joeynmt.training - \tHypothesis: It's about what you think with God \"No, love.\"\n",
      "2022-06-04 21:07:24,461 - INFO - joeynmt.training - Epoch   1, Step:     7100, Batch Loss:     1.723976, Batch Acc: 0.007435, Tokens per Sec:     3064, Lr: 0.000150\n",
      "2022-06-04 21:07:51,293 - INFO - joeynmt.training - Epoch   1, Step:     7200, Batch Loss:     1.868087, Batch Acc: 0.006683, Tokens per Sec:     3513, Lr: 0.000149\n",
      "2022-06-04 21:08:17,983 - INFO - joeynmt.training - Epoch   1, Step:     7300, Batch Loss:     1.663398, Batch Acc: 0.007155, Tokens per Sec:     3597, Lr: 0.000148\n",
      "2022-06-04 21:08:43,027 - INFO - joeynmt.training - Epoch   1: total training loss 6240.50\n",
      "2022-06-04 21:08:43,028 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-06-04 21:08:44,938 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.552310, Batch Acc: 0.100600, Tokens per Sec:     3318, Lr: 0.000147\n",
      "2022-06-04 21:09:11,684 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.611410, Batch Acc: 0.006134, Tokens per Sec:     3523, Lr: 0.000146\n",
      "2022-06-04 21:09:38,289 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.513177, Batch Acc: 0.007730, Tokens per Sec:     3516, Lr: 0.000145\n",
      "2022-06-04 21:10:05,891 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.476209, Batch Acc: 0.007273, Tokens per Sec:     3452, Lr: 0.000144\n",
      "2022-06-04 21:10:32,555 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.450078, Batch Acc: 0.007061, Tokens per Sec:     3516, Lr: 0.000143\n",
      "2022-06-04 21:10:58,834 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.430762, Batch Acc: 0.007802, Tokens per Sec:     3570, Lr: 0.000142\n",
      "2022-06-04 21:11:25,708 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.589353, Batch Acc: 0.006988, Tokens per Sec:     3552, Lr: 0.000141\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.47ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 10993.26ex/s]\n",
      "2022-06-04 21:11:26,550 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=8000\n",
      "2022-06-04 21:11:26,550 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:11:29,869 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:11:29,869 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  25.86, loss:   1.98, ppl:   7.23, acc:   0.62, generation: 3.2891[sec], evaluation: 0.0246[sec]\n",
      "2022-06-04 21:11:29,870 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:11:30,802 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:11:30,807 - INFO - joeynmt.training - \tSource:     Warum fragst du?\n",
      "2022-06-04 21:11:30,807 - INFO - joeynmt.training - \tReference:  Why are you asking?\n",
      "2022-06-04 21:11:30,808 - INFO - joeynmt.training - \tHypothesis: Why do you ask?\n",
      "2022-06-04 21:11:30,808 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:11:30,810 - INFO - joeynmt.training - \tSource:     Meine Mutter spricht nicht sehr gut Englisch.\n",
      "2022-06-04 21:11:30,811 - INFO - joeynmt.training - \tReference:  My mom doesn't speak English very well.\n",
      "2022-06-04 21:11:30,811 - INFO - joeynmt.training - \tHypothesis: My mother doesn't speak English very well.\n",
      "2022-06-04 21:11:30,811 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - \tSource:     Heute Abend gehen wir in die Kirche.\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - \tReference:  Tonight we're going to church.\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - \tHypothesis: We'll go to church tonight.\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:11:30,815 - INFO - joeynmt.training - \tSource:     Das ist das DÃ¼mmste, was ich je gesagt habe.\n",
      "2022-06-04 21:11:30,816 - INFO - joeynmt.training - \tReference:  That's the stupidest thing I've ever said.\n",
      "2022-06-04 21:11:30,816 - INFO - joeynmt.training - \tHypothesis: This is the stupid thing I've ever said.\n",
      "2022-06-04 21:11:58,588 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.422772, Batch Acc: 0.007385, Tokens per Sec:     3156, Lr: 0.000141\n",
      "2022-06-04 21:12:26,094 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.628181, Batch Acc: 0.006469, Tokens per Sec:     3462, Lr: 0.000140\n",
      "2022-06-04 21:12:52,885 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.649422, Batch Acc: 0.006705, Tokens per Sec:     3524, Lr: 0.000139\n",
      "2022-06-04 21:13:19,722 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.354998, Batch Acc: 0.007463, Tokens per Sec:     3515, Lr: 0.000138\n",
      "2022-06-04 21:13:45,860 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.442650, Batch Acc: 0.007173, Tokens per Sec:     3531, Lr: 0.000137\n",
      "2022-06-04 21:14:12,681 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.333545, Batch Acc: 0.008145, Tokens per Sec:     3498, Lr: 0.000136\n",
      "2022-06-04 21:14:39,445 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.602624, Batch Acc: 0.006743, Tokens per Sec:     3557, Lr: 0.000136\n",
      "2022-06-04 21:15:06,897 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.814612, Batch Acc: 0.006291, Tokens per Sec:     3468, Lr: 0.000135\n",
      "2022-06-04 21:15:33,510 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     1.564209, Batch Acc: 0.006815, Tokens per Sec:     3589, Lr: 0.000134\n",
      "2022-06-04 21:16:00,475 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     1.405532, Batch Acc: 0.007405, Tokens per Sec:     3476, Lr: 0.000133\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.44ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11394.87ex/s]\n",
      "2022-06-04 21:16:01,301 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=9000\n",
      "2022-06-04 21:16:01,302 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:16:05,609 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:16:05,609 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  25.79, loss:   1.94, ppl:   6.94, acc:   0.62, generation: 4.2798[sec], evaluation: 0.0224[sec]\n",
      "2022-06-04 21:16:06,728 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:16:06,731 - INFO - joeynmt.training - \tSource:     Bildung und Bewegung von Hurrikanen sind unberechenbar, sogar mit unserer modernen Technologie.\n",
      "2022-06-04 21:16:06,731 - INFO - joeynmt.training - \tReference:  The formation and movement of hurricanes are capricious, even with our present-day technology.\n",
      "2022-06-04 21:16:06,732 - INFO - joeynmt.training - \tHypothesis: Education and nuclearssss are nuclearsss, even 'scause of our solar system.\n",
      "2022-06-04 21:16:06,732 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - \tSource:     Wenn ich keine Antwort gegeben hÃ¤tte, hÃ¤tte ich nicht gesprochen.\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - \tReference:  If I gave no answer, I would not have spoken.\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - \tHypothesis: If I had no answer, I wouldn't have spoken.\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:16:06,736 - INFO - joeynmt.training - \tSource:     Niemand versteht mich.\n",
      "2022-06-04 21:16:06,736 - INFO - joeynmt.training - \tReference:  No one understands me.\n",
      "2022-06-04 21:16:06,736 - INFO - joeynmt.training - \tHypothesis: No one understands me.\n",
      "2022-06-04 21:16:06,737 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:16:06,738 - INFO - joeynmt.training - \tSource:     Wo ist das Problem?\n",
      "2022-06-04 21:16:06,739 - INFO - joeynmt.training - \tReference:  What is the problem?\n",
      "2022-06-04 21:16:06,739 - INFO - joeynmt.training - \tHypothesis: Where is the problem?\n",
      "2022-06-04 21:16:34,583 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     1.166789, Batch Acc: 0.008751, Tokens per Sec:     3141, Lr: 0.000133\n",
      "2022-06-04 21:17:02,063 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     1.397016, Batch Acc: 0.007049, Tokens per Sec:     3459, Lr: 0.000132\n",
      "2022-06-04 21:17:29,841 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     1.600673, Batch Acc: 0.006411, Tokens per Sec:     3363, Lr: 0.000131\n",
      "2022-06-04 21:17:56,748 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     1.418394, Batch Acc: 0.007788, Tokens per Sec:     3503, Lr: 0.000130\n",
      "2022-06-04 21:18:23,946 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     1.389611, Batch Acc: 0.007491, Tokens per Sec:     3485, Lr: 0.000130\n",
      "2022-06-04 21:18:51,133 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.234426, Batch Acc: 0.005569, Tokens per Sec:     3521, Lr: 0.000129\n",
      "2022-06-04 21:19:20,243 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     1.382914, Batch Acc: 0.007523, Tokens per Sec:     3220, Lr: 0.000128\n",
      "2022-06-04 21:19:47,649 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     1.126925, Batch Acc: 0.008240, Tokens per Sec:     3458, Lr: 0.000128\n",
      "2022-06-04 21:20:15,604 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     1.359352, Batch Acc: 0.007019, Tokens per Sec:     3420, Lr: 0.000127\n",
      "2022-06-04 21:20:42,570 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     1.710780, Batch Acc: 0.006730, Tokens per Sec:     3527, Lr: 0.000126\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 71.74ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 9970.84ex/s]\n",
      "2022-06-04 21:20:43,410 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=10000\n",
      "2022-06-04 21:20:43,410 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:20:47,314 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:20:47,314 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  30.19, loss:   1.85, ppl:   6.38, acc:   0.64, generation: 3.8779[sec], evaluation: 0.0206[sec]\n",
      "2022-06-04 21:20:47,315 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:20:48,657 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/5000.ckpt\n",
      "2022-06-04 21:20:48,687 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:20:48,690 - INFO - joeynmt.training - \tSource:     Beeil dich!\n",
      "2022-06-04 21:20:48,690 - INFO - joeynmt.training - \tReference:  Hurry it up!\n",
      "2022-06-04 21:20:48,690 - INFO - joeynmt.training - \tHypothesis: Hurry up!\n",
      "2022-06-04 21:20:48,691 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - \tSource:     Es gibt Leute auf der Welt, die so hungrig sind, dass Gott ihnen nicht erscheinen kann, auÃŸer in Form von Brot.\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - \tReference:  There are people in the world so hungry, that God cannot appear to them except in the form of bread.\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - \tHypothesis: There are people in the world that are so hungry that God can't seem to change the meaning of bread.\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:20:48,695 - INFO - joeynmt.training - \tSource:     Computer machen die Leute dumm.\n",
      "2022-06-04 21:20:48,695 - INFO - joeynmt.training - \tReference:  Computers make people stupid.\n",
      "2022-06-04 21:20:48,696 - INFO - joeynmt.training - \tHypothesis: Your computer are stupid.\n",
      "2022-06-04 21:20:48,696 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:20:48,700 - INFO - joeynmt.training - \tSource:     Das hat mich Ã¼berrascht, ich wusste nicht, was ich tun sollte.\n",
      "2022-06-04 21:20:48,701 - INFO - joeynmt.training - \tReference:  It caught me off guard; I didn't know what to do.\n",
      "2022-06-04 21:20:48,702 - INFO - joeynmt.training - \tHypothesis: I was surprised, I didn't know what to do.\n",
      "2022-06-04 21:21:16,499 - INFO - joeynmt.training - Epoch   2, Step:    10100, Batch Loss:     1.298121, Batch Acc: 0.008145, Tokens per Sec:     3132, Lr: 0.000126\n",
      "2022-06-04 21:21:42,946 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     1.239453, Batch Acc: 0.008386, Tokens per Sec:     3544, Lr: 0.000125\n",
      "2022-06-04 21:22:09,555 - INFO - joeynmt.training - Epoch   2, Step:    10300, Batch Loss:     1.354857, Batch Acc: 0.007747, Tokens per Sec:     3580, Lr: 0.000125\n",
      "2022-06-04 21:22:36,959 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     1.315276, Batch Acc: 0.007599, Tokens per Sec:     3453, Lr: 0.000124\n",
      "2022-06-04 21:23:03,797 - INFO - joeynmt.training - Epoch   2, Step:    10500, Batch Loss:     1.179908, Batch Acc: 0.008729, Tokens per Sec:     3526, Lr: 0.000123\n",
      "2022-06-04 21:23:30,307 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     1.535451, Batch Acc: 0.007907, Tokens per Sec:     3568, Lr: 0.000123\n",
      "2022-06-04 21:23:38,360 - INFO - joeynmt.training - Epoch   2: total training loss 4744.08\n",
      "2022-06-04 21:23:38,360 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-06-04 21:23:57,543 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.084358, Batch Acc: 0.012840, Tokens per Sec:     3504, Lr: 0.000122\n",
      "2022-06-04 21:24:23,957 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.370948, Batch Acc: 0.008079, Tokens per Sec:     3575, Lr: 0.000122\n",
      "2022-06-04 21:24:50,675 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.194668, Batch Acc: 0.008189, Tokens per Sec:     3561, Lr: 0.000121\n",
      "2022-06-04 21:25:17,990 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.434793, Batch Acc: 0.006377, Tokens per Sec:     3439, Lr: 0.000121\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 66.28ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 10930.41ex/s]\n",
      "2022-06-04 21:25:18,856 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=11000\n",
      "2022-06-04 21:25:18,857 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:25:22,608 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:25:22,609 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  29.51, loss:   1.69, ppl:   5.44, acc:   0.66, generation: 3.7267[sec], evaluation: 0.0202[sec]\n",
      "2022-06-04 21:25:23,526 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/7000.ckpt\n",
      "2022-06-04 21:25:23,556 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - \tSource:     Ein Stundenplan ist ein Ausweis fÃ¼r die Zeit, nur, wenn man keinen Stundenplan hat, ist die Zeit nicht da.\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - \tReference:  A schedule is an identity card for time, but, if you don't have a schedule, the time isn't there.\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - \tHypothesis: A hours plan is a smile for time, just if you have no time.\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:25:23,562 - INFO - joeynmt.training - \tSource:     Es ist schwierig, ein GesprÃ¤ch mit jemandem zu fÃ¼hren, der nur \"Ja\" und \"Nein\" sagt.\n",
      "2022-06-04 21:25:23,563 - INFO - joeynmt.training - \tReference:  It is difficult to keep up a conversation with someone who only says \"yes\" and \"no\".\n",
      "2022-06-04 21:25:23,563 - INFO - joeynmt.training - \tHypothesis: It's difficult to use a conversation with someone who only means \"No.\"\n",
      "2022-06-04 21:25:23,564 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:25:23,565 - INFO - joeynmt.training - \tSource:     Die meisten Leute denken, ich sei verrÃ¼ckt.\n",
      "2022-06-04 21:25:23,566 - INFO - joeynmt.training - \tReference:  Most people think I'm crazy.\n",
      "2022-06-04 21:25:23,566 - INFO - joeynmt.training - \tHypothesis: Most people think I'm crazy.\n",
      "2022-06-04 21:25:23,566 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:25:23,568 - INFO - joeynmt.training - \tSource:     Bist du sicher?\n",
      "2022-06-04 21:25:23,568 - INFO - joeynmt.training - \tReference:  Are you sure?\n",
      "2022-06-04 21:25:23,568 - INFO - joeynmt.training - \tHypothesis: Are you sure?\n",
      "2022-06-04 21:25:50,694 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.404580, Batch Acc: 0.005673, Tokens per Sec:     3263, Lr: 0.000120\n",
      "2022-06-04 21:26:17,158 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.328782, Batch Acc: 0.007381, Tokens per Sec:     3584, Lr: 0.000120\n",
      "2022-06-04 21:26:43,788 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.225472, Batch Acc: 0.007584, Tokens per Sec:     3570, Lr: 0.000119\n",
      "2022-06-04 21:27:10,265 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.364240, Batch Acc: 0.006407, Tokens per Sec:     3543, Lr: 0.000118\n",
      "2022-06-04 21:27:37,872 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.045176, Batch Acc: 0.008393, Tokens per Sec:     3388, Lr: 0.000118\n",
      "2022-06-04 21:28:04,561 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.096500, Batch Acc: 0.008267, Tokens per Sec:     3544, Lr: 0.000117\n",
      "2022-06-04 21:28:31,270 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.408800, Batch Acc: 0.005898, Tokens per Sec:     3548, Lr: 0.000117\n",
      "2022-06-04 21:28:57,776 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.222268, Batch Acc: 0.006867, Tokens per Sec:     3599, Lr: 0.000116\n",
      "2022-06-04 21:29:24,045 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.418336, Batch Acc: 0.006734, Tokens per Sec:     3578, Lr: 0.000116\n",
      "2022-06-04 21:29:50,483 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.180122, Batch Acc: 0.007419, Tokens per Sec:     3584, Lr: 0.000115\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 73.06ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11010.55ex/s]\n",
      "2022-06-04 21:29:51,305 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=12000\n",
      "2022-06-04 21:29:51,305 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:29:56,301 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:29:56,301 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  31.12, loss:   1.76, ppl:   5.79, acc:   0.64, generation: 4.9687[sec], evaluation: 0.0222[sec]\n",
      "2022-06-04 21:29:56,302 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:29:57,597 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/6000.ckpt\n",
      "2022-06-04 21:29:57,631 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:29:57,634 - INFO - joeynmt.training - \tSource:     Aaah!! Mein Computer ist kaputt!\n",
      "2022-06-04 21:29:57,634 - INFO - joeynmt.training - \tReference:  Aaah!! My computer is broken!\n",
      "2022-06-04 21:29:57,634 - INFO - joeynmt.training - \tHypothesis: Ea! My computer is broken!\n",
      "2022-06-04 21:29:57,635 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:29:57,637 - INFO - joeynmt.training - \tSource:     Wenn ich Leute frage, was sie in Bezug aufs Gymnasium am meisten bedauern, sagen fast alle das Gleiche: dass sie so viel Zeit verschwendet haben.\n",
      "2022-06-04 21:29:57,637 - INFO - joeynmt.training - \tReference:  When I ask people what they regret most about high school, they nearly all say the same thing: that they wasted so much time.\n",
      "2022-06-04 21:29:57,637 - INFO - joeynmt.training - \tHypothesis: If I ask people what they think about high school, all the same high school, they have almost the same time.\n",
      "2022-06-04 21:29:57,638 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - \tSource:     Sei bitte geduldig, das braucht Zeit.\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - \tReference:  Be patient please. It takes time.\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - \tHypothesis: Please be patient to need time.\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:29:57,643 - INFO - joeynmt.training - \tSource:     Nein, er ist nicht mein neuer Freund.\n",
      "2022-06-04 21:29:57,643 - INFO - joeynmt.training - \tReference:  No, he's not my new boyfriend.\n",
      "2022-06-04 21:29:57,643 - INFO - joeynmt.training - \tHypothesis: No, he is not my new friend.\n",
      "2022-06-04 21:30:25,995 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.161913, Batch Acc: 0.007499, Tokens per Sec:     3085, Lr: 0.000115\n",
      "2022-06-04 21:30:52,611 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.145465, Batch Acc: 0.007659, Tokens per Sec:     3542, Lr: 0.000115\n",
      "2022-06-04 21:31:18,851 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.283632, Batch Acc: 0.006720, Tokens per Sec:     3596, Lr: 0.000114\n",
      "2022-06-04 21:31:45,363 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.312924, Batch Acc: 0.008340, Tokens per Sec:     3578, Lr: 0.000114\n",
      "2022-06-04 21:32:12,168 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.236772, Batch Acc: 0.005806, Tokens per Sec:     3534, Lr: 0.000113\n",
      "2022-06-04 21:32:38,574 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.007404, Batch Acc: 0.007911, Tokens per Sec:     3609, Lr: 0.000113\n",
      "2022-06-04 21:33:05,823 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.618528, Batch Acc: 0.006191, Tokens per Sec:     3474, Lr: 0.000112\n",
      "2022-06-04 21:33:32,476 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.256278, Batch Acc: 0.007669, Tokens per Sec:     3537, Lr: 0.000112\n",
      "2022-06-04 21:33:58,801 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.148804, Batch Acc: 0.007714, Tokens per Sec:     3575, Lr: 0.000111\n",
      "2022-06-04 21:34:25,106 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.247037, Batch Acc: 0.007222, Tokens per Sec:     3580, Lr: 0.000111\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 68.84ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11681.70ex/s]\n",
      "2022-06-04 21:34:25,974 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=13000\n",
      "2022-06-04 21:34:25,974 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:34:30,373 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:34:30,374 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  32.29, loss:   1.77, ppl:   5.90, acc:   0.65, generation: 4.3738[sec], evaluation: 0.0211[sec]\n",
      "2022-06-04 21:34:30,374 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:34:31,338 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/9000.ckpt\n",
      "2022-06-04 21:34:31,371 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:34:31,374 - INFO - joeynmt.training - \tSource:     In einem WÃ¶rterbuch wie diesem sollte es mindestens zwei SÃ¤tze mit \"KÃ¼hlschrank\" geben.\n",
      "2022-06-04 21:34:31,374 - INFO - joeynmt.training - \tReference:  In a dictionary like this one there should be at least two sentences with \"fridge\".\n",
      "2022-06-04 21:34:31,375 - INFO - joeynmt.training - \tHypothesis: In a dictionary as to give it two sentences with \"I'm.\"\n",
      "2022-06-04 21:34:31,375 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:34:31,377 - INFO - joeynmt.training - \tSource:     Ich werde ihn erschieÃŸen.\n",
      "2022-06-04 21:34:31,377 - INFO - joeynmt.training - \tReference:  I'm going to shoot him dead.\n",
      "2022-06-04 21:34:31,377 - INFO - joeynmt.training - \tHypothesis: I'll shoot him.\n",
      "2022-06-04 21:34:31,378 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - \tSource:     Ich kann in 10 Minuten zur Schule gehen.\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - \tReference:  I can walk to school in 10 minutes.\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - \tHypothesis: I can go to school ten minutes.\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:34:31,382 - INFO - joeynmt.training - \tSource:     Die Mathematik ist der Teil der Wissenschaft, den man weiter betreiben kÃ¶nnte, wenn man morgen aufwachen wÃ¼rde und entdecken wÃ¼rde, dass das Universum weg ist.\n",
      "2022-06-04 21:34:31,383 - INFO - joeynmt.training - \tReference:  Mathematics is the part of science you could continue to do if you woke up tomorrow and discovered the universe was gone.\n",
      "2022-06-04 21:34:31,383 - INFO - joeynmt.training - \tHypothesis: Mathematics is the part of science that could run when you wake up tomorrow and the universe is away.\n",
      "2022-06-04 21:34:59,022 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.365483, Batch Acc: 0.007153, Tokens per Sec:     3188, Lr: 0.000111\n",
      "2022-06-04 21:35:26,319 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     1.192539, Batch Acc: 0.007747, Tokens per Sec:     3471, Lr: 0.000110\n",
      "2022-06-04 21:35:53,282 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     1.222802, Batch Acc: 0.006808, Tokens per Sec:     3519, Lr: 0.000110\n",
      "2022-06-04 21:36:19,755 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     1.323586, Batch Acc: 0.007356, Tokens per Sec:     3564, Lr: 0.000109\n",
      "2022-06-04 21:36:46,746 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     1.118663, Batch Acc: 0.007235, Tokens per Sec:     3457, Lr: 0.000109\n",
      "2022-06-04 21:37:13,892 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     1.004997, Batch Acc: 0.008137, Tokens per Sec:     3490, Lr: 0.000108\n",
      "2022-06-04 21:37:40,792 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     1.188175, Batch Acc: 0.007845, Tokens per Sec:     3521, Lr: 0.000108\n",
      "2022-06-04 21:38:08,226 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     1.062657, Batch Acc: 0.007259, Tokens per Sec:     3480, Lr: 0.000108\n",
      "2022-06-04 21:38:25,120 - INFO - joeynmt.training - Epoch   3: total training loss 4061.75\n",
      "2022-06-04 21:38:25,121 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-06-04 21:38:34,945 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.028237, Batch Acc: 0.021588, Tokens per Sec:     3537, Lr: 0.000107\n",
      "2022-06-04 21:39:01,761 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.041151, Batch Acc: 0.008362, Tokens per Sec:     3550, Lr: 0.000107\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 70.98ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11332.86ex/s]\n",
      "2022-06-04 21:39:02,652 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=14000\n",
      "2022-06-04 21:39:02,652 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:39:06,533 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:39:06,533 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.23, loss:   1.67, ppl:   5.31, acc:   0.67, generation: 3.8549[sec], evaluation: 0.0207[sec]\n",
      "2022-06-04 21:39:06,534 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:39:07,417 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/8000.ckpt\n",
      "2022-06-04 21:39:07,449 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - \tSource:     Ich habe einen Traum.\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - \tReference:  I have a dream.\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - \tHypothesis: I have a dream.\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:39:07,462 - INFO - joeynmt.training - \tSource:     Was hast du heute zum Mittagessen gegessen?\n",
      "2022-06-04 21:39:07,462 - INFO - joeynmt.training - \tReference:  What did you have for lunch today?\n",
      "2022-06-04 21:39:07,463 - INFO - joeynmt.training - \tHypothesis: What have you eaten for lunch today?\n",
      "2022-06-04 21:39:07,463 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:39:07,465 - INFO - joeynmt.training - \tSource:     Ich hasse Chemie.\n",
      "2022-06-04 21:39:07,465 - INFO - joeynmt.training - \tReference:  I hate chemistry.\n",
      "2022-06-04 21:39:07,465 - INFO - joeynmt.training - \tHypothesis: I hate chemistry.\n",
      "2022-06-04 21:39:07,466 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:39:07,468 - INFO - joeynmt.training - \tSource:     Wenn ich dich erschrecken wollte, wÃ¼rde ich dir erzÃ¤hlen, was ich vor ein paar Wochen getrÃ¤umt habe.\n",
      "2022-06-04 21:39:07,468 - INFO - joeynmt.training - \tReference:  If I wanted to scare you, I would tell you what I dreamt about a few weeks ago.\n",
      "2022-06-04 21:39:07,468 - INFO - joeynmt.training - \tHypothesis: If I wanted to scare you, I would tell you what I dreamed of a few weeks ago.\n",
      "2022-06-04 21:39:34,733 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.139699, Batch Acc: 0.007947, Tokens per Sec:     3240, Lr: 0.000107\n",
      "2022-06-04 21:40:02,574 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     0.979903, Batch Acc: 0.007910, Tokens per Sec:     3456, Lr: 0.000106\n",
      "2022-06-04 21:40:29,001 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.033339, Batch Acc: 0.008048, Tokens per Sec:     3578, Lr: 0.000106\n",
      "2022-06-04 21:40:55,492 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.169329, Batch Acc: 0.008417, Tokens per Sec:     3516, Lr: 0.000105\n",
      "2022-06-04 21:41:22,354 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.122081, Batch Acc: 0.007491, Tokens per Sec:     3558, Lr: 0.000105\n",
      "2022-06-04 21:41:48,766 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.096020, Batch Acc: 0.008211, Tokens per Sec:     3550, Lr: 0.000105\n",
      "2022-06-04 21:42:15,539 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     0.951469, Batch Acc: 0.007975, Tokens per Sec:     3513, Lr: 0.000104\n",
      "2022-06-04 21:42:42,163 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.252295, Batch Acc: 0.007638, Tokens per Sec:     3570, Lr: 0.000104\n",
      "2022-06-04 21:43:09,163 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.101365, Batch Acc: 0.006967, Tokens per Sec:     3450, Lr: 0.000104\n",
      "2022-06-04 21:43:35,587 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.206033, Batch Acc: 0.006756, Tokens per Sec:     3568, Lr: 0.000103\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 69.57ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11510.35ex/s]\n",
      "2022-06-04 21:43:36,456 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=15000\n",
      "2022-06-04 21:43:36,456 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:43:40,367 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:43:40,367 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.08, loss:   1.71, ppl:   5.55, acc:   0.67, generation: 3.8849[sec], evaluation: 0.0214[sec]\n",
      "2022-06-04 21:43:41,250 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/11000.ckpt\n",
      "2022-06-04 21:43:41,280 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:43:41,283 - INFO - joeynmt.training - \tSource:     Die Verpflichtung zum Schulbesuch wird selten analysiert in der Vielzahl der Werke, die den mannigfaltigen Arten gewidmet sind, bei Kindern den Wunsch zu lernen zu entwickeln.\n",
      "2022-06-04 21:43:41,284 - INFO - joeynmt.training - \tReference:  The mandatory character of schooling is rarely analyzed in the multitude of works dedicated to the study of the various ways to develop within children the desire to learn.\n",
      "2022-06-04 21:43:41,284 - INFO - joeynmt.training - \tHypothesis: The service for school is rarely to spend a lot of work in the works, the works of the way to learn the children's wish to travel in the wish.\n",
      "2022-06-04 21:43:41,284 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:43:41,286 - INFO - joeynmt.training - \tSource:     Ich bin fast fertig.\n",
      "2022-06-04 21:43:41,286 - INFO - joeynmt.training - \tReference:  I'm about ready.\n",
      "2022-06-04 21:43:41,287 - INFO - joeynmt.training - \tHypothesis: I'm almost finished.\n",
      "2022-06-04 21:43:41,287 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - \tSource:     â€Ich habe Lust, Karten zu spielen.â€œ â€“ â€Ich auch.â€œ\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - \tReference:  \"I feel like playing cards.\" \"So do I.\"\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - \tHypothesis: \"I feel like playing cards.\" \"I'm too.\"\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:43:41,292 - INFO - joeynmt.training - \tSource:     â€Das sieht ziemlich interessant ausâ€œ, sagt Hiroshi.\n",
      "2022-06-04 21:43:41,292 - INFO - joeynmt.training - \tReference:  \"This looks pretty interesting,\" Hiroshi says.\n",
      "2022-06-04 21:43:41,292 - INFO - joeynmt.training - \tHypothesis: \"That looks pretty interesting,\" says Mr. Hio.\n",
      "2022-06-04 21:44:09,192 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.106495, Batch Acc: 0.008080, Tokens per Sec:     3235, Lr: 0.000103\n",
      "2022-06-04 21:44:35,364 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.067895, Batch Acc: 0.007854, Tokens per Sec:     3561, Lr: 0.000103\n",
      "2022-06-04 21:45:02,084 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.436536, Batch Acc: 0.007691, Tokens per Sec:     3528, Lr: 0.000102\n",
      "2022-06-04 21:45:29,823 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.151152, Batch Acc: 0.006763, Tokens per Sec:     3460, Lr: 0.000102\n",
      "2022-06-04 21:45:56,283 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.339136, Batch Acc: 0.007155, Tokens per Sec:     3544, Lr: 0.000102\n",
      "2022-06-04 21:46:22,992 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.037250, Batch Acc: 0.006793, Tokens per Sec:     3561, Lr: 0.000101\n",
      "2022-06-04 21:46:49,333 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     0.949777, Batch Acc: 0.008185, Tokens per Sec:     3576, Lr: 0.000101\n",
      "2022-06-04 21:47:15,871 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.139402, Batch Acc: 0.006658, Tokens per Sec:     3566, Lr: 0.000101\n",
      "2022-06-04 21:47:43,543 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     0.912606, Batch Acc: 0.010270, Tokens per Sec:     3403, Lr: 0.000100\n",
      "2022-06-04 21:48:10,068 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     0.975524, Batch Acc: 0.008769, Tokens per Sec:     3551, Lr: 0.000100\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 75.92ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11277.28ex/s]\n",
      "2022-06-04 21:48:10,885 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=16000\n",
      "2022-06-04 21:48:10,885 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:48:14,337 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:48:14,338 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.15, loss:   1.56, ppl:   4.77, acc:   0.69, generation: 3.4234[sec], evaluation: 0.0234[sec]\n",
      "2022-06-04 21:48:15,247 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/10000.ckpt\n",
      "2022-06-04 21:48:15,277 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:48:15,280 - INFO - joeynmt.training - \tSource:     Das ist ein totales Schlamassel und es geht mir auf die Nerven.\n",
      "2022-06-04 21:48:15,281 - INFO - joeynmt.training - \tReference:  It's a complete mess, and it's getting on my nerves.\n",
      "2022-06-04 21:48:15,281 - INFO - joeynmt.training - \tHypothesis: This is a complete mess and it's on my nerves.\n",
      "2022-06-04 21:48:15,281 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:48:15,283 - INFO - joeynmt.training - \tSource:     Ich bin am Leben, obwohl ich kein Lebenszeichen gebe.\n",
      "2022-06-04 21:48:15,283 - INFO - joeynmt.training - \tReference:  I am alive even though I am not giving any sign of life.\n",
      "2022-06-04 21:48:15,283 - INFO - joeynmt.training - \tHypothesis: I'm alive even though I don't give a sign of life.\n",
      "2022-06-04 21:48:15,284 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - \tSource:     Gibt es hier in der NÃ¤he eine Jugendherberge?\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - \tReference:  Is there a youth hostel near here?\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - \tHypothesis: Is there a hot youth here?\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:48:15,288 - INFO - joeynmt.training - \tSource:     Heute Abend gehen wir in die Kirche.\n",
      "2022-06-04 21:48:15,288 - INFO - joeynmt.training - \tReference:  Tonight we're going to church.\n",
      "2022-06-04 21:48:15,288 - INFO - joeynmt.training - \tHypothesis: We'll go to church tonight.\n",
      "2022-06-04 21:48:42,798 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     0.932757, Batch Acc: 0.008238, Tokens per Sec:     3263, Lr: 0.000100\n",
      "2022-06-04 21:49:09,867 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     0.978481, Batch Acc: 0.008462, Tokens per Sec:     3506, Lr: 0.000099\n",
      "2022-06-04 21:49:36,275 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     0.996044, Batch Acc: 0.008364, Tokens per Sec:     3541, Lr: 0.000099\n",
      "2022-06-04 21:50:02,749 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.016374, Batch Acc: 0.007528, Tokens per Sec:     3558, Lr: 0.000099\n",
      "2022-06-04 21:50:30,221 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.090051, Batch Acc: 0.007588, Tokens per Sec:     3440, Lr: 0.000098\n",
      "2022-06-04 21:50:56,914 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.061105, Batch Acc: 0.007805, Tokens per Sec:     3590, Lr: 0.000098\n",
      "2022-06-04 21:51:23,333 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.161416, Batch Acc: 0.008373, Tokens per Sec:     3558, Lr: 0.000098\n",
      "2022-06-04 21:51:49,870 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.062989, Batch Acc: 0.007999, Tokens per Sec:     3548, Lr: 0.000098\n",
      "2022-06-04 21:52:16,378 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     0.946536, Batch Acc: 0.009350, Tokens per Sec:     3535, Lr: 0.000097\n",
      "2022-06-04 21:52:43,637 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.032760, Batch Acc: 0.007729, Tokens per Sec:     3536, Lr: 0.000097\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 78.29ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11895.87ex/s]\n",
      "2022-06-04 21:52:44,467 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=17000\n",
      "2022-06-04 21:52:44,467 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:52:48,473 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:52:48,474 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.75, loss:   1.55, ppl:   4.73, acc:   0.68, generation: 3.9776[sec], evaluation: 0.0235[sec]\n",
      "2022-06-04 21:52:48,474 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:52:49,404 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/12000.ckpt\n",
      "2022-06-04 21:52:49,436 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:52:49,439 - INFO - joeynmt.training - \tSource:     Die Welt ist ein Irrenhaus.\n",
      "2022-06-04 21:52:49,439 - INFO - joeynmt.training - \tReference:  This world is just an insane asylum.\n",
      "2022-06-04 21:52:49,440 - INFO - joeynmt.training - \tHypothesis: The world is a crazy job.\n",
      "2022-06-04 21:52:49,440 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - \tSource:     Nicht Ã¶ffnen, bevor der Zug hÃ¤lt.\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - \tReference:  Do not open before the train stops.\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - \tHypothesis: Not open before the train stops.\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:52:49,444 - INFO - joeynmt.training - \tSource:     Die meisten Leute wollen nur ihre eigene Wahrheit hÃ¶ren.\n",
      "2022-06-04 21:52:49,444 - INFO - joeynmt.training - \tReference:  Most people only want to hear their own truth.\n",
      "2022-06-04 21:52:49,444 - INFO - joeynmt.training - \tHypothesis: Most people want to hear their own truth.\n",
      "2022-06-04 21:52:49,445 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:52:49,447 - INFO - joeynmt.training - \tSource:     Wo ist die Toilette?\n",
      "2022-06-04 21:52:49,447 - INFO - joeynmt.training - \tReference:  Where's the toilet?\n",
      "2022-06-04 21:52:49,447 - INFO - joeynmt.training - \tHypothesis: Where's the toilet?\n",
      "2022-06-04 21:53:15,904 - INFO - joeynmt.training - Epoch   4: total training loss 3647.91\n",
      "2022-06-04 21:53:15,905 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-06-04 21:53:17,003 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     0.816354, Batch Acc: 0.214172, Tokens per Sec:     3436, Lr: 0.000097\n",
      "2022-06-04 21:53:43,546 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.113853, Batch Acc: 0.007868, Tokens per Sec:     3539, Lr: 0.000096\n",
      "2022-06-04 21:54:10,043 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     0.957400, Batch Acc: 0.008095, Tokens per Sec:     3599, Lr: 0.000096\n",
      "2022-06-04 21:54:36,425 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.196077, Batch Acc: 0.006420, Tokens per Sec:     3602, Lr: 0.000096\n",
      "2022-06-04 21:55:04,228 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.145908, Batch Acc: 0.007529, Tokens per Sec:     3373, Lr: 0.000096\n",
      "2022-06-04 21:55:30,579 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.281533, Batch Acc: 0.006726, Tokens per Sec:     3594, Lr: 0.000095\n",
      "2022-06-04 21:55:57,203 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.276523, Batch Acc: 0.006568, Tokens per Sec:     3557, Lr: 0.000095\n",
      "2022-06-04 21:56:23,764 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     0.826367, Batch Acc: 0.008737, Tokens per Sec:     3538, Lr: 0.000095\n",
      "2022-06-04 21:56:50,199 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.088839, Batch Acc: 0.008014, Tokens per Sec:     3621, Lr: 0.000095\n",
      "2022-06-04 21:57:17,197 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.006327, Batch Acc: 0.007726, Tokens per Sec:     3505, Lr: 0.000094\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.85ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11014.40ex/s]\n",
      "2022-06-04 21:57:18,034 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=18000\n",
      "2022-06-04 21:57:18,035 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:57:22,814 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:57:22,814 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.98, loss:   1.62, ppl:   5.08, acc:   0.68, generation: 4.7524[sec], evaluation: 0.0216[sec]\n",
      "2022-06-04 21:57:23,680 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/13000.ckpt\n",
      "2022-06-04 21:57:23,709 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - \tSource:     Die AnklÃ¤ger bei Gericht mÃ¼ssen ihre Klagen erhÃ¤rten, um einen VerdÃ¤chtigen als schuldig zu beweisen.\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - \tReference:  Prosecutors in court have to substantiate their claims in order to prove a suspect is guilty.\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - \tHypothesis: The bed bedns must have to be guilty of her complaints to prove a suspect.\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:57:23,714 - INFO - joeynmt.training - \tSource:     Es ist schon 11 Uhr.\n",
      "2022-06-04 21:57:23,715 - INFO - joeynmt.training - \tReference:  It's already eleven.\n",
      "2022-06-04 21:57:23,715 - INFO - joeynmt.training - \tHypothesis: It's already eleven.\n",
      "2022-06-04 21:57:23,715 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:57:23,717 - INFO - joeynmt.training - \tSource:     Leider wÃ¼rden viele Leute Dinge glauben, die man ihnen per E-Mail sagt, die sie persÃ¶nlich erzÃ¤hlt nicht plausibel finden wÃ¼rden.\n",
      "2022-06-04 21:57:23,717 - INFO - joeynmt.training - \tReference:  Sadly many people will believe things told to them via an email which they would find implausible face-to-face.\n",
      "2022-06-04 21:57:23,718 - INFO - joeynmt.training - \tHypothesis: Unfortunately, many people think they say they say with e-mails, they wouldn't be telling them if they weren't being being being told.\n",
      "2022-06-04 21:57:23,718 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:57:23,720 - INFO - joeynmt.training - \tSource:     Ich habe morgen Unterricht.\n",
      "2022-06-04 21:57:23,720 - INFO - joeynmt.training - \tReference:  I have class tomorrow.\n",
      "2022-06-04 21:57:23,720 - INFO - joeynmt.training - \tHypothesis: I have class tomorrow.\n",
      "2022-06-04 21:57:51,775 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     0.841608, Batch Acc: 0.008215, Tokens per Sec:     3199, Lr: 0.000094\n",
      "2022-06-04 21:58:18,950 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.207774, Batch Acc: 0.008036, Tokens per Sec:     3467, Lr: 0.000094\n",
      "2022-06-04 21:58:46,356 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     0.918700, Batch Acc: 0.008609, Tokens per Sec:     3416, Lr: 0.000094\n",
      "2022-06-04 21:59:13,787 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     0.998069, Batch Acc: 0.008126, Tokens per Sec:     3441, Lr: 0.000093\n",
      "2022-06-04 21:59:40,517 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     0.870410, Batch Acc: 0.009325, Tokens per Sec:     3511, Lr: 0.000093\n",
      "2022-06-04 22:00:09,255 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.061796, Batch Acc: 0.007891, Tokens per Sec:     3215, Lr: 0.000093\n",
      "2022-06-04 22:00:36,334 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.059263, Batch Acc: 0.007125, Tokens per Sec:     3504, Lr: 0.000092\n",
      "2022-06-04 22:01:03,200 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     0.860989, Batch Acc: 0.008765, Tokens per Sec:     3516, Lr: 0.000092\n",
      "2022-06-04 22:01:30,413 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.078698, Batch Acc: 0.008641, Tokens per Sec:     3479, Lr: 0.000092\n",
      "2022-06-04 22:01:58,224 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.009859, Batch Acc: 0.008402, Tokens per Sec:     3450, Lr: 0.000092\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 78.28ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 10899.31ex/s]\n",
      "2022-06-04 22:01:59,056 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=19000\n",
      "2022-06-04 22:01:59,056 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:02:02,343 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 22:02:02,344 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  36.58, loss:   1.50, ppl:   4.50, acc:   0.70, generation: 3.2613[sec], evaluation: 0.0210[sec]\n",
      "2022-06-04 22:02:02,344 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 22:02:03,293 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/18000.ckpt\n",
      "2022-06-04 22:02:03,297 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 22:02:03,300 - INFO - joeynmt.training - \tSource:     Oh, meine weiÃŸe Hose! Sie war neu.\n",
      "2022-06-04 22:02:03,301 - INFO - joeynmt.training - \tReference:  Oh, my white pants! And they were new.\n",
      "2022-06-04 22:02:03,301 - INFO - joeynmt.training - \tHypothesis: Oh, my white pants! She was new.\n",
      "2022-06-04 22:02:03,301 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - \tSource:     UnterschÃ¤tze nicht meine Macht!\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - \tReference:  Don't underestimate my power.\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - \tHypothesis: Don't underestimate my power.\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 22:02:03,305 - INFO - joeynmt.training - \tSource:     Der Langsamste beim Versprechen ist der Treueste beim Einhalten.\n",
      "2022-06-04 22:02:03,306 - INFO - joeynmt.training - \tReference:  The slowest one to make a promise is the most faithful one in keeping it.\n",
      "2022-06-04 22:02:03,306 - INFO - joeynmt.training - \tHypothesis: The first promise is the most faithful to stop.\n",
      "2022-06-04 22:02:03,306 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 22:02:03,308 - INFO - joeynmt.training - \tSource:     Vielen Dank!\n",
      "2022-06-04 22:02:03,308 - INFO - joeynmt.training - \tReference:  Many thanks.\n",
      "2022-06-04 22:02:03,308 - INFO - joeynmt.training - \tHypothesis: Thank you very much.\n",
      "2022-06-04 22:02:30,963 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     0.925297, Batch Acc: 0.007425, Tokens per Sec:     3174, Lr: 0.000092\n",
      "2022-06-04 22:02:57,404 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     0.941763, Batch Acc: 0.008718, Tokens per Sec:     3575, Lr: 0.000091\n",
      "2022-06-04 22:03:24,113 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     0.964910, Batch Acc: 0.009509, Tokens per Sec:     3555, Lr: 0.000091\n",
      "2022-06-04 22:03:51,031 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.039105, Batch Acc: 0.008619, Tokens per Sec:     3487, Lr: 0.000091\n",
      "2022-06-04 22:04:20,103 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.018815, Batch Acc: 0.008912, Tokens per Sec:     3269, Lr: 0.000091\n",
      "2022-06-04 22:04:47,334 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     0.896303, Batch Acc: 0.009908, Tokens per Sec:     3451, Lr: 0.000090\n",
      "2022-06-04 22:05:14,250 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     0.778399, Batch Acc: 0.007928, Tokens per Sec:     3571, Lr: 0.000090\n",
      "2022-06-04 22:05:40,536 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.042853, Batch Acc: 0.007230, Tokens per Sec:     3583, Lr: 0.000090\n",
      "2022-06-04 22:06:08,838 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.000512, Batch Acc: 0.007895, Tokens per Sec:     3294, Lr: 0.000090\n",
      "2022-06-04 22:06:36,707 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.039244, Batch Acc: 0.008091, Tokens per Sec:     3357, Lr: 0.000089\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 77.61ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11546.79ex/s]\n",
      "2022-06-04 22:06:37,522 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=20000\n",
      "2022-06-04 22:06:37,523 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:06:41,183 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 22:06:41,183 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.45, loss:   1.51, ppl:   4.51, acc:   0.68, generation: 3.6310[sec], evaluation: 0.0242[sec]\n",
      "2022-06-04 22:06:41,187 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 22:06:41,190 - INFO - joeynmt.training - \tSource:     \"Danke fÃ¼r die Hilfe.\" \"Keine Ursache.\"\n",
      "2022-06-04 22:06:41,191 - INFO - joeynmt.training - \tReference:  \"Thank you for helping me.\" \"Don't mention it.\"\n",
      "2022-06-04 22:06:41,191 - INFO - joeynmt.training - \tHypothesis: \"Thanks for the help.\" \"I don't care.\"\n",
      "2022-06-04 22:06:41,191 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 22:06:41,193 - INFO - joeynmt.training - \tSource:     Ich bin so dick.\n",
      "2022-06-04 22:06:41,194 - INFO - joeynmt.training - \tReference:  I'm so fat.\n",
      "2022-06-04 22:06:41,194 - INFO - joeynmt.training - \tHypothesis: I'm so fat.\n",
      "2022-06-04 22:06:41,197 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 22:06:41,199 - INFO - joeynmt.training - \tSource:     Wenn man nicht machen kann, was man will, macht man, was man kann.\n",
      "2022-06-04 22:06:41,199 - INFO - joeynmt.training - \tReference:  When you can't do what you want, you do what you can.\n",
      "2022-06-04 22:06:41,199 - INFO - joeynmt.training - \tHypothesis: If you can't do what you want to do, you can.\n",
      "2022-06-04 22:06:41,200 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 22:06:41,202 - INFO - joeynmt.training - \tSource:     Jimmy versuchte, seine Eltern dazu zu kriegen, ihn mit seinen Freunden quer durch das Land fahren zu lassen.\n",
      "2022-06-04 22:06:41,202 - INFO - joeynmt.training - \tReference:  Jimmy tried to cajole his parents into letting him drive across the country with his friends.\n",
      "2022-06-04 22:06:41,202 - INFO - joeynmt.training - \tHypothesis: Jimmy tried to get his parents to let him drive his friends through the country through the country.\n",
      "2022-06-04 22:07:07,957 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     0.979173, Batch Acc: 0.007844, Tokens per Sec:     3406, Lr: 0.000089\n",
      "2022-06-04 22:07:16,671 - INFO - joeynmt.training - Skipping test after training\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/resume_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaHtCsyJf1yD"
   },
   "source": [
    "> ğŸ’¡ It starts counting the epochs from the beginning again, but step numbers should continue from before and you should find a \"reloading\" line in the training log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVv1ja0eCk66"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The `test` mode can be used to translate (and evaluate on) the test set specified in the configuration. We usually do this only once after we've tuned hyperparameters on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5T0OEp22BnX",
    "outputId": "327025da-0e8a-4b54-d602-019e1289c2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 17:51:13,445 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-05 17:51:13,445 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-05 17:51:13,545 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 17:51:13,546 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 17:51:13,546 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-05 17:51:28,042 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-05 17:51:28,706 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-e3360f65f1f28706.arrow\n",
      "2022-08-05 17:51:29,067 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-bbcf864065c98515.arrow\n",
      "2022-08-05 17:51:29,068 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-05 17:51:29,439 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-0099eb081810535f.arrow\n",
      "2022-08-05 17:51:29,820 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-7c540055e789733b.arrow\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers - Train dataset: None\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) â–Tom (6) ' (7) â–I (8) ? (9) â–a\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) â–Tom (6) ' (7) â–I (8) ? (9) â–a\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-05 17:51:29,821 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-05 17:51:30,294 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-05 17:51:30,297 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-05 17:51:34,692 - INFO - joeynmt.helpers - Load model from /home/lconti/en-pt_tatoeba/models/no_tf/18000.ckpt.\n",
      "2022-08-05 17:51:35,198 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-06617e88cb272dab.arrow\n",
      "2022-08-05 17:51:35,612 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-3d931698fbe82046.arrow\n",
      "2022-08-05 17:51:35,613 - INFO - joeynmt.prediction - Decoding on dev set...\n",
      "2022-08-05 17:51:35,613 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:53:40,606 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:53:40,607 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  21.30, generation: 124.8949[sec], evaluation: 0.0841[sec]\n",
      "2022-08-05 17:53:40,981 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-60b0c2cf34a85431.arrow\n",
      "2022-08-05 17:53:41,403 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-087e226f4078b277.arrow\n",
      "2022-08-05 17:53:41,405 - INFO - joeynmt.prediction - Decoding on test set...\n",
      "2022-08-05 17:53:41,405 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:55:40,672 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:55:40,672 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  26.05, generation: 119.1568[sec], evaluation: 0.0938[sec]\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt test {data_dir}/config_no_tf.yaml --ckpt {model_dir}/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy5Y4Qr_3p3I"
   },
   "source": [
    "> âš  In beam search, the batch size is expanded {beam_size} times. For instance, if batch_size=10, batch_type=sentence and beam_size=5, joeynmt internally creates a batch of length 10*5=50. It may cause an out-of-memory error. Please specify the batch_size in `testing` section of config.yaml by taking this into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ripZMg6kCqzd"
   },
   "source": [
    "The `translate` mode is more interactive and takes prompts to translate interactively.\n",
    "\n",
    "Let's Translate a few examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEcyEwpS1Pvi",
    "outputId": "2259e7d7-ca22-409c-f6ea-593fa3b5de83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:43:59,643 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:44:17,067 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:44:17,429 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:44:21,857 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Ãœbersetzung macht SpaÃŸ!\n",
      "2022-06-04 22:46:22,281 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:46:22,379 - INFO - joeynmt.prediction - Generation took 0.0963[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Ãœbersetzung Sinn?\n",
      "2022-06-04 22:47:21,390 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:47:21,489 - INFO - joeynmt.prediction - Generation took 0.0975[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\n",
      "Please enter a source sentence:\n",
      "\n",
      "Bye.\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 2037, in shutdown\n",
      "    h.close()\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1103, in close\n",
      "    stream.close()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH5QwafwIYZu"
   },
   "source": [
    "You can also get the n-best hypotheses (up to the size of the beam, in our example 5), not only the highest scoring one. The better your model gets, the more interesting should the alternatives be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "GzpnG_3IJwkj"
   },
   "outputs": [],
   "source": [
    "nbest_config = config.replace('n_best: 1', 'n_best: 5')\\\n",
    "  .replace('#return_prob: \"hyp\"', 'return_prob: \"hyp\"')\n",
    "\n",
    "with (Path(data_dir) / \"nbest_config.yaml\").open('w') as f:\n",
    "    f.write(nbest_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nya4YXTGRurr",
    "outputId": "b879c6e2-2d12-4d22-d1ad-d441f87eaae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:48:11,849 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:48:29,174 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:48:29,557 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:48:34,476 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Ãœbersetzung macht SpaÃŸ!\n",
      "2022-06-04 22:49:28,668 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:28,766 - INFO - joeynmt.prediction - Generation took 0.0962[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\ttokens: ['â–Don', \"'\", 't', 'â–drink', 'â–translation', 'â–is', 'â–fun', '.', '</s>']\n",
      "\tsequence score: -3.362283706665039\n",
      "#2: Don't make any translation fun.\n",
      "\ttokens: ['â–Don', \"'\", 't', 'â–make', 'â–any', 'â–translation', 'â–fun', '.', '</s>']\n",
      "\tsequence score: -3.677445411682129\n",
      "#3: Don't make sense to the machine.\n",
      "\ttokens: ['â–Don', \"'\", 't', 'â–make', 'â–sense', 'â–to', 'â–the', 'â–machine', '.', '</s>']\n",
      "\tsequence score: -3.9387967586517334\n",
      "#4: Don't drink translation is fun!\n",
      "\ttokens: ['â–Don', \"'\", 't', 'â–drink', 'â–translation', 'â–is', 'â–fun', '!', '</s>']\n",
      "\tsequence score: -3.9769201278686523\n",
      "#5: Don't make a good translation.\n",
      "\ttokens: ['â–Don', \"'\", 't', 'â–make', 'â–a', 'â–good', 'â–translation', '.', '</s>']\n",
      "\tsequence score: -4.1912617683410645\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Ãœbersetzung Sinn?\n",
      "2022-06-04 22:49:47,934 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:48,022 - INFO - joeynmt.prediction - Generation took 0.0862[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\ttokens: ['â–When', 'â–does', 'â–machine', 'â–make', 'â–sense', '?', '</s>']\n",
      "\tsequence score: -2.5040993690490723\n",
      "#2: When does the translation make sense?\n",
      "\ttokens: ['â–When', 'â–does', 'â–the', 'â–translation', 'â–make', 'â–sense', '?', '</s>']\n",
      "\tsequence score: -2.5658979415893555\n",
      "#3: When does machine make sense to sense?\n",
      "\ttokens: ['â–When', 'â–does', 'â–machine', 'â–make', 'â–sense', 'â–to', 'â–sense', '?', '</s>']\n",
      "\tsequence score: -3.325543165206909\n",
      "#4: When does machine make sense to make?\n",
      "\ttokens: ['â–When', 'â–does', 'â–machine', 'â–make', 'â–sense', 'â–to', 'â–make', '?', '</s>']\n",
      "\tsequence score: -3.422945976257324\n",
      "#5: When does the translation use?\n",
      "\ttokens: ['â–When', 'â–does', 'â–the', 'â–translation', 'â–use', '?', '</s>']\n",
      "\tsequence score: -3.6186904907226562\n",
      "\n",
      "Please enter a source sentence:\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 57, in main\n",
      "    output_path=args.output_path,\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/nbest_config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28sXN0GNIYAM"
   },
   "source": [
    "> ğŸ’¡ In BPE decoding, there are multiple ways to tokenize one sequence. That is, the same output string sequence might appear multiple times in the n best list, because they have different tokenization and thus different sequence in the generation.\n",
    "> For instance, say 3-best generation were:\n",
    "> ```\n",
    "> #1 best ['â–', 'N', 'e', 'w', 'â–York']\n",
    "> #2 best ['â–', 'New', 'â–York']\n",
    "> #3 best ['â–', 'New', 'â–Y', 'o', 'r', 'k']\n",
    "> ````\n",
    "All three were different in next-token prediction, but ended up the same string sequence `New York` after being un-bpe-ed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "\n",
    "### plotting learning curves\n",
    "\n",
    "`plot_validations.py` script will generate validation learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/lconti/joeynmt/scripts/plot_validations.py {model_dir} --output_path /home/lconti/results/no_tf_learning_curve.png"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "quick-start-with-joeynmt2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('my_jnmt': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "86dc5b16c9dc4e6fbdf96ccbf0568163a9040f9d731ebaeec1938114527cf668"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1795a2b1d6354fd2b1c69b12c9487f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27e4365db7d644d486745b9f334d06db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ff1b97d66404e488862d694facdfc23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fdb10ecb2040f8bd5e07834b422ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a00af231d8a44e9972f6916948418fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79a7716ac54b4f50981033df15f06f8e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c26c414246154062bbd0682340613e40",
      "value": 1
     }
    },
    "4a5a5a13229f404f9e4085751b0e2104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c447e15d9354c3c9a47c3ff843e74ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51afb334205f4a26b43fd6ddd0036cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52fc941d087447aa88ca1336e89d1d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6505b17c75e64c709fc6a91cc362444d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a841a34dd549f4bbe9fed60fc03853",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_31fdb10ecb2040f8bd5e07834b422ca0",
      "value": "Generating train split: "
     }
    },
    "65a841a34dd549f4bbe9fed60fc03853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6690085804c940ccb8dbd87e3f563678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6505b17c75e64c709fc6a91cc362444d",
       "IPY_MODEL_3a00af231d8a44e9972f6916948418fd",
       "IPY_MODEL_e79f7e89c05d4ce4935f53929b8aaaa1"
      ],
      "layout": "IPY_MODEL_4c447e15d9354c3c9a47c3ff843e74ce"
     }
    },
    "6d393ab342cc46109d1c723d736320ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52fc941d087447aa88ca1336e89d1d0f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1795a2b1d6354fd2b1c69b12c9487f0b",
      "value": " 1/1 [00:00&lt;00:00, 24.21ba/s]"
     }
    },
    "79a7716ac54b4f50981033df15f06f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9c838f660f7a46da92cfdb54c25fcb58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e9312e5e6646739ae3e6738de0e90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff1b97d66404e488862d694facdfc23",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_27e4365db7d644d486745b9f334d06db",
      "value": "Flattening the indices: 100%"
     }
    },
    "b669da1ab44a4e8cbfa6674f2dac1e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14c2320cc944c469a52a5579ee471bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c838f660f7a46da92cfdb54c25fcb58",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51afb334205f4a26b43fd6ddd0036cbe",
      "value": 1
     }
    },
    "c26c414246154062bbd0682340613e40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c951a5fc9f1540249dbb03a0ecf2f2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e9312e5e6646739ae3e6738de0e90c",
       "IPY_MODEL_c14c2320cc944c469a52a5579ee471bb",
       "IPY_MODEL_6d393ab342cc46109d1c723d736320ad"
      ],
      "layout": "IPY_MODEL_b669da1ab44a4e8cbfa6674f2dac1e5b"
     }
    },
    "e73400a7748241d29340d21f94784a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e79f7e89c05d4ce4935f53929b8aaaa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e73400a7748241d29340d21f94784a2e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4a5a5a13229f404f9e4085751b0e2104",
      "value": " 304551/0 [00:13&lt;00:00, 21566.82 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
