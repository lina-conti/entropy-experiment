{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k051L-Y3rTu"
   },
   "source": [
    "\n",
    "# Fine-tuning with no teacher forcing an en-pt NMT model trained with JoeyNMT 2.0 on the tatoeba corpus\n",
    "\n",
    "This notebook is based on [this demo](https://github.com/joeynmt/joeynmt/blob/main/notebooks/quick-start-with-joeynmt2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un8VWHfq5a-T"
   },
   "source": [
    "> ⚠ **Important:** Before you start, set runtime type to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_s7s4uevEtx",
    "outputId": "c58870bf-7812-4013-b93e-e323e84dbc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  5 11:08:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 26%   27C    P0    53W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 29%   28C    P0    53W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 29%   29C    P0    58W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:D9:00.0 Off |                  N/A |\n",
      "| 10%   26C    P0    57W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "242SJA2q5dRr"
   },
   "source": [
    "Make sure that you have a compatible PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pQwoOS-OvMLf",
    "outputId": "0bd93e90-3ff0-4812-f5a1-7eb255a5116e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rHdT56SF1J3"
   },
   "source": [
    "Install joeynmt (it's important to clone it from my fork, so teacher forcing can be deactivated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxQUAeHKR7Im",
    "outputId": "eb9697ec-8159-4cf5-a68c-3e2768d6d203"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/lina-conti/joeynmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/lconti/joeynmt\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: future in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.18.2)\n",
      "Requirement already satisfied: pillow in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.23.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (62.3.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.10.1+cu111)\n",
      "Requirement already satisfied: protobuf==3.20.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (3.20.1)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.9.0)\n",
      "Requirement already satisfied: sacrebleu>=2.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.2.0)\n",
      "Requirement already satisfied: sentencepiece in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.1.96)\n",
      "Requirement already satisfied: subword-nmt in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.3.8)\n",
      "Requirement already satisfied: matplotlib in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (3.5.2)\n",
      "Requirement already satisfied: seaborn in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (6.0)\n",
      "Requirement already satisfied: six>=1.12 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.14.1)\n",
      "Requirement already satisfied: pylint in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.14.5)\n",
      "Requirement already satisfied: yapf in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.32.0)\n",
      "Requirement already satisfied: flake8 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (5.0.3)\n",
      "Requirement already satisfied: pytest in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (7.1.2)\n",
      "Requirement already satisfied: datasets in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.4.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (0.8.10)\n",
      "Requirement already satisfied: portalocker in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (2.5.1)\n",
      "Requirement already satisfied: lxml in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (4.9.1)\n",
      "Requirement already satisfied: regex in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (2022.7.25)\n",
      "Requirement already satisfied: colorama in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (0.4.5)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (1.47.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (2.9.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torch>=1.10.0->joeynmt==2.0.0) (4.3.0)\n",
      "Requirement already satisfied: packaging in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (2022.7.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (1.4.3)\n",
      "Requirement already satisfied: aiohttp in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (8.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: multiprocess in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.70.13)\n",
      "Requirement already satisfied: xxhash in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyflakes<2.6.0,>=2.5.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from flake8->joeynmt==2.0.0) (2.5.0)\n",
      "Requirement already satisfied: pycodestyle<2.10.0,>=2.9.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from flake8->joeynmt==2.0.0) (2.9.0)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from flake8->joeynmt==2.0.0) (0.7.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (0.11.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: tomlkit>=0.10.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (0.11.1)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (5.10.1)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (2.5.2)\n",
      "Requirement already satisfied: astroid<=2.12.0-dev0,>=2.11.6 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (2.11.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (22.1.0)\n",
      "Requirement already satisfied: iniconfig in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (1.11.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from seaborn->joeynmt==2.0.0) (1.9.0)\n",
      "Requirement already satisfied: mock in /home/lconti/my_jnmt/lib/python3.9/site-packages (from subword-nmt->joeynmt==2.0.0) (4.0.3)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from astroid<=2.12.0-dev0,>=2.11.6->pylint->joeynmt==2.0.0) (1.7.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==2.0.0) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/lconti/my_jnmt/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->joeynmt==2.0.0) (3.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==2.0.0) (4.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pandas->datasets->joeynmt==2.0.0) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=1.15->joeynmt==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (1.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==2.0.0) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==2.0.0) (3.2.0)\n",
      "Installing collected packages: joeynmt\n",
      "  Attempting uninstall: joeynmt\n",
      "    Found existing installation: joeynmt 2.0.0\n",
      "    Uninstalling joeynmt-2.0.0:\n",
      "      Successfully uninstalled joeynmt-2.0.0\n",
      "  Running setup.py develop for joeynmt\n",
      "Successfully installed joeynmt-2.0.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip3 install -e /home/lconti/joeynmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1RMfXeT-V1m"
   },
   "source": [
    "### Dataset and vocabulary\n",
    "\n",
    "The dataset and vocabulary are the same as for the pre-trained model, so we don't need to compute them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/lconti/en-pt_tatoeba\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6cN9CPtAaPl"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Joey NMT reads model and training hyperparameters from a configuration file. We're generating this now to configure paths in the appropriate places.\n",
    "\n",
    "The configuration below builds a small Transformer model with shared embeddings between source and target language on the base of the subword vocabularies created above.\n",
    "\n",
    "Note the \"teacher_forcing\" configuration in \"model\" — this is specific to my fork of joeynmt. It is where you can choose between \"on\", \"off\" or \"alternating\" (default is on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model_dir = \"/home/lconti/en-pt_tatoeba/models/finetune_tf\"\n",
    "new_model_dir = \"/home/lconti/en-pt_tatoeba/models/finetune2_tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fJML2jYR1PlG"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"tatoeba_enpt_finetune2_tf_sp\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"{data_dir}/train\"\n",
    "    dev: \"{data_dir}/validation\"\n",
    "    test: \"{data_dir}/test\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    #dataset_cfg:           # not necessary for manually saved pyarray daraset\n",
    "    #    name: \"en-pt\"\n",
    "    sample_dev_subset: 200\n",
    "    src:\n",
    "        lang: \"en\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "    trg:\n",
    "        lang: \"pt\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "\"\"\".format(data_dir=data_dir)\n",
    "with (Path(data_dir) / \"config_finetune2.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OJcLOr_S2BTD"
   },
   "outputs": [],
   "source": [
    "config += \"\"\"\n",
    "testing:\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 256\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 100\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    #return_prob: \"hyp\"\n",
    "    #return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "\n",
    "training:\n",
    "    load_model: \"{old_model_dir}/latest.ckpt\"\n",
    "    reset_best_ckpt: True\n",
    "    reset_scheduler: True\n",
    "    reset_optimizer: True\n",
    "    #reset_iter_state: False\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999]\n",
    "    scheduling: \"warmupinversesquareroot\"\n",
    "    learning_rate_warmup: 2000\n",
    "    learning_rate: 0.0002\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    loss: \"crossentropy\"\n",
    "    batch_size: 512\n",
    "    batch_type: \"token\"\n",
    "    batch_multiplier: 4\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 10\n",
    "    updates: 40000\n",
    "    validation_freq: 1000\n",
    "    logging_freq: 100\n",
    "    model_dir: \"{new_model_dir}\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\".format(old_model_dir=old_model_dir, new_model_dir=new_model_dir)\n",
    "with (Path(data_dir) / \"config_finetune2.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w45HbBfeMW38"
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH0-HBLLBLmR"
   },
   "source": [
    "### Run training\n",
    "⏳ This will take a while. Model parameters will be stored on mounted google drive. The log reports the training process, look out for the prints of example translations and the BLEU evaluation scores to get an impression of the current quality.\n",
    "\n",
    "> ⛔ If you execute this twice, you might get an error that the model directory already exists. You can specify in the configuration to overwrite it, or delete it manually (`!rm -r {model_dir}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTbfgVOq2BfB",
    "outputId": "b667e5d9-4587-4f92-f8a5-308b7826ac51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 11:16:21,054 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-05 11:16:21,054 - INFO - joeynmt.helpers -                           cfg.name : tatoeba_enpt_finetune2_tf_sp\n",
      "2022-08-05 11:16:21,054 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-08-05 11:16:21,054 - INFO - joeynmt.helpers -                     cfg.data.train : /home/lconti/en-pt_tatoeba/train\n",
      "2022-08-05 11:16:21,054 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/lconti/en-pt_tatoeba/validation\n",
      "2022-08-05 11:16:21,054 - INFO - joeynmt.helpers -                      cfg.data.test : /home/lconti/en-pt_tatoeba/test\n",
      "2022-08-05 11:16:21,054 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : 200\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/lconti/en-pt_tatoeba/vocab.txt\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : sentencepiece\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/sp.model\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : pt\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/lconti/en-pt_tatoeba/vocab.txt\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : sentencepiece\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/sp.model\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 256\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-08-05 11:16:21,055 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/lconti/en-pt_tatoeba/models/finetune_tf/latest.ckpt\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 2000\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0002\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -            cfg.training.batch_size : 512\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -                cfg.training.epochs : 10\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -               cfg.training.updates : 40000\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/lconti/en-pt_tatoeba/models/finetune2_tf\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -             cfg.training.overwrite : False\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
      "2022-08-05 11:16:21,056 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\n",
      "2022-08-05 11:16:21,057 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-08-05 11:16:24,166 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-05 11:16:24,249 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 11:16:24,249 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 11:16:24,249 - INFO - joeynmt.data - Loading train set...\n",
      "2022-08-05 11:16:24,502 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/train/cache-4be94cce75c187c1.arrow\n",
      "2022-08-05 11:16:24,517 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/train/cache-0066646d166ee7b1.arrow\n",
      "2022-08-05 11:16:24,521 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-05 11:16:36,693 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-05 11:16:37,026 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-e3360f65f1f28706.arrow\n",
      "2022-08-05 11:16:37,351 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-bbcf864065c98515.arrow\n",
      "2022-08-05 11:16:37,352 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-05 11:16:37,677 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-0099eb081810535f.arrow\n",
      "2022-08-05 11:16:38,004 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-7c540055e789733b.arrow\n",
      "2022-08-05 11:16:38,005 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-05 11:16:38,005 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=215642, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=train, path=/home/lconti/en-pt_tatoeba/train)\n",
      "2022-08-05 11:16:38,005 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-05 11:16:38,005 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-05 11:16:38,006 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] ▁What ▁do ▁you ▁want ▁now ?\n",
      "\t[TRG] ▁O ▁que ▁você ▁deseja ▁agora ?\n",
      "2022-08-05 11:16:38,006 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-05 11:16:38,006 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-05 11:16:38,006 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-05 11:16:38,006 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-05 11:16:38,020 - WARNING - joeynmt.tokenizers - /home/lconti/en-pt_tatoeba/models/finetune2_tf/sp.model already exists. Stop copying.\n",
      "2022-08-05 11:16:38,021 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-05 11:16:38,322 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-05 11:16:38,327 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-05 11:16:38,328 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-08-05 11:16:41,643 - INFO - joeynmt.builders - Adam(lr=0.0002, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-08-05 11:16:41,643 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=2000, decay_rate=0.008944, peak_rate=0.0002, min_rate=1e-08)\n",
      "2022-08-05 11:16:41,643 - INFO - joeynmt.training - Loading model from /home/lconti/en-pt_tatoeba/models/finetune_tf/latest.ckpt\n",
      "2022-08-05 11:16:41,852 - INFO - joeynmt.helpers - Load model from /home/lconti/en-pt_tatoeba/models/finetune_tf/30000.ckpt.\n",
      "2022-08-05 11:16:41,906 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 4\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 128\n",
      "\teffective batch size (w. parallel & accumulation): 2048\n",
      "2022-08-05 11:16:41,906 - INFO - joeynmt.training - EPOCH 1\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/config_finetune2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7qULQu0B5Yl"
   },
   "source": [
    "### Continue training after interruption\n",
    "To continue after an interruption, the configuration needs to be modified in the following places:\n",
    "\n",
    "- `load_model` to point to the checkpoint to load.\n",
    "- `reset_*` options (must be False) to resume the previous session.\n",
    "- `model_dir` to create a new directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When resuming training, I get the bug described [here](https://github.com/pytorch/pytorch/issues/80809). I haven't been able to find another solution besides downgrading pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.10.1+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torch-1.10.1%2Bcu111-cp39-cp39-linux_x86_64.whl (2137.7 MB)\n",
      "Collecting torchvision==0.11.2+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torchvision-0.11.2%2Bcu111-cp39-cp39-linux_x86_64.whl (24.5 MB)\n",
      "Collecting torchaudio==0.10.1\n",
      "  Using cached https://download.pytorch.org/whl/rocm4.1/torchaudio-0.10.1%2Brocm4.1-cp39-cp39-linux_x86_64.whl (2.7 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torch==1.10.1+cu111) (4.3.0)\n",
      "Requirement already satisfied: numpy in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torchvision==0.11.2+cu111) (1.23.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torchvision==0.11.2+cu111) (9.2.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "Successfully installed torch-1.10.1+cu111 torchaudio-0.10.1+rocm4.1 torchvision-0.11.2+cu111\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 62.3.2\n",
      "    Uninstalling setuptools-62.3.2:\n",
      "      Successfully uninstalled setuptools-62.3.2\n",
      "Successfully installed setuptools-59.5.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install setuptools==59.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr3mEDga2BiJ"
   },
   "outputs": [],
   "source": [
    "resume_config = config\\\n",
    "  .replace('#load_model:', 'load_model:')\\\n",
    "  .replace('#reset_best_ckpt: False', 'reset_best_ckpt: False')\\\n",
    "  .replace('#reset_scheduler: False', 'reset_scheduler: False')\\\n",
    "  .replace('#reset_optimizer: False', 'reset_optimizer: False')\\\n",
    "  .replace('#reset_iter_state: False', 'reset_iter_state: False')\\\n",
    "  .replace(f'model_dir: \"{model_dir}\"', f'model_dir: \"{model_dir}_resume\"')\n",
    "\n",
    "with (Path(data_dir) / \"resume_config.yaml\").open('w') as f:\n",
    "    f.write(resume_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgH1vAsV2Bkw",
    "outputId": "56439036-1e0d-4fb9-cffd-257f09903d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 20:53:16,099 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 20:53:16,100 - INFO - joeynmt.helpers -                           cfg.name : tatoeba_deen_sp\n",
      "2022-06-04 20:53:16,100 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-06-04 20:53:16,100 - INFO - joeynmt.helpers -                     cfg.data.train : /content/drive/MyDrive/tatoeba_deen/train\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -                       cfg.data.dev : /content/drive/MyDrive/tatoeba_deen/validation\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -                      cfg.data.test : /content/drive/MyDrive/tatoeba_deen/test\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : 200\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\n",
      "2022-06-04 20:53:16,101 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /content/drive/MyDrive/tatoeba_deen/vocab.txt\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : sentencepiece\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.model_file : /content/drive/MyDrive/tatoeba_deen/sp.model\n",
      "2022-06-04 20:53:16,102 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /content/drive/MyDrive/tatoeba_deen/vocab.txt\n",
      "2022-06-04 20:53:16,103 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : sentencepiece\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.model_file : /content/drive/MyDrive/tatoeba_deen/sp.model\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 256\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\n",
      "2022-06-04 20:53:16,104 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -            cfg.training.load_model : /content/drive/MyDrive/models/tatoeba_deen/latest.ckpt\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-06-04 20:53:16,105 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 4000\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0002\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-06-04 20:53:16,106 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -            cfg.training.batch_size : 512\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -                cfg.training.epochs : 10\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -             cfg.training.model_dir : /content/drive/MyDrive/models/tatoeba_deen_resume\n",
      "2022-06-04 20:53:16,107 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 5\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-06-04 20:53:16,108 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-06-04 20:53:16,109 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-06-04 20:53:16,110 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\n",
      "2022-06-04 20:53:16,111 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-06-04 20:53:16,154 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-06-04 20:53:16,280 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 20:53:16,281 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 20:53:16,281 - INFO - joeynmt.data - Loading train set...\n",
      "2022-06-04 20:53:16,350 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n",
      "2022-06-04 20:53:18,870 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/train/cache-e9418b4482dd4017.arrow\n",
      "2022-06-04 20:53:18,903 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/train/cache-50210139b427e8ae.arrow\n",
      "2022-06-04 20:53:18,951 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-06-04 20:53:34,005 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-06-04 20:53:34,363 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-ce681e2c839418f9.arrow\n",
      "2022-06-04 20:53:34,702 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-0fdab69a63fd22e6.arrow\n",
      "2022-06-04 20:53:34,705 - INFO - joeynmt.data - Loading test set...\n",
      "2022-06-04 20:53:35,048 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/test/cache-4f5eef71bf1d5346.arrow\n",
      "2022-06-04 20:53:35,382 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/test/cache-a5f9783fc69136ac.arrow\n",
      "2022-06-04 20:53:35,385 - INFO - joeynmt.data - Data loaded.\n",
      "2022-06-04 20:53:35,385 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=305367, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, split=train, path=/content/drive/MyDrive/tatoeba_deen/train)\n",
      "2022-06-04 20:53:35,386 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=de, trg_lang=en, has_trg=True, random_subset=200, split=validation, path=/content/drive/MyDrive/tatoeba_deen/validation)\n",
      "2022-06-04 20:53:35,386 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1, split=test, path=/content/drive/MyDrive/tatoeba_deen/test)\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] ▁Sie ▁können ▁nicht ▁reiten .\n",
      "\t[TRG] ▁You ▁can ' t ▁ride ▁a ▁horse .\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ▁Tom (7) ' (8) ? (9) ▁I\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ▁Tom (7) ' (8) ? (9) ▁I\n",
      "2022-06-04 20:53:35,387 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-06-04 20:53:35,388 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-06-04 20:53:35,437 - WARNING - joeynmt.tokenizers - /content/drive/MyDrive/models/tatoeba_deen_resume/sp.model already exists. Stop copying.\n",
      "2022-06-04 20:53:35,447 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 20:53:35,800 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 20:53:35,812 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-06-04 20:53:35,813 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-06-04 20:53:40,214 - INFO - joeynmt.builders - Adam(lr=0.0002, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-06-04 20:53:40,214 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=4000, decay_rate=0.012649, peak_rate=0.0002, min_rate=1e-08)\n",
      "2022-06-04 20:53:40,215 - INFO - joeynmt.training - Loading model from /content/drive/MyDrive/models/tatoeba_deen/latest.ckpt\n",
      "2022-06-04 20:53:40,758 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen/4161.ckpt.\n",
      "2022-06-04 20:53:40,844 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 512\n",
      "\teffective batch size (w. parallel & accumulation): 2048\n",
      "2022-06-04 20:53:40,845 - INFO - joeynmt.training - EPOCH 1\n",
      "2022-06-04 20:53:51,110 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.288305, Batch Acc: 0.014659, Tokens per Sec:     3569, Lr: 0.000195\n",
      "2022-06-04 20:54:18,849 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.190759, Batch Acc: 0.006044, Tokens per Sec:     3472, Lr: 0.000193\n",
      "2022-06-04 20:54:45,568 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.643943, Batch Acc: 0.004332, Tokens per Sec:     3534, Lr: 0.000191\n",
      "2022-06-04 20:55:12,250 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.348576, Batch Acc: 0.005454, Tokens per Sec:     3546, Lr: 0.000189\n",
      "2022-06-04 20:55:39,271 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.120830, Batch Acc: 0.006304, Tokens per Sec:     3481, Lr: 0.000187\n",
      "2022-06-04 20:56:05,869 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     2.034187, Batch Acc: 0.005908, Tokens per Sec:     3589, Lr: 0.000185\n",
      "2022-06-04 20:56:32,403 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     2.197401, Batch Acc: 0.006395, Tokens per Sec:     3589, Lr: 0.000183\n",
      "2022-06-04 20:56:59,968 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     2.014929, Batch Acc: 0.006460, Tokens per Sec:     3431, Lr: 0.000181\n",
      "2022-06-04 20:57:26,724 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     2.061943, Batch Acc: 0.006459, Tokens per Sec:     3495, Lr: 0.000179\n",
      "2022-06-04 20:57:27,075 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-d2b87a894115655b.arrow\n",
      "2022-06-04 20:57:27,413 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/MyDrive/tatoeba_deen/validation/cache-f6dc53774c4904a4.arrow\n",
      "2022-06-04 20:57:27,431 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=5000\n",
      "2022-06-04 20:57:27,432 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 20:57:37,835 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 20:57:37,835 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.44, loss:   2.44, ppl:  11.53, acc:   0.55, generation: 10.3712[sec], evaluation: 0.0263[sec]\n",
      "2022-06-04 20:57:37,836 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 20:57:38,886 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 20:57:38,890 - INFO - joeynmt.training - \tSource:     Die neulichen Skandale, in die Messdiener und religiöse Oberhäupter verwickelt waren, haben den Glauben, den die Leute in die Kirche haben, unterminiert.\n",
      "2022-06-04 20:57:38,891 - INFO - joeynmt.training - \tReference:  The recent scandals involving altar boys and religious leaders have undermined the faith people have in the Church.\n",
      "2022-06-04 20:57:38,891 - INFO - joeynmt.training - \tHypothesis: The other day, the ttttsss and played out of the most tttttsssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "2022-06-04 20:57:38,891 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 20:57:38,893 - INFO - joeynmt.training - \tSource:     Wie bist du auf diese verrückte Idee gekommen?\n",
      "2022-06-04 20:57:38,894 - INFO - joeynmt.training - \tReference:  How did you come up with this crazy idea?\n",
      "2022-06-04 20:57:38,894 - INFO - joeynmt.training - \tHypothesis: How did you come out of this idea?\n",
      "2022-06-04 20:57:38,894 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - \tSource:     Ein Leben ohne Liebe hat überhaupt keinen Sinn.\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - \tReference:  Life without love has no meaning at all.\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - \tHypothesis: A life without not a good way.\n",
      "2022-06-04 20:57:38,897 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 20:57:38,899 - INFO - joeynmt.training - \tSource:     Das wäre etwas, das ich programmieren sollte.\n",
      "2022-06-04 20:57:38,900 - INFO - joeynmt.training - \tReference:  It would be something I'd have to program.\n",
      "2022-06-04 20:57:38,900 - INFO - joeynmt.training - \tHypothesis: That would be something I should make.\n",
      "2022-06-04 20:58:06,718 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     2.013755, Batch Acc: 0.006844, Tokens per Sec:     3180, Lr: 0.000177\n",
      "2022-06-04 20:58:33,871 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     1.729190, Batch Acc: 0.006564, Tokens per Sec:     3479, Lr: 0.000175\n",
      "2022-06-04 20:59:00,603 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     1.810303, Batch Acc: 0.006589, Tokens per Sec:     3509, Lr: 0.000174\n",
      "2022-06-04 20:59:28,238 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     1.856463, Batch Acc: 0.006088, Tokens per Sec:     3376, Lr: 0.000172\n",
      "2022-06-04 20:59:55,245 - INFO - joeynmt.training - Epoch   1, Step:     5500, Batch Loss:     1.824483, Batch Acc: 0.006628, Tokens per Sec:     3553, Lr: 0.000171\n",
      "2022-06-04 21:00:21,641 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     2.152315, Batch Acc: 0.005813, Tokens per Sec:     3591, Lr: 0.000169\n",
      "2022-06-04 21:00:48,463 - INFO - joeynmt.training - Epoch   1, Step:     5700, Batch Loss:     1.766821, Batch Acc: 0.007215, Tokens per Sec:     3529, Lr: 0.000168\n",
      "2022-06-04 21:01:15,064 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     1.598522, Batch Acc: 0.006001, Tokens per Sec:     3558, Lr: 0.000166\n",
      "2022-06-04 21:01:41,877 - INFO - joeynmt.training - Epoch   1, Step:     5900, Batch Loss:     1.886486, Batch Acc: 0.006554, Tokens per Sec:     3534, Lr: 0.000165\n",
      "2022-06-04 21:02:09,422 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     1.926557, Batch Acc: 0.006216, Tokens per Sec:     3417, Lr: 0.000163\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 72.90ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11569.76ex/s]\n",
      "2022-06-04 21:02:10,261 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=6000\n",
      "2022-06-04 21:02:10,262 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:02:13,849 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:02:13,849 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  23.15, loss:   2.18, ppl:   8.83, acc:   0.59, generation: 3.5618[sec], evaluation: 0.0203[sec]\n",
      "2022-06-04 21:02:13,850 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:02:14,848 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:02:14,852 - INFO - joeynmt.training - \tSource:     Das fängt nicht vor acht Uhr dreißig an.\n",
      "2022-06-04 21:02:14,852 - INFO - joeynmt.training - \tReference:  It won't start before eight-thirty.\n",
      "2022-06-04 21:02:14,852 - INFO - joeynmt.training - \tHypothesis: That won't be thirty o'clock.\n",
      "2022-06-04 21:02:14,853 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - \tSource:     Sie fragt, wie das möglich ist.\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - \tReference:  She's asking how that's possible.\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - \tHypothesis: She will ask how it is possible.\n",
      "2022-06-04 21:02:14,855 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:02:14,857 - INFO - joeynmt.training - \tSource:     Bill Clinton sprach eine mehrdeutige Sprache, als er gebeten wurde, sein Verhältnis mit Monika Lewinsky zu beschreiben.\n",
      "2022-06-04 21:02:14,858 - INFO - joeynmt.training - \tReference:  Bill Clinton spoke in ambiguous language when asked to describe his relationship with Monica Lewinsky.\n",
      "2022-06-04 21:02:14,858 - INFO - joeynmt.training - \tHypothesis: Bill was a more-ttttt of the language when he asked him to because of the 's-meet.\n",
      "2022-06-04 21:02:14,858 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:02:14,860 - INFO - joeynmt.training - \tSource:     Sie will nicht darüber sprechen.\n",
      "2022-06-04 21:02:14,860 - INFO - joeynmt.training - \tReference:  She doesn't want to talk about it.\n",
      "2022-06-04 21:02:14,860 - INFO - joeynmt.training - \tHypothesis: She doesn't want to talk about it.\n",
      "2022-06-04 21:02:42,777 - INFO - joeynmt.training - Epoch   1, Step:     6100, Batch Loss:     2.206654, Batch Acc: 0.005183, Tokens per Sec:     3209, Lr: 0.000162\n",
      "2022-06-04 21:03:09,323 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     2.004051, Batch Acc: 0.005673, Tokens per Sec:     3592, Lr: 0.000161\n",
      "2022-06-04 21:03:36,067 - INFO - joeynmt.training - Epoch   1, Step:     6300, Batch Loss:     2.067890, Batch Acc: 0.005431, Tokens per Sec:     3552, Lr: 0.000159\n",
      "2022-06-04 21:04:02,975 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     1.810111, Batch Acc: 0.007169, Tokens per Sec:     3520, Lr: 0.000158\n",
      "2022-06-04 21:04:30,526 - INFO - joeynmt.training - Epoch   1, Step:     6500, Batch Loss:     1.852140, Batch Acc: 0.006594, Tokens per Sec:     3435, Lr: 0.000157\n",
      "2022-06-04 21:04:57,488 - INFO - joeynmt.training - Epoch   1, Step:     6600, Batch Loss:     1.497860, Batch Acc: 0.008099, Tokens per Sec:     3471, Lr: 0.000156\n",
      "2022-06-04 21:05:24,082 - INFO - joeynmt.training - Epoch   1, Step:     6700, Batch Loss:     1.597221, Batch Acc: 0.007165, Tokens per Sec:     3542, Lr: 0.000155\n",
      "2022-06-04 21:05:50,676 - INFO - joeynmt.training - Epoch   1, Step:     6800, Batch Loss:     1.645426, Batch Acc: 0.006132, Tokens per Sec:     3551, Lr: 0.000153\n",
      "2022-06-04 21:06:17,313 - INFO - joeynmt.training - Epoch   1, Step:     6900, Batch Loss:     1.655118, Batch Acc: 0.006630, Tokens per Sec:     3522, Lr: 0.000152\n",
      "2022-06-04 21:06:44,486 - INFO - joeynmt.training - Epoch   1, Step:     7000, Batch Loss:     1.632364, Batch Acc: 0.006844, Tokens per Sec:     3469, Lr: 0.000151\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.26ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11684.14ex/s]\n",
      "2022-06-04 21:06:45,309 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=7000\n",
      "2022-06-04 21:06:45,310 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:06:54,551 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:06:54,552 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.88, loss:   2.20, ppl:   9.03, acc:   0.58, generation: 9.2057[sec], evaluation: 0.0277[sec]\n",
      "2022-06-04 21:06:55,521 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - \tSource:     Ich wollte nicht, dass das passiert.\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - \tReference:  I didn't want this to happen.\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - \tHypothesis: I didn't want this happened.\n",
      "2022-06-04 21:06:55,525 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:06:55,527 - INFO - joeynmt.training - \tSource:     Eins, zwei, drei, vier, fünf, sechs, sieben, acht, neun, zehn.\n",
      "2022-06-04 21:06:55,527 - INFO - joeynmt.training - \tReference:  One, two, three, four, five, six, seven, eight, nine, ten.\n",
      "2022-06-04 21:06:55,527 - INFO - joeynmt.training - \tHypothesis: A one, two, three, four, five, five, ten, seven, seven, seven, ten, seven, ten.\n",
      "2022-06-04 21:06:55,528 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:06:55,529 - INFO - joeynmt.training - \tSource:     Sprichst du Italienisch?\n",
      "2022-06-04 21:06:55,530 - INFO - joeynmt.training - \tReference:  Do you speak Italian?\n",
      "2022-06-04 21:06:55,530 - INFO - joeynmt.training - \tHypothesis: Do you speak Italian?\n",
      "2022-06-04 21:06:55,530 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:06:55,532 - INFO - joeynmt.training - \tSource:     Es kommt darauf an, was du mit an Gott \"glauben\" meinst.\n",
      "2022-06-04 21:06:55,532 - INFO - joeynmt.training - \tReference:  It depends what you mean by \"believe\" in God.\n",
      "2022-06-04 21:06:55,532 - INFO - joeynmt.training - \tHypothesis: It's about what you think with God \"No, love.\"\n",
      "2022-06-04 21:07:24,461 - INFO - joeynmt.training - Epoch   1, Step:     7100, Batch Loss:     1.723976, Batch Acc: 0.007435, Tokens per Sec:     3064, Lr: 0.000150\n",
      "2022-06-04 21:07:51,293 - INFO - joeynmt.training - Epoch   1, Step:     7200, Batch Loss:     1.868087, Batch Acc: 0.006683, Tokens per Sec:     3513, Lr: 0.000149\n",
      "2022-06-04 21:08:17,983 - INFO - joeynmt.training - Epoch   1, Step:     7300, Batch Loss:     1.663398, Batch Acc: 0.007155, Tokens per Sec:     3597, Lr: 0.000148\n",
      "2022-06-04 21:08:43,027 - INFO - joeynmt.training - Epoch   1: total training loss 6240.50\n",
      "2022-06-04 21:08:43,028 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-06-04 21:08:44,938 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.552310, Batch Acc: 0.100600, Tokens per Sec:     3318, Lr: 0.000147\n",
      "2022-06-04 21:09:11,684 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.611410, Batch Acc: 0.006134, Tokens per Sec:     3523, Lr: 0.000146\n",
      "2022-06-04 21:09:38,289 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.513177, Batch Acc: 0.007730, Tokens per Sec:     3516, Lr: 0.000145\n",
      "2022-06-04 21:10:05,891 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.476209, Batch Acc: 0.007273, Tokens per Sec:     3452, Lr: 0.000144\n",
      "2022-06-04 21:10:32,555 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.450078, Batch Acc: 0.007061, Tokens per Sec:     3516, Lr: 0.000143\n",
      "2022-06-04 21:10:58,834 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.430762, Batch Acc: 0.007802, Tokens per Sec:     3570, Lr: 0.000142\n",
      "2022-06-04 21:11:25,708 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.589353, Batch Acc: 0.006988, Tokens per Sec:     3552, Lr: 0.000141\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.47ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 10993.26ex/s]\n",
      "2022-06-04 21:11:26,550 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=8000\n",
      "2022-06-04 21:11:26,550 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:11:29,869 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:11:29,869 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  25.86, loss:   1.98, ppl:   7.23, acc:   0.62, generation: 3.2891[sec], evaluation: 0.0246[sec]\n",
      "2022-06-04 21:11:29,870 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:11:30,802 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:11:30,807 - INFO - joeynmt.training - \tSource:     Warum fragst du?\n",
      "2022-06-04 21:11:30,807 - INFO - joeynmt.training - \tReference:  Why are you asking?\n",
      "2022-06-04 21:11:30,808 - INFO - joeynmt.training - \tHypothesis: Why do you ask?\n",
      "2022-06-04 21:11:30,808 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:11:30,810 - INFO - joeynmt.training - \tSource:     Meine Mutter spricht nicht sehr gut Englisch.\n",
      "2022-06-04 21:11:30,811 - INFO - joeynmt.training - \tReference:  My mom doesn't speak English very well.\n",
      "2022-06-04 21:11:30,811 - INFO - joeynmt.training - \tHypothesis: My mother doesn't speak English very well.\n",
      "2022-06-04 21:11:30,811 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - \tSource:     Heute Abend gehen wir in die Kirche.\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - \tReference:  Tonight we're going to church.\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - \tHypothesis: We'll go to church tonight.\n",
      "2022-06-04 21:11:30,813 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:11:30,815 - INFO - joeynmt.training - \tSource:     Das ist das Dümmste, was ich je gesagt habe.\n",
      "2022-06-04 21:11:30,816 - INFO - joeynmt.training - \tReference:  That's the stupidest thing I've ever said.\n",
      "2022-06-04 21:11:30,816 - INFO - joeynmt.training - \tHypothesis: This is the stupid thing I've ever said.\n",
      "2022-06-04 21:11:58,588 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.422772, Batch Acc: 0.007385, Tokens per Sec:     3156, Lr: 0.000141\n",
      "2022-06-04 21:12:26,094 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.628181, Batch Acc: 0.006469, Tokens per Sec:     3462, Lr: 0.000140\n",
      "2022-06-04 21:12:52,885 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.649422, Batch Acc: 0.006705, Tokens per Sec:     3524, Lr: 0.000139\n",
      "2022-06-04 21:13:19,722 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.354998, Batch Acc: 0.007463, Tokens per Sec:     3515, Lr: 0.000138\n",
      "2022-06-04 21:13:45,860 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.442650, Batch Acc: 0.007173, Tokens per Sec:     3531, Lr: 0.000137\n",
      "2022-06-04 21:14:12,681 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.333545, Batch Acc: 0.008145, Tokens per Sec:     3498, Lr: 0.000136\n",
      "2022-06-04 21:14:39,445 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.602624, Batch Acc: 0.006743, Tokens per Sec:     3557, Lr: 0.000136\n",
      "2022-06-04 21:15:06,897 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.814612, Batch Acc: 0.006291, Tokens per Sec:     3468, Lr: 0.000135\n",
      "2022-06-04 21:15:33,510 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     1.564209, Batch Acc: 0.006815, Tokens per Sec:     3589, Lr: 0.000134\n",
      "2022-06-04 21:16:00,475 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     1.405532, Batch Acc: 0.007405, Tokens per Sec:     3476, Lr: 0.000133\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.44ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11394.87ex/s]\n",
      "2022-06-04 21:16:01,301 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=9000\n",
      "2022-06-04 21:16:01,302 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:16:05,609 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:16:05,609 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  25.79, loss:   1.94, ppl:   6.94, acc:   0.62, generation: 4.2798[sec], evaluation: 0.0224[sec]\n",
      "2022-06-04 21:16:06,728 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:16:06,731 - INFO - joeynmt.training - \tSource:     Bildung und Bewegung von Hurrikanen sind unberechenbar, sogar mit unserer modernen Technologie.\n",
      "2022-06-04 21:16:06,731 - INFO - joeynmt.training - \tReference:  The formation and movement of hurricanes are capricious, even with our present-day technology.\n",
      "2022-06-04 21:16:06,732 - INFO - joeynmt.training - \tHypothesis: Education and nuclearssss are nuclearsss, even 'scause of our solar system.\n",
      "2022-06-04 21:16:06,732 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - \tSource:     Wenn ich keine Antwort gegeben hätte, hätte ich nicht gesprochen.\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - \tReference:  If I gave no answer, I would not have spoken.\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - \tHypothesis: If I had no answer, I wouldn't have spoken.\n",
      "2022-06-04 21:16:06,734 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:16:06,736 - INFO - joeynmt.training - \tSource:     Niemand versteht mich.\n",
      "2022-06-04 21:16:06,736 - INFO - joeynmt.training - \tReference:  No one understands me.\n",
      "2022-06-04 21:16:06,736 - INFO - joeynmt.training - \tHypothesis: No one understands me.\n",
      "2022-06-04 21:16:06,737 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:16:06,738 - INFO - joeynmt.training - \tSource:     Wo ist das Problem?\n",
      "2022-06-04 21:16:06,739 - INFO - joeynmt.training - \tReference:  What is the problem?\n",
      "2022-06-04 21:16:06,739 - INFO - joeynmt.training - \tHypothesis: Where is the problem?\n",
      "2022-06-04 21:16:34,583 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     1.166789, Batch Acc: 0.008751, Tokens per Sec:     3141, Lr: 0.000133\n",
      "2022-06-04 21:17:02,063 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     1.397016, Batch Acc: 0.007049, Tokens per Sec:     3459, Lr: 0.000132\n",
      "2022-06-04 21:17:29,841 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     1.600673, Batch Acc: 0.006411, Tokens per Sec:     3363, Lr: 0.000131\n",
      "2022-06-04 21:17:56,748 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     1.418394, Batch Acc: 0.007788, Tokens per Sec:     3503, Lr: 0.000130\n",
      "2022-06-04 21:18:23,946 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     1.389611, Batch Acc: 0.007491, Tokens per Sec:     3485, Lr: 0.000130\n",
      "2022-06-04 21:18:51,133 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.234426, Batch Acc: 0.005569, Tokens per Sec:     3521, Lr: 0.000129\n",
      "2022-06-04 21:19:20,243 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     1.382914, Batch Acc: 0.007523, Tokens per Sec:     3220, Lr: 0.000128\n",
      "2022-06-04 21:19:47,649 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     1.126925, Batch Acc: 0.008240, Tokens per Sec:     3458, Lr: 0.000128\n",
      "2022-06-04 21:20:15,604 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     1.359352, Batch Acc: 0.007019, Tokens per Sec:     3420, Lr: 0.000127\n",
      "2022-06-04 21:20:42,570 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     1.710780, Batch Acc: 0.006730, Tokens per Sec:     3527, Lr: 0.000126\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 71.74ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 9970.84ex/s]\n",
      "2022-06-04 21:20:43,410 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=10000\n",
      "2022-06-04 21:20:43,410 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:20:47,314 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:20:47,314 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  30.19, loss:   1.85, ppl:   6.38, acc:   0.64, generation: 3.8779[sec], evaluation: 0.0206[sec]\n",
      "2022-06-04 21:20:47,315 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:20:48,657 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/5000.ckpt\n",
      "2022-06-04 21:20:48,687 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:20:48,690 - INFO - joeynmt.training - \tSource:     Beeil dich!\n",
      "2022-06-04 21:20:48,690 - INFO - joeynmt.training - \tReference:  Hurry it up!\n",
      "2022-06-04 21:20:48,690 - INFO - joeynmt.training - \tHypothesis: Hurry up!\n",
      "2022-06-04 21:20:48,691 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - \tSource:     Es gibt Leute auf der Welt, die so hungrig sind, dass Gott ihnen nicht erscheinen kann, außer in Form von Brot.\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - \tReference:  There are people in the world so hungry, that God cannot appear to them except in the form of bread.\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - \tHypothesis: There are people in the world that are so hungry that God can't seem to change the meaning of bread.\n",
      "2022-06-04 21:20:48,693 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:20:48,695 - INFO - joeynmt.training - \tSource:     Computer machen die Leute dumm.\n",
      "2022-06-04 21:20:48,695 - INFO - joeynmt.training - \tReference:  Computers make people stupid.\n",
      "2022-06-04 21:20:48,696 - INFO - joeynmt.training - \tHypothesis: Your computer are stupid.\n",
      "2022-06-04 21:20:48,696 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:20:48,700 - INFO - joeynmt.training - \tSource:     Das hat mich überrascht, ich wusste nicht, was ich tun sollte.\n",
      "2022-06-04 21:20:48,701 - INFO - joeynmt.training - \tReference:  It caught me off guard; I didn't know what to do.\n",
      "2022-06-04 21:20:48,702 - INFO - joeynmt.training - \tHypothesis: I was surprised, I didn't know what to do.\n",
      "2022-06-04 21:21:16,499 - INFO - joeynmt.training - Epoch   2, Step:    10100, Batch Loss:     1.298121, Batch Acc: 0.008145, Tokens per Sec:     3132, Lr: 0.000126\n",
      "2022-06-04 21:21:42,946 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     1.239453, Batch Acc: 0.008386, Tokens per Sec:     3544, Lr: 0.000125\n",
      "2022-06-04 21:22:09,555 - INFO - joeynmt.training - Epoch   2, Step:    10300, Batch Loss:     1.354857, Batch Acc: 0.007747, Tokens per Sec:     3580, Lr: 0.000125\n",
      "2022-06-04 21:22:36,959 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     1.315276, Batch Acc: 0.007599, Tokens per Sec:     3453, Lr: 0.000124\n",
      "2022-06-04 21:23:03,797 - INFO - joeynmt.training - Epoch   2, Step:    10500, Batch Loss:     1.179908, Batch Acc: 0.008729, Tokens per Sec:     3526, Lr: 0.000123\n",
      "2022-06-04 21:23:30,307 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     1.535451, Batch Acc: 0.007907, Tokens per Sec:     3568, Lr: 0.000123\n",
      "2022-06-04 21:23:38,360 - INFO - joeynmt.training - Epoch   2: total training loss 4744.08\n",
      "2022-06-04 21:23:38,360 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-06-04 21:23:57,543 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.084358, Batch Acc: 0.012840, Tokens per Sec:     3504, Lr: 0.000122\n",
      "2022-06-04 21:24:23,957 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.370948, Batch Acc: 0.008079, Tokens per Sec:     3575, Lr: 0.000122\n",
      "2022-06-04 21:24:50,675 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.194668, Batch Acc: 0.008189, Tokens per Sec:     3561, Lr: 0.000121\n",
      "2022-06-04 21:25:17,990 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.434793, Batch Acc: 0.006377, Tokens per Sec:     3439, Lr: 0.000121\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 66.28ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 10930.41ex/s]\n",
      "2022-06-04 21:25:18,856 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=11000\n",
      "2022-06-04 21:25:18,857 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:25:22,608 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:25:22,609 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  29.51, loss:   1.69, ppl:   5.44, acc:   0.66, generation: 3.7267[sec], evaluation: 0.0202[sec]\n",
      "2022-06-04 21:25:23,526 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/7000.ckpt\n",
      "2022-06-04 21:25:23,556 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - \tSource:     Ein Stundenplan ist ein Ausweis für die Zeit, nur, wenn man keinen Stundenplan hat, ist die Zeit nicht da.\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - \tReference:  A schedule is an identity card for time, but, if you don't have a schedule, the time isn't there.\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - \tHypothesis: A hours plan is a smile for time, just if you have no time.\n",
      "2022-06-04 21:25:23,559 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:25:23,562 - INFO - joeynmt.training - \tSource:     Es ist schwierig, ein Gespräch mit jemandem zu führen, der nur \"Ja\" und \"Nein\" sagt.\n",
      "2022-06-04 21:25:23,563 - INFO - joeynmt.training - \tReference:  It is difficult to keep up a conversation with someone who only says \"yes\" and \"no\".\n",
      "2022-06-04 21:25:23,563 - INFO - joeynmt.training - \tHypothesis: It's difficult to use a conversation with someone who only means \"No.\"\n",
      "2022-06-04 21:25:23,564 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:25:23,565 - INFO - joeynmt.training - \tSource:     Die meisten Leute denken, ich sei verrückt.\n",
      "2022-06-04 21:25:23,566 - INFO - joeynmt.training - \tReference:  Most people think I'm crazy.\n",
      "2022-06-04 21:25:23,566 - INFO - joeynmt.training - \tHypothesis: Most people think I'm crazy.\n",
      "2022-06-04 21:25:23,566 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:25:23,568 - INFO - joeynmt.training - \tSource:     Bist du sicher?\n",
      "2022-06-04 21:25:23,568 - INFO - joeynmt.training - \tReference:  Are you sure?\n",
      "2022-06-04 21:25:23,568 - INFO - joeynmt.training - \tHypothesis: Are you sure?\n",
      "2022-06-04 21:25:50,694 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.404580, Batch Acc: 0.005673, Tokens per Sec:     3263, Lr: 0.000120\n",
      "2022-06-04 21:26:17,158 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.328782, Batch Acc: 0.007381, Tokens per Sec:     3584, Lr: 0.000120\n",
      "2022-06-04 21:26:43,788 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.225472, Batch Acc: 0.007584, Tokens per Sec:     3570, Lr: 0.000119\n",
      "2022-06-04 21:27:10,265 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.364240, Batch Acc: 0.006407, Tokens per Sec:     3543, Lr: 0.000118\n",
      "2022-06-04 21:27:37,872 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.045176, Batch Acc: 0.008393, Tokens per Sec:     3388, Lr: 0.000118\n",
      "2022-06-04 21:28:04,561 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.096500, Batch Acc: 0.008267, Tokens per Sec:     3544, Lr: 0.000117\n",
      "2022-06-04 21:28:31,270 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.408800, Batch Acc: 0.005898, Tokens per Sec:     3548, Lr: 0.000117\n",
      "2022-06-04 21:28:57,776 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.222268, Batch Acc: 0.006867, Tokens per Sec:     3599, Lr: 0.000116\n",
      "2022-06-04 21:29:24,045 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.418336, Batch Acc: 0.006734, Tokens per Sec:     3578, Lr: 0.000116\n",
      "2022-06-04 21:29:50,483 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.180122, Batch Acc: 0.007419, Tokens per Sec:     3584, Lr: 0.000115\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 73.06ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11010.55ex/s]\n",
      "2022-06-04 21:29:51,305 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=12000\n",
      "2022-06-04 21:29:51,305 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:29:56,301 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:29:56,301 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  31.12, loss:   1.76, ppl:   5.79, acc:   0.64, generation: 4.9687[sec], evaluation: 0.0222[sec]\n",
      "2022-06-04 21:29:56,302 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:29:57,597 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/6000.ckpt\n",
      "2022-06-04 21:29:57,631 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:29:57,634 - INFO - joeynmt.training - \tSource:     Aaah!! Mein Computer ist kaputt!\n",
      "2022-06-04 21:29:57,634 - INFO - joeynmt.training - \tReference:  Aaah!! My computer is broken!\n",
      "2022-06-04 21:29:57,634 - INFO - joeynmt.training - \tHypothesis: Ea! My computer is broken!\n",
      "2022-06-04 21:29:57,635 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:29:57,637 - INFO - joeynmt.training - \tSource:     Wenn ich Leute frage, was sie in Bezug aufs Gymnasium am meisten bedauern, sagen fast alle das Gleiche: dass sie so viel Zeit verschwendet haben.\n",
      "2022-06-04 21:29:57,637 - INFO - joeynmt.training - \tReference:  When I ask people what they regret most about high school, they nearly all say the same thing: that they wasted so much time.\n",
      "2022-06-04 21:29:57,637 - INFO - joeynmt.training - \tHypothesis: If I ask people what they think about high school, all the same high school, they have almost the same time.\n",
      "2022-06-04 21:29:57,638 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - \tSource:     Sei bitte geduldig, das braucht Zeit.\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - \tReference:  Be patient please. It takes time.\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - \tHypothesis: Please be patient to need time.\n",
      "2022-06-04 21:29:57,640 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:29:57,643 - INFO - joeynmt.training - \tSource:     Nein, er ist nicht mein neuer Freund.\n",
      "2022-06-04 21:29:57,643 - INFO - joeynmt.training - \tReference:  No, he's not my new boyfriend.\n",
      "2022-06-04 21:29:57,643 - INFO - joeynmt.training - \tHypothesis: No, he is not my new friend.\n",
      "2022-06-04 21:30:25,995 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.161913, Batch Acc: 0.007499, Tokens per Sec:     3085, Lr: 0.000115\n",
      "2022-06-04 21:30:52,611 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.145465, Batch Acc: 0.007659, Tokens per Sec:     3542, Lr: 0.000115\n",
      "2022-06-04 21:31:18,851 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.283632, Batch Acc: 0.006720, Tokens per Sec:     3596, Lr: 0.000114\n",
      "2022-06-04 21:31:45,363 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.312924, Batch Acc: 0.008340, Tokens per Sec:     3578, Lr: 0.000114\n",
      "2022-06-04 21:32:12,168 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.236772, Batch Acc: 0.005806, Tokens per Sec:     3534, Lr: 0.000113\n",
      "2022-06-04 21:32:38,574 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.007404, Batch Acc: 0.007911, Tokens per Sec:     3609, Lr: 0.000113\n",
      "2022-06-04 21:33:05,823 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.618528, Batch Acc: 0.006191, Tokens per Sec:     3474, Lr: 0.000112\n",
      "2022-06-04 21:33:32,476 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.256278, Batch Acc: 0.007669, Tokens per Sec:     3537, Lr: 0.000112\n",
      "2022-06-04 21:33:58,801 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.148804, Batch Acc: 0.007714, Tokens per Sec:     3575, Lr: 0.000111\n",
      "2022-06-04 21:34:25,106 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.247037, Batch Acc: 0.007222, Tokens per Sec:     3580, Lr: 0.000111\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 68.84ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11681.70ex/s]\n",
      "2022-06-04 21:34:25,974 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=13000\n",
      "2022-06-04 21:34:25,974 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:34:30,373 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:34:30,374 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  32.29, loss:   1.77, ppl:   5.90, acc:   0.65, generation: 4.3738[sec], evaluation: 0.0211[sec]\n",
      "2022-06-04 21:34:30,374 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:34:31,338 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/9000.ckpt\n",
      "2022-06-04 21:34:31,371 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:34:31,374 - INFO - joeynmt.training - \tSource:     In einem Wörterbuch wie diesem sollte es mindestens zwei Sätze mit \"Kühlschrank\" geben.\n",
      "2022-06-04 21:34:31,374 - INFO - joeynmt.training - \tReference:  In a dictionary like this one there should be at least two sentences with \"fridge\".\n",
      "2022-06-04 21:34:31,375 - INFO - joeynmt.training - \tHypothesis: In a dictionary as to give it two sentences with \"I'm.\"\n",
      "2022-06-04 21:34:31,375 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:34:31,377 - INFO - joeynmt.training - \tSource:     Ich werde ihn erschießen.\n",
      "2022-06-04 21:34:31,377 - INFO - joeynmt.training - \tReference:  I'm going to shoot him dead.\n",
      "2022-06-04 21:34:31,377 - INFO - joeynmt.training - \tHypothesis: I'll shoot him.\n",
      "2022-06-04 21:34:31,378 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - \tSource:     Ich kann in 10 Minuten zur Schule gehen.\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - \tReference:  I can walk to school in 10 minutes.\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - \tHypothesis: I can go to school ten minutes.\n",
      "2022-06-04 21:34:31,380 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:34:31,382 - INFO - joeynmt.training - \tSource:     Die Mathematik ist der Teil der Wissenschaft, den man weiter betreiben könnte, wenn man morgen aufwachen würde und entdecken würde, dass das Universum weg ist.\n",
      "2022-06-04 21:34:31,383 - INFO - joeynmt.training - \tReference:  Mathematics is the part of science you could continue to do if you woke up tomorrow and discovered the universe was gone.\n",
      "2022-06-04 21:34:31,383 - INFO - joeynmt.training - \tHypothesis: Mathematics is the part of science that could run when you wake up tomorrow and the universe is away.\n",
      "2022-06-04 21:34:59,022 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.365483, Batch Acc: 0.007153, Tokens per Sec:     3188, Lr: 0.000111\n",
      "2022-06-04 21:35:26,319 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     1.192539, Batch Acc: 0.007747, Tokens per Sec:     3471, Lr: 0.000110\n",
      "2022-06-04 21:35:53,282 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     1.222802, Batch Acc: 0.006808, Tokens per Sec:     3519, Lr: 0.000110\n",
      "2022-06-04 21:36:19,755 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     1.323586, Batch Acc: 0.007356, Tokens per Sec:     3564, Lr: 0.000109\n",
      "2022-06-04 21:36:46,746 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     1.118663, Batch Acc: 0.007235, Tokens per Sec:     3457, Lr: 0.000109\n",
      "2022-06-04 21:37:13,892 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     1.004997, Batch Acc: 0.008137, Tokens per Sec:     3490, Lr: 0.000108\n",
      "2022-06-04 21:37:40,792 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     1.188175, Batch Acc: 0.007845, Tokens per Sec:     3521, Lr: 0.000108\n",
      "2022-06-04 21:38:08,226 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     1.062657, Batch Acc: 0.007259, Tokens per Sec:     3480, Lr: 0.000108\n",
      "2022-06-04 21:38:25,120 - INFO - joeynmt.training - Epoch   3: total training loss 4061.75\n",
      "2022-06-04 21:38:25,121 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-06-04 21:38:34,945 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.028237, Batch Acc: 0.021588, Tokens per Sec:     3537, Lr: 0.000107\n",
      "2022-06-04 21:39:01,761 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.041151, Batch Acc: 0.008362, Tokens per Sec:     3550, Lr: 0.000107\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 70.98ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11332.86ex/s]\n",
      "2022-06-04 21:39:02,652 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=14000\n",
      "2022-06-04 21:39:02,652 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:39:06,533 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:39:06,533 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.23, loss:   1.67, ppl:   5.31, acc:   0.67, generation: 3.8549[sec], evaluation: 0.0207[sec]\n",
      "2022-06-04 21:39:06,534 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:39:07,417 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/8000.ckpt\n",
      "2022-06-04 21:39:07,449 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - \tSource:     Ich habe einen Traum.\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - \tReference:  I have a dream.\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - \tHypothesis: I have a dream.\n",
      "2022-06-04 21:39:07,456 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:39:07,462 - INFO - joeynmt.training - \tSource:     Was hast du heute zum Mittagessen gegessen?\n",
      "2022-06-04 21:39:07,462 - INFO - joeynmt.training - \tReference:  What did you have for lunch today?\n",
      "2022-06-04 21:39:07,463 - INFO - joeynmt.training - \tHypothesis: What have you eaten for lunch today?\n",
      "2022-06-04 21:39:07,463 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:39:07,465 - INFO - joeynmt.training - \tSource:     Ich hasse Chemie.\n",
      "2022-06-04 21:39:07,465 - INFO - joeynmt.training - \tReference:  I hate chemistry.\n",
      "2022-06-04 21:39:07,465 - INFO - joeynmt.training - \tHypothesis: I hate chemistry.\n",
      "2022-06-04 21:39:07,466 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:39:07,468 - INFO - joeynmt.training - \tSource:     Wenn ich dich erschrecken wollte, würde ich dir erzählen, was ich vor ein paar Wochen geträumt habe.\n",
      "2022-06-04 21:39:07,468 - INFO - joeynmt.training - \tReference:  If I wanted to scare you, I would tell you what I dreamt about a few weeks ago.\n",
      "2022-06-04 21:39:07,468 - INFO - joeynmt.training - \tHypothesis: If I wanted to scare you, I would tell you what I dreamed of a few weeks ago.\n",
      "2022-06-04 21:39:34,733 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.139699, Batch Acc: 0.007947, Tokens per Sec:     3240, Lr: 0.000107\n",
      "2022-06-04 21:40:02,574 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     0.979903, Batch Acc: 0.007910, Tokens per Sec:     3456, Lr: 0.000106\n",
      "2022-06-04 21:40:29,001 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.033339, Batch Acc: 0.008048, Tokens per Sec:     3578, Lr: 0.000106\n",
      "2022-06-04 21:40:55,492 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.169329, Batch Acc: 0.008417, Tokens per Sec:     3516, Lr: 0.000105\n",
      "2022-06-04 21:41:22,354 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.122081, Batch Acc: 0.007491, Tokens per Sec:     3558, Lr: 0.000105\n",
      "2022-06-04 21:41:48,766 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.096020, Batch Acc: 0.008211, Tokens per Sec:     3550, Lr: 0.000105\n",
      "2022-06-04 21:42:15,539 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     0.951469, Batch Acc: 0.007975, Tokens per Sec:     3513, Lr: 0.000104\n",
      "2022-06-04 21:42:42,163 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.252295, Batch Acc: 0.007638, Tokens per Sec:     3570, Lr: 0.000104\n",
      "2022-06-04 21:43:09,163 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.101365, Batch Acc: 0.006967, Tokens per Sec:     3450, Lr: 0.000104\n",
      "2022-06-04 21:43:35,587 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.206033, Batch Acc: 0.006756, Tokens per Sec:     3568, Lr: 0.000103\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 69.57ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11510.35ex/s]\n",
      "2022-06-04 21:43:36,456 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=15000\n",
      "2022-06-04 21:43:36,456 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:43:40,367 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:43:40,367 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.08, loss:   1.71, ppl:   5.55, acc:   0.67, generation: 3.8849[sec], evaluation: 0.0214[sec]\n",
      "2022-06-04 21:43:41,250 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/11000.ckpt\n",
      "2022-06-04 21:43:41,280 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:43:41,283 - INFO - joeynmt.training - \tSource:     Die Verpflichtung zum Schulbesuch wird selten analysiert in der Vielzahl der Werke, die den mannigfaltigen Arten gewidmet sind, bei Kindern den Wunsch zu lernen zu entwickeln.\n",
      "2022-06-04 21:43:41,284 - INFO - joeynmt.training - \tReference:  The mandatory character of schooling is rarely analyzed in the multitude of works dedicated to the study of the various ways to develop within children the desire to learn.\n",
      "2022-06-04 21:43:41,284 - INFO - joeynmt.training - \tHypothesis: The service for school is rarely to spend a lot of work in the works, the works of the way to learn the children's wish to travel in the wish.\n",
      "2022-06-04 21:43:41,284 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:43:41,286 - INFO - joeynmt.training - \tSource:     Ich bin fast fertig.\n",
      "2022-06-04 21:43:41,286 - INFO - joeynmt.training - \tReference:  I'm about ready.\n",
      "2022-06-04 21:43:41,287 - INFO - joeynmt.training - \tHypothesis: I'm almost finished.\n",
      "2022-06-04 21:43:41,287 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - \tSource:     „Ich habe Lust, Karten zu spielen.“ – „Ich auch.“\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - \tReference:  \"I feel like playing cards.\" \"So do I.\"\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - \tHypothesis: \"I feel like playing cards.\" \"I'm too.\"\n",
      "2022-06-04 21:43:41,289 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:43:41,292 - INFO - joeynmt.training - \tSource:     „Das sieht ziemlich interessant aus“, sagt Hiroshi.\n",
      "2022-06-04 21:43:41,292 - INFO - joeynmt.training - \tReference:  \"This looks pretty interesting,\" Hiroshi says.\n",
      "2022-06-04 21:43:41,292 - INFO - joeynmt.training - \tHypothesis: \"That looks pretty interesting,\" says Mr. Hio.\n",
      "2022-06-04 21:44:09,192 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.106495, Batch Acc: 0.008080, Tokens per Sec:     3235, Lr: 0.000103\n",
      "2022-06-04 21:44:35,364 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.067895, Batch Acc: 0.007854, Tokens per Sec:     3561, Lr: 0.000103\n",
      "2022-06-04 21:45:02,084 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.436536, Batch Acc: 0.007691, Tokens per Sec:     3528, Lr: 0.000102\n",
      "2022-06-04 21:45:29,823 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.151152, Batch Acc: 0.006763, Tokens per Sec:     3460, Lr: 0.000102\n",
      "2022-06-04 21:45:56,283 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.339136, Batch Acc: 0.007155, Tokens per Sec:     3544, Lr: 0.000102\n",
      "2022-06-04 21:46:22,992 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.037250, Batch Acc: 0.006793, Tokens per Sec:     3561, Lr: 0.000101\n",
      "2022-06-04 21:46:49,333 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     0.949777, Batch Acc: 0.008185, Tokens per Sec:     3576, Lr: 0.000101\n",
      "2022-06-04 21:47:15,871 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.139402, Batch Acc: 0.006658, Tokens per Sec:     3566, Lr: 0.000101\n",
      "2022-06-04 21:47:43,543 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     0.912606, Batch Acc: 0.010270, Tokens per Sec:     3403, Lr: 0.000100\n",
      "2022-06-04 21:48:10,068 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     0.975524, Batch Acc: 0.008769, Tokens per Sec:     3551, Lr: 0.000100\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 75.92ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11277.28ex/s]\n",
      "2022-06-04 21:48:10,885 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=16000\n",
      "2022-06-04 21:48:10,885 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:48:14,337 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:48:14,338 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.15, loss:   1.56, ppl:   4.77, acc:   0.69, generation: 3.4234[sec], evaluation: 0.0234[sec]\n",
      "2022-06-04 21:48:15,247 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/10000.ckpt\n",
      "2022-06-04 21:48:15,277 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:48:15,280 - INFO - joeynmt.training - \tSource:     Das ist ein totales Schlamassel und es geht mir auf die Nerven.\n",
      "2022-06-04 21:48:15,281 - INFO - joeynmt.training - \tReference:  It's a complete mess, and it's getting on my nerves.\n",
      "2022-06-04 21:48:15,281 - INFO - joeynmt.training - \tHypothesis: This is a complete mess and it's on my nerves.\n",
      "2022-06-04 21:48:15,281 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:48:15,283 - INFO - joeynmt.training - \tSource:     Ich bin am Leben, obwohl ich kein Lebenszeichen gebe.\n",
      "2022-06-04 21:48:15,283 - INFO - joeynmt.training - \tReference:  I am alive even though I am not giving any sign of life.\n",
      "2022-06-04 21:48:15,283 - INFO - joeynmt.training - \tHypothesis: I'm alive even though I don't give a sign of life.\n",
      "2022-06-04 21:48:15,284 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - \tSource:     Gibt es hier in der Nähe eine Jugendherberge?\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - \tReference:  Is there a youth hostel near here?\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - \tHypothesis: Is there a hot youth here?\n",
      "2022-06-04 21:48:15,286 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:48:15,288 - INFO - joeynmt.training - \tSource:     Heute Abend gehen wir in die Kirche.\n",
      "2022-06-04 21:48:15,288 - INFO - joeynmt.training - \tReference:  Tonight we're going to church.\n",
      "2022-06-04 21:48:15,288 - INFO - joeynmt.training - \tHypothesis: We'll go to church tonight.\n",
      "2022-06-04 21:48:42,798 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     0.932757, Batch Acc: 0.008238, Tokens per Sec:     3263, Lr: 0.000100\n",
      "2022-06-04 21:49:09,867 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     0.978481, Batch Acc: 0.008462, Tokens per Sec:     3506, Lr: 0.000099\n",
      "2022-06-04 21:49:36,275 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     0.996044, Batch Acc: 0.008364, Tokens per Sec:     3541, Lr: 0.000099\n",
      "2022-06-04 21:50:02,749 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.016374, Batch Acc: 0.007528, Tokens per Sec:     3558, Lr: 0.000099\n",
      "2022-06-04 21:50:30,221 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.090051, Batch Acc: 0.007588, Tokens per Sec:     3440, Lr: 0.000098\n",
      "2022-06-04 21:50:56,914 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.061105, Batch Acc: 0.007805, Tokens per Sec:     3590, Lr: 0.000098\n",
      "2022-06-04 21:51:23,333 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.161416, Batch Acc: 0.008373, Tokens per Sec:     3558, Lr: 0.000098\n",
      "2022-06-04 21:51:49,870 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.062989, Batch Acc: 0.007999, Tokens per Sec:     3548, Lr: 0.000098\n",
      "2022-06-04 21:52:16,378 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     0.946536, Batch Acc: 0.009350, Tokens per Sec:     3535, Lr: 0.000097\n",
      "2022-06-04 21:52:43,637 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.032760, Batch Acc: 0.007729, Tokens per Sec:     3536, Lr: 0.000097\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 78.29ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11895.87ex/s]\n",
      "2022-06-04 21:52:44,467 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=17000\n",
      "2022-06-04 21:52:44,467 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:52:48,473 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:52:48,474 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.75, loss:   1.55, ppl:   4.73, acc:   0.68, generation: 3.9776[sec], evaluation: 0.0235[sec]\n",
      "2022-06-04 21:52:48,474 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 21:52:49,404 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/12000.ckpt\n",
      "2022-06-04 21:52:49,436 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:52:49,439 - INFO - joeynmt.training - \tSource:     Die Welt ist ein Irrenhaus.\n",
      "2022-06-04 21:52:49,439 - INFO - joeynmt.training - \tReference:  This world is just an insane asylum.\n",
      "2022-06-04 21:52:49,440 - INFO - joeynmt.training - \tHypothesis: The world is a crazy job.\n",
      "2022-06-04 21:52:49,440 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - \tSource:     Nicht öffnen, bevor der Zug hält.\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - \tReference:  Do not open before the train stops.\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - \tHypothesis: Not open before the train stops.\n",
      "2022-06-04 21:52:49,442 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:52:49,444 - INFO - joeynmt.training - \tSource:     Die meisten Leute wollen nur ihre eigene Wahrheit hören.\n",
      "2022-06-04 21:52:49,444 - INFO - joeynmt.training - \tReference:  Most people only want to hear their own truth.\n",
      "2022-06-04 21:52:49,444 - INFO - joeynmt.training - \tHypothesis: Most people want to hear their own truth.\n",
      "2022-06-04 21:52:49,445 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:52:49,447 - INFO - joeynmt.training - \tSource:     Wo ist die Toilette?\n",
      "2022-06-04 21:52:49,447 - INFO - joeynmt.training - \tReference:  Where's the toilet?\n",
      "2022-06-04 21:52:49,447 - INFO - joeynmt.training - \tHypothesis: Where's the toilet?\n",
      "2022-06-04 21:53:15,904 - INFO - joeynmt.training - Epoch   4: total training loss 3647.91\n",
      "2022-06-04 21:53:15,905 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-06-04 21:53:17,003 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     0.816354, Batch Acc: 0.214172, Tokens per Sec:     3436, Lr: 0.000097\n",
      "2022-06-04 21:53:43,546 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.113853, Batch Acc: 0.007868, Tokens per Sec:     3539, Lr: 0.000096\n",
      "2022-06-04 21:54:10,043 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     0.957400, Batch Acc: 0.008095, Tokens per Sec:     3599, Lr: 0.000096\n",
      "2022-06-04 21:54:36,425 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.196077, Batch Acc: 0.006420, Tokens per Sec:     3602, Lr: 0.000096\n",
      "2022-06-04 21:55:04,228 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.145908, Batch Acc: 0.007529, Tokens per Sec:     3373, Lr: 0.000096\n",
      "2022-06-04 21:55:30,579 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.281533, Batch Acc: 0.006726, Tokens per Sec:     3594, Lr: 0.000095\n",
      "2022-06-04 21:55:57,203 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.276523, Batch Acc: 0.006568, Tokens per Sec:     3557, Lr: 0.000095\n",
      "2022-06-04 21:56:23,764 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     0.826367, Batch Acc: 0.008737, Tokens per Sec:     3538, Lr: 0.000095\n",
      "2022-06-04 21:56:50,199 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.088839, Batch Acc: 0.008014, Tokens per Sec:     3621, Lr: 0.000095\n",
      "2022-06-04 21:57:17,197 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.006327, Batch Acc: 0.007726, Tokens per Sec:     3505, Lr: 0.000094\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 76.85ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11014.40ex/s]\n",
      "2022-06-04 21:57:18,034 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=18000\n",
      "2022-06-04 21:57:18,035 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 21:57:22,814 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 21:57:22,814 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.98, loss:   1.62, ppl:   5.08, acc:   0.68, generation: 4.7524[sec], evaluation: 0.0216[sec]\n",
      "2022-06-04 21:57:23,680 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/13000.ckpt\n",
      "2022-06-04 21:57:23,709 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - \tSource:     Die Ankläger bei Gericht müssen ihre Klagen erhärten, um einen Verdächtigen als schuldig zu beweisen.\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - \tReference:  Prosecutors in court have to substantiate their claims in order to prove a suspect is guilty.\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - \tHypothesis: The bed bedns must have to be guilty of her complaints to prove a suspect.\n",
      "2022-06-04 21:57:23,712 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 21:57:23,714 - INFO - joeynmt.training - \tSource:     Es ist schon 11 Uhr.\n",
      "2022-06-04 21:57:23,715 - INFO - joeynmt.training - \tReference:  It's already eleven.\n",
      "2022-06-04 21:57:23,715 - INFO - joeynmt.training - \tHypothesis: It's already eleven.\n",
      "2022-06-04 21:57:23,715 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 21:57:23,717 - INFO - joeynmt.training - \tSource:     Leider würden viele Leute Dinge glauben, die man ihnen per E-Mail sagt, die sie persönlich erzählt nicht plausibel finden würden.\n",
      "2022-06-04 21:57:23,717 - INFO - joeynmt.training - \tReference:  Sadly many people will believe things told to them via an email which they would find implausible face-to-face.\n",
      "2022-06-04 21:57:23,718 - INFO - joeynmt.training - \tHypothesis: Unfortunately, many people think they say they say with e-mails, they wouldn't be telling them if they weren't being being being told.\n",
      "2022-06-04 21:57:23,718 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 21:57:23,720 - INFO - joeynmt.training - \tSource:     Ich habe morgen Unterricht.\n",
      "2022-06-04 21:57:23,720 - INFO - joeynmt.training - \tReference:  I have class tomorrow.\n",
      "2022-06-04 21:57:23,720 - INFO - joeynmt.training - \tHypothesis: I have class tomorrow.\n",
      "2022-06-04 21:57:51,775 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     0.841608, Batch Acc: 0.008215, Tokens per Sec:     3199, Lr: 0.000094\n",
      "2022-06-04 21:58:18,950 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.207774, Batch Acc: 0.008036, Tokens per Sec:     3467, Lr: 0.000094\n",
      "2022-06-04 21:58:46,356 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     0.918700, Batch Acc: 0.008609, Tokens per Sec:     3416, Lr: 0.000094\n",
      "2022-06-04 21:59:13,787 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     0.998069, Batch Acc: 0.008126, Tokens per Sec:     3441, Lr: 0.000093\n",
      "2022-06-04 21:59:40,517 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     0.870410, Batch Acc: 0.009325, Tokens per Sec:     3511, Lr: 0.000093\n",
      "2022-06-04 22:00:09,255 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.061796, Batch Acc: 0.007891, Tokens per Sec:     3215, Lr: 0.000093\n",
      "2022-06-04 22:00:36,334 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.059263, Batch Acc: 0.007125, Tokens per Sec:     3504, Lr: 0.000092\n",
      "2022-06-04 22:01:03,200 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     0.860989, Batch Acc: 0.008765, Tokens per Sec:     3516, Lr: 0.000092\n",
      "2022-06-04 22:01:30,413 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.078698, Batch Acc: 0.008641, Tokens per Sec:     3479, Lr: 0.000092\n",
      "2022-06-04 22:01:58,224 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.009859, Batch Acc: 0.008402, Tokens per Sec:     3450, Lr: 0.000092\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 78.28ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 10899.31ex/s]\n",
      "2022-06-04 22:01:59,056 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=19000\n",
      "2022-06-04 22:01:59,056 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:02:02,343 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 22:02:02,344 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  36.58, loss:   1.50, ppl:   4.50, acc:   0.70, generation: 3.2613[sec], evaluation: 0.0210[sec]\n",
      "2022-06-04 22:02:02,344 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
      "2022-06-04 22:02:03,293 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/models/tatoeba_deen_resume/18000.ckpt\n",
      "2022-06-04 22:02:03,297 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 22:02:03,300 - INFO - joeynmt.training - \tSource:     Oh, meine weiße Hose! Sie war neu.\n",
      "2022-06-04 22:02:03,301 - INFO - joeynmt.training - \tReference:  Oh, my white pants! And they were new.\n",
      "2022-06-04 22:02:03,301 - INFO - joeynmt.training - \tHypothesis: Oh, my white pants! She was new.\n",
      "2022-06-04 22:02:03,301 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - \tSource:     Unterschätze nicht meine Macht!\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - \tReference:  Don't underestimate my power.\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - \tHypothesis: Don't underestimate my power.\n",
      "2022-06-04 22:02:03,303 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 22:02:03,305 - INFO - joeynmt.training - \tSource:     Der Langsamste beim Versprechen ist der Treueste beim Einhalten.\n",
      "2022-06-04 22:02:03,306 - INFO - joeynmt.training - \tReference:  The slowest one to make a promise is the most faithful one in keeping it.\n",
      "2022-06-04 22:02:03,306 - INFO - joeynmt.training - \tHypothesis: The first promise is the most faithful to stop.\n",
      "2022-06-04 22:02:03,306 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 22:02:03,308 - INFO - joeynmt.training - \tSource:     Vielen Dank!\n",
      "2022-06-04 22:02:03,308 - INFO - joeynmt.training - \tReference:  Many thanks.\n",
      "2022-06-04 22:02:03,308 - INFO - joeynmt.training - \tHypothesis: Thank you very much.\n",
      "2022-06-04 22:02:30,963 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     0.925297, Batch Acc: 0.007425, Tokens per Sec:     3174, Lr: 0.000092\n",
      "2022-06-04 22:02:57,404 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     0.941763, Batch Acc: 0.008718, Tokens per Sec:     3575, Lr: 0.000091\n",
      "2022-06-04 22:03:24,113 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     0.964910, Batch Acc: 0.009509, Tokens per Sec:     3555, Lr: 0.000091\n",
      "2022-06-04 22:03:51,031 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.039105, Batch Acc: 0.008619, Tokens per Sec:     3487, Lr: 0.000091\n",
      "2022-06-04 22:04:20,103 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.018815, Batch Acc: 0.008912, Tokens per Sec:     3269, Lr: 0.000091\n",
      "2022-06-04 22:04:47,334 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     0.896303, Batch Acc: 0.009908, Tokens per Sec:     3451, Lr: 0.000090\n",
      "2022-06-04 22:05:14,250 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     0.778399, Batch Acc: 0.007928, Tokens per Sec:     3571, Lr: 0.000090\n",
      "2022-06-04 22:05:40,536 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.042853, Batch Acc: 0.007230, Tokens per Sec:     3583, Lr: 0.000090\n",
      "2022-06-04 22:06:08,838 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.000512, Batch Acc: 0.007895, Tokens per Sec:     3294, Lr: 0.000090\n",
      "2022-06-04 22:06:36,707 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.039244, Batch Acc: 0.008091, Tokens per Sec:     3357, Lr: 0.000089\n",
      "Dropping NaN...: 100% 1/1 [00:00<00:00, 77.61ba/s]\n",
      "Preprocessing...: 100% 1000/1000 [00:00<00:00, 11546.79ex/s]\n",
      "2022-06-04 22:06:37,522 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=20000\n",
      "2022-06-04 22:06:37,523 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:06:41,183 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0\n",
      "2022-06-04 22:06:41,183 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.45, loss:   1.51, ppl:   4.51, acc:   0.68, generation: 3.6310[sec], evaluation: 0.0242[sec]\n",
      "2022-06-04 22:06:41,187 - INFO - joeynmt.training - Example #0\n",
      "2022-06-04 22:06:41,190 - INFO - joeynmt.training - \tSource:     \"Danke für die Hilfe.\" \"Keine Ursache.\"\n",
      "2022-06-04 22:06:41,191 - INFO - joeynmt.training - \tReference:  \"Thank you for helping me.\" \"Don't mention it.\"\n",
      "2022-06-04 22:06:41,191 - INFO - joeynmt.training - \tHypothesis: \"Thanks for the help.\" \"I don't care.\"\n",
      "2022-06-04 22:06:41,191 - INFO - joeynmt.training - Example #1\n",
      "2022-06-04 22:06:41,193 - INFO - joeynmt.training - \tSource:     Ich bin so dick.\n",
      "2022-06-04 22:06:41,194 - INFO - joeynmt.training - \tReference:  I'm so fat.\n",
      "2022-06-04 22:06:41,194 - INFO - joeynmt.training - \tHypothesis: I'm so fat.\n",
      "2022-06-04 22:06:41,197 - INFO - joeynmt.training - Example #2\n",
      "2022-06-04 22:06:41,199 - INFO - joeynmt.training - \tSource:     Wenn man nicht machen kann, was man will, macht man, was man kann.\n",
      "2022-06-04 22:06:41,199 - INFO - joeynmt.training - \tReference:  When you can't do what you want, you do what you can.\n",
      "2022-06-04 22:06:41,199 - INFO - joeynmt.training - \tHypothesis: If you can't do what you want to do, you can.\n",
      "2022-06-04 22:06:41,200 - INFO - joeynmt.training - Example #3\n",
      "2022-06-04 22:06:41,202 - INFO - joeynmt.training - \tSource:     Jimmy versuchte, seine Eltern dazu zu kriegen, ihn mit seinen Freunden quer durch das Land fahren zu lassen.\n",
      "2022-06-04 22:06:41,202 - INFO - joeynmt.training - \tReference:  Jimmy tried to cajole his parents into letting him drive across the country with his friends.\n",
      "2022-06-04 22:06:41,202 - INFO - joeynmt.training - \tHypothesis: Jimmy tried to get his parents to let him drive his friends through the country through the country.\n",
      "2022-06-04 22:07:07,957 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     0.979173, Batch Acc: 0.007844, Tokens per Sec:     3406, Lr: 0.000089\n",
      "2022-06-04 22:07:16,671 - INFO - joeynmt.training - Skipping test after training\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/resume_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaHtCsyJf1yD"
   },
   "source": [
    "> 💡 It starts counting the epochs from the beginning again, but step numbers should continue from before and you should find a \"reloading\" line in the training log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVv1ja0eCk66"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The `test` mode can be used to translate (and evaluate on) the test set specified in the configuration. We usually do this only once after we've tuned hyperparameters on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5T0OEp22BnX",
    "outputId": "327025da-0e8a-4b54-d602-019e1289c2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-03 22:57:04,258 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-03 22:57:04,258 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-03 22:57:04,343 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-03 22:57:04,343 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-03 22:57:04,343 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-03 22:57:16,574 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-03 22:57:17,331 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-e3360f65f1f28706.arrow\n",
      "2022-08-03 22:57:17,662 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-bbcf864065c98515.arrow\n",
      "2022-08-03 22:57:17,663 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-03 22:57:17,998 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-0099eb081810535f.arrow\n",
      "2022-08-03 22:57:18,329 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-7c540055e789733b.arrow\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers - Train dataset: None\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-03 22:57:18,330 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-03 22:57:18,637 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-03 22:57:18,640 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-03 22:57:21,055 - INFO - joeynmt.helpers - Load model from /home/lconti/en-pt_tatoeba/models/finetune_tf/25000.ckpt.\n",
      "2022-08-03 22:57:21,478 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-06617e88cb272dab.arrow\n",
      "2022-08-03 22:57:21,805 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-3d931698fbe82046.arrow\n",
      "2022-08-03 22:57:21,806 - INFO - joeynmt.prediction - Decoding on dev set...\n",
      "2022-08-03 22:57:21,806 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-03 22:58:59,121 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-03 22:58:59,121 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  36.29, generation: 97.2224[sec], evaluation: 0.0734[sec]\n",
      "2022-08-03 22:58:59,462 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-60b0c2cf34a85431.arrow\n",
      "2022-08-03 22:58:59,795 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-087e226f4078b277.arrow\n",
      "2022-08-03 22:58:59,796 - INFO - joeynmt.prediction - Decoding on test set...\n",
      "2022-08-03 22:58:59,797 - INFO - joeynmt.prediction - Predicting 1000 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-03 23:00:39,397 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-03 23:00:39,398 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  40.93, generation: 99.5117[sec], evaluation: 0.0744[sec]\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt test {data_dir}/config_finetune.yaml --ckpt {new_model_dir}/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy5Y4Qr_3p3I"
   },
   "source": [
    "> ⚠ In beam search, the batch size is expanded {beam_size} times. For instance, if batch_size=10, batch_type=sentence and beam_size=5, joeynmt internally creates a batch of length 10*5=50. It may cause an out-of-memory error. Please specify the batch_size in `testing` section of config.yaml by taking this into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ripZMg6kCqzd"
   },
   "source": [
    "The `translate` mode is more interactive and takes prompts to translate interactively.\n",
    "\n",
    "Let's Translate a few examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEcyEwpS1Pvi",
    "outputId": "2259e7d7-ca22-409c-f6ea-593fa3b5de83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:43:59,643 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:44:17,067 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:44:17,429 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:44:21,857 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Übersetzung macht Spaß!\n",
      "2022-06-04 22:46:22,281 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:46:22,379 - INFO - joeynmt.prediction - Generation took 0.0963[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Übersetzung Sinn?\n",
      "2022-06-04 22:47:21,390 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:47:21,489 - INFO - joeynmt.prediction - Generation took 0.0975[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\n",
      "Please enter a source sentence:\n",
      "\n",
      "Bye.\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 2037, in shutdown\n",
      "    h.close()\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1103, in close\n",
      "    stream.close()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH5QwafwIYZu"
   },
   "source": [
    "You can also get the n-best hypotheses (up to the size of the beam, in our example 5), not only the highest scoring one. The better your model gets, the more interesting should the alternatives be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzpnG_3IJwkj"
   },
   "outputs": [],
   "source": [
    "nbest_config = config.replace('n_best: 1', 'n_best: 5')\\\n",
    "  .replace('#return_prob: \"hyp\"', 'return_prob: \"hyp\"')\n",
    "\n",
    "with (Path(data_dir) / \"nbest_config.yaml\").open('w') as f:\n",
    "    f.write(nbest_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nya4YXTGRurr",
    "outputId": "b879c6e2-2d12-4d22-d1ad-d441f87eaae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:48:11,849 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:48:29,174 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:48:29,557 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:48:34,476 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Übersetzung macht Spaß!\n",
      "2022-06-04 22:49:28,668 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:28,766 - INFO - joeynmt.prediction - Generation took 0.0962[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁drink', '▁translation', '▁is', '▁fun', '.', '</s>']\n",
      "\tsequence score: -3.362283706665039\n",
      "#2: Don't make any translation fun.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁make', '▁any', '▁translation', '▁fun', '.', '</s>']\n",
      "\tsequence score: -3.677445411682129\n",
      "#3: Don't make sense to the machine.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁make', '▁sense', '▁to', '▁the', '▁machine', '.', '</s>']\n",
      "\tsequence score: -3.9387967586517334\n",
      "#4: Don't drink translation is fun!\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁drink', '▁translation', '▁is', '▁fun', '!', '</s>']\n",
      "\tsequence score: -3.9769201278686523\n",
      "#5: Don't make a good translation.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁make', '▁a', '▁good', '▁translation', '.', '</s>']\n",
      "\tsequence score: -4.1912617683410645\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Übersetzung Sinn?\n",
      "2022-06-04 22:49:47,934 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:48,022 - INFO - joeynmt.prediction - Generation took 0.0862[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\ttokens: ['▁When', '▁does', '▁machine', '▁make', '▁sense', '?', '</s>']\n",
      "\tsequence score: -2.5040993690490723\n",
      "#2: When does the translation make sense?\n",
      "\ttokens: ['▁When', '▁does', '▁the', '▁translation', '▁make', '▁sense', '?', '</s>']\n",
      "\tsequence score: -2.5658979415893555\n",
      "#3: When does machine make sense to sense?\n",
      "\ttokens: ['▁When', '▁does', '▁machine', '▁make', '▁sense', '▁to', '▁sense', '?', '</s>']\n",
      "\tsequence score: -3.325543165206909\n",
      "#4: When does machine make sense to make?\n",
      "\ttokens: ['▁When', '▁does', '▁machine', '▁make', '▁sense', '▁to', '▁make', '?', '</s>']\n",
      "\tsequence score: -3.422945976257324\n",
      "#5: When does the translation use?\n",
      "\ttokens: ['▁When', '▁does', '▁the', '▁translation', '▁use', '?', '</s>']\n",
      "\tsequence score: -3.6186904907226562\n",
      "\n",
      "Please enter a source sentence:\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 57, in main\n",
      "    output_path=args.output_path,\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/nbest_config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28sXN0GNIYAM"
   },
   "source": [
    "> 💡 In BPE decoding, there are multiple ways to tokenize one sequence. That is, the same output string sequence might appear multiple times in the n best list, because they have different tokenization and thus different sequence in the generation.\n",
    "> For instance, say 3-best generation were:\n",
    "> ```\n",
    "> #1 best ['▁', 'N', 'e', 'w', '▁York']\n",
    "> #2 best ['▁', 'New', '▁York']\n",
    "> #3 best ['▁', 'New', '▁Y', 'o', 'r', 'k']\n",
    "> ````\n",
    "All three were different in next-token prediction, but ended up the same string sequence `New York` after being un-bpe-ed.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "quick-start-with-joeynmt2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('my_jnmt': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "86dc5b16c9dc4e6fbdf96ccbf0568163a9040f9d731ebaeec1938114527cf668"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1795a2b1d6354fd2b1c69b12c9487f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27e4365db7d644d486745b9f334d06db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ff1b97d66404e488862d694facdfc23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fdb10ecb2040f8bd5e07834b422ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a00af231d8a44e9972f6916948418fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79a7716ac54b4f50981033df15f06f8e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c26c414246154062bbd0682340613e40",
      "value": 1
     }
    },
    "4a5a5a13229f404f9e4085751b0e2104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c447e15d9354c3c9a47c3ff843e74ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51afb334205f4a26b43fd6ddd0036cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52fc941d087447aa88ca1336e89d1d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6505b17c75e64c709fc6a91cc362444d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a841a34dd549f4bbe9fed60fc03853",
      "placeholder": "​",
      "style": "IPY_MODEL_31fdb10ecb2040f8bd5e07834b422ca0",
      "value": "Generating train split: "
     }
    },
    "65a841a34dd549f4bbe9fed60fc03853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6690085804c940ccb8dbd87e3f563678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6505b17c75e64c709fc6a91cc362444d",
       "IPY_MODEL_3a00af231d8a44e9972f6916948418fd",
       "IPY_MODEL_e79f7e89c05d4ce4935f53929b8aaaa1"
      ],
      "layout": "IPY_MODEL_4c447e15d9354c3c9a47c3ff843e74ce"
     }
    },
    "6d393ab342cc46109d1c723d736320ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52fc941d087447aa88ca1336e89d1d0f",
      "placeholder": "​",
      "style": "IPY_MODEL_1795a2b1d6354fd2b1c69b12c9487f0b",
      "value": " 1/1 [00:00&lt;00:00, 24.21ba/s]"
     }
    },
    "79a7716ac54b4f50981033df15f06f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9c838f660f7a46da92cfdb54c25fcb58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e9312e5e6646739ae3e6738de0e90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff1b97d66404e488862d694facdfc23",
      "placeholder": "​",
      "style": "IPY_MODEL_27e4365db7d644d486745b9f334d06db",
      "value": "Flattening the indices: 100%"
     }
    },
    "b669da1ab44a4e8cbfa6674f2dac1e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14c2320cc944c469a52a5579ee471bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c838f660f7a46da92cfdb54c25fcb58",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51afb334205f4a26b43fd6ddd0036cbe",
      "value": 1
     }
    },
    "c26c414246154062bbd0682340613e40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c951a5fc9f1540249dbb03a0ecf2f2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e9312e5e6646739ae3e6738de0e90c",
       "IPY_MODEL_c14c2320cc944c469a52a5579ee471bb",
       "IPY_MODEL_6d393ab342cc46109d1c723d736320ad"
      ],
      "layout": "IPY_MODEL_b669da1ab44a4e8cbfa6674f2dac1e5b"
     }
    },
    "e73400a7748241d29340d21f94784a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e79f7e89c05d4ce4935f53929b8aaaa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e73400a7748241d29340d21f94784a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_4a5a5a13229f404f9e4085751b0e2104",
      "value": " 304551/0 [00:13&lt;00:00, 21566.82 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
