{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k051L-Y3rTu"
   },
   "source": [
    "\n",
    "# Fine-tuning with no teacher forcing an en-pt NMT model trained with JoeyNMT 2.0 on the tatoeba corpus\n",
    "\n",
    "This notebook is based on [this demo](https://github.com/joeynmt/joeynmt/blob/main/notebooks/quick-start-with-joeynmt2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un8VWHfq5a-T"
   },
   "source": [
    "> ⚠ **Important:** Before you start, set runtime type to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_s7s4uevEtx",
    "outputId": "c58870bf-7812-4013-b93e-e323e84dbc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  5 11:08:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 26%   27C    P0    53W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 29%   28C    P0    53W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 29%   29C    P0    58W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:D9:00.0 Off |                  N/A |\n",
      "| 10%   26C    P0    57W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "242SJA2q5dRr"
   },
   "source": [
    "Make sure that you have a compatible PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pQwoOS-OvMLf",
    "outputId": "0bd93e90-3ff0-4812-f5a1-7eb255a5116e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rHdT56SF1J3"
   },
   "source": [
    "Install joeynmt (it's important to clone it from my fork, so teacher forcing can be deactivated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxQUAeHKR7Im",
    "outputId": "eb9697ec-8159-4cf5-a68c-3e2768d6d203"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/lina-conti/joeynmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/lconti/joeynmt\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: future in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.18.2)\n",
      "Requirement already satisfied: pillow in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.23.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (62.3.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.10.1+cu111)\n",
      "Requirement already satisfied: protobuf==3.20.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (3.20.1)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.9.0)\n",
      "Requirement already satisfied: sacrebleu>=2.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.2.0)\n",
      "Requirement already satisfied: sentencepiece in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.1.96)\n",
      "Requirement already satisfied: subword-nmt in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.3.8)\n",
      "Requirement already satisfied: matplotlib in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (3.5.2)\n",
      "Requirement already satisfied: seaborn in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (6.0)\n",
      "Requirement already satisfied: six>=1.12 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (1.14.1)\n",
      "Requirement already satisfied: pylint in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.14.5)\n",
      "Requirement already satisfied: yapf in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (0.32.0)\n",
      "Requirement already satisfied: flake8 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (5.0.3)\n",
      "Requirement already satisfied: pytest in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (7.1.2)\n",
      "Requirement already satisfied: datasets in /home/lconti/my_jnmt/lib/python3.9/site-packages (from joeynmt==2.0.0) (2.4.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (0.8.10)\n",
      "Requirement already satisfied: portalocker in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (2.5.1)\n",
      "Requirement already satisfied: lxml in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (4.9.1)\n",
      "Requirement already satisfied: regex in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (2022.7.25)\n",
      "Requirement already satisfied: colorama in /home/lconti/my_jnmt/lib/python3.9/site-packages (from sacrebleu>=2.0.0->joeynmt==2.0.0) (0.4.5)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (1.47.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (2.9.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from tensorboard>=1.15->joeynmt==2.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torch>=1.10.0->joeynmt==2.0.0) (4.3.0)\n",
      "Requirement already satisfied: packaging in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (2022.7.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (1.4.3)\n",
      "Requirement already satisfied: aiohttp in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (8.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: multiprocess in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (0.70.13)\n",
      "Requirement already satisfied: xxhash in /home/lconti/my_jnmt/lib/python3.9/site-packages (from datasets->joeynmt==2.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyflakes<2.6.0,>=2.5.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from flake8->joeynmt==2.0.0) (2.5.0)\n",
      "Requirement already satisfied: pycodestyle<2.10.0,>=2.9.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from flake8->joeynmt==2.0.0) (2.9.0)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from flake8->joeynmt==2.0.0) (0.7.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from matplotlib->joeynmt==2.0.0) (0.11.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: tomlkit>=0.10.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (0.11.1)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (5.10.1)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (2.5.2)\n",
      "Requirement already satisfied: astroid<=2.12.0-dev0,>=2.11.6 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pylint->joeynmt==2.0.0) (2.11.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (22.1.0)\n",
      "Requirement already satisfied: iniconfig in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pytest->joeynmt==2.0.0) (1.11.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from seaborn->joeynmt==2.0.0) (1.9.0)\n",
      "Requirement already satisfied: mock in /home/lconti/my_jnmt/lib/python3.9/site-packages (from subword-nmt->joeynmt==2.0.0) (4.0.3)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from astroid<=2.12.0-dev0,>=2.11.6->pylint->joeynmt==2.0.0) (1.7.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==2.0.0) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/lconti/my_jnmt/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->joeynmt==2.0.0) (3.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==2.0.0) (4.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pandas->datasets->joeynmt==2.0.0) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==2.0.0) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=1.15->joeynmt==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from aiohttp->datasets->joeynmt==2.0.0) (1.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==2.0.0) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==2.0.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==2.0.0) (3.2.0)\n",
      "Installing collected packages: joeynmt\n",
      "  Attempting uninstall: joeynmt\n",
      "    Found existing installation: joeynmt 2.0.0\n",
      "    Uninstalling joeynmt-2.0.0:\n",
      "      Successfully uninstalled joeynmt-2.0.0\n",
      "  Running setup.py develop for joeynmt\n",
      "Successfully installed joeynmt-2.0.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip3 install -e /home/lconti/joeynmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1RMfXeT-V1m"
   },
   "source": [
    "### Dataset and vocabulary\n",
    "\n",
    "The dataset and vocabulary are the same as for the pre-trained model, so we don't need to compute them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/lconti/en-pt_tatoeba\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6cN9CPtAaPl"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Joey NMT reads model and training hyperparameters from a configuration file. We're generating this now to configure paths in the appropriate places.\n",
    "\n",
    "The configuration below builds a small Transformer model with shared embeddings between source and target language on the base of the subword vocabularies created above.\n",
    "\n",
    "Note the \"teacher_forcing\" configuration in \"model\" — this is specific to my fork of joeynmt. It is where you can choose between \"on\", \"off\" or \"alternating\" (default is on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model_dir = \"/home/lconti/en-pt_tatoeba/models/normal_tf\"\n",
    "new_model_dir = \"/home/lconti/en-pt_tatoeba/models/finetune_tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fJML2jYR1PlG"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"tatoeba_enpt_finetune_tf_sp\"\n",
    "joeynmt_version: \"2.0.0\"\n",
    "\n",
    "data:\n",
    "    train: \"{data_dir}/train\"\n",
    "    dev: \"{data_dir}/validation\"\n",
    "    test: \"{data_dir}/test\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    #dataset_cfg:           # not necessary for manually saved pyarray daraset\n",
    "    #    name: \"en-pt\"\n",
    "    sample_dev_subset: 200\n",
    "    src:\n",
    "        lang: \"en\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "    trg:\n",
    "        lang: \"pt\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 32000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "\n",
    "\"\"\".format(data_dir=data_dir)\n",
    "with (Path(data_dir) / \"config_finetune.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OJcLOr_S2BTD"
   },
   "outputs": [],
   "source": [
    "config += \"\"\"\n",
    "testing:\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 256\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 100\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    #return_prob: \"hyp\"\n",
    "    #return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "\n",
    "training:\n",
    "    load_model: \"{old_model_dir}/latest.ckpt\"\n",
    "    reset_best_ckpt: True\n",
    "    reset_scheduler: True\n",
    "    reset_optimizer: True\n",
    "    #reset_iter_state: False\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999]\n",
    "    scheduling: \"warmupinversesquareroot\"\n",
    "    learning_rate_warmup: 2000\n",
    "    learning_rate: 0.0002\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    loss: \"crossentropy\"\n",
    "    batch_size: 512\n",
    "    batch_type: \"token\"\n",
    "    batch_multiplier: 4\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 10\n",
    "    updates: 40000\n",
    "    validation_freq: 1000\n",
    "    logging_freq: 100\n",
    "    model_dir: \"{new_model_dir}\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\".format(old_model_dir=old_model_dir, new_model_dir=new_model_dir)\n",
    "with (Path(data_dir) / \"config_finetune.yaml\").open('w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w45HbBfeMW38"
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH0-HBLLBLmR"
   },
   "source": [
    "### Run training\n",
    "⏳ This will take a while. Model parameters will be stored on mounted google drive. The log reports the training process, look out for the prints of example translations and the BLEU evaluation scores to get an impression of the current quality.\n",
    "\n",
    "> ⛔ If you execute this twice, you might get an error that the model directory already exists. You can specify in the configuration to overwrite it, or delete it manually (`!rm -r {model_dir}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTbfgVOq2BfB",
    "outputId": "b667e5d9-4587-4f92-f8a5-308b7826ac51"
   },
   "outputs": [],
   "source": [
    "!python -m joeynmt train {data_dir}/config_finetune.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7qULQu0B5Yl"
   },
   "source": [
    "### Continue training after interruption\n",
    "To continue after an interruption, the configuration needs to be modified in the following places:\n",
    "\n",
    "- `load_model` to point to the checkpoint to load.\n",
    "- `reset_*` options (must be False) to resume the previous session.\n",
    "- `model_dir` to create a new directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When resuming training, I get the bug described [here](https://github.com/pytorch/pytorch/issues/80809). I haven't been able to find another solution besides downgrading pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.10.1+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torch-1.10.1%2Bcu111-cp39-cp39-linux_x86_64.whl (2137.7 MB)\n",
      "Collecting torchvision==0.11.2+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torchvision-0.11.2%2Bcu111-cp39-cp39-linux_x86_64.whl (24.5 MB)\n",
      "Collecting torchaudio==0.10.1\n",
      "  Using cached https://download.pytorch.org/whl/rocm4.1/torchaudio-0.10.1%2Brocm4.1-cp39-cp39-linux_x86_64.whl (2.7 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torch==1.10.1+cu111) (4.3.0)\n",
      "Requirement already satisfied: numpy in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torchvision==0.11.2+cu111) (1.23.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/lconti/my_jnmt/lib/python3.9/site-packages (from torchvision==0.11.2+cu111) (9.2.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "Successfully installed torch-1.10.1+cu111 torchaudio-0.10.1+rocm4.1 torchvision-0.11.2+cu111\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 62.3.2\n",
      "    Uninstalling setuptools-62.3.2:\n",
      "      Successfully uninstalled setuptools-62.3.2\n",
      "Successfully installed setuptools-59.5.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install setuptools==59.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Fr3mEDga2BiJ"
   },
   "outputs": [],
   "source": [
    "resume_config = config\\\n",
    "  .replace('load_model: \"/home/lconti/en-pt_tatoeba/models/normal_tf/latest.ckpt\"', '\"/home/lconti/en-pt_tatoeba/models/finetune_tf/latest.ckpt\"')\\\n",
    "  .replace('reset_best_ckpt: True', 'reset_best_ckpt: False')\\\n",
    "  .replace('reset_scheduler: True', 'reset_scheduler: False')\\\n",
    "  .replace('reset_optimizer: True', 'reset_optimizer: False')\\\n",
    "  .replace('#reset_iter_state: False', 'reset_iter_state: False')\\\n",
    "  .replace('model_dir: \"/home/lconti/en-pt_tatoeba/models/finetune_tf\"', 'model_dir: \"/home/lconti/en-pt_tatoeba/models/finetune2_tf\"')\n",
    "\n",
    "with (Path(data_dir) / \"config_finetune2.yaml\").open('w') as f:\n",
    "    f.write(resume_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgH1vAsV2Bkw",
    "outputId": "56439036-1e0d-4fb9-cffd-257f09903d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 17:00:26,618 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -                           cfg.name : tatoeba_enpt_finetune2_tf_sp\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -                     cfg.data.train : /home/lconti/en-pt_tatoeba/train\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/lconti/en-pt_tatoeba/validation\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -                      cfg.data.test : /home/lconti/en-pt_tatoeba/test\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -              cfg.data.dataset_type : huggingface\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : 200\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100\n",
      "2022-08-05 17:00:26,618 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/lconti/en-pt_tatoeba/vocab.txt\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : sentencepiece\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/sp.model\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : pt\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/lconti/en-pt_tatoeba/vocab.txt\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : sentencepiece\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.model_file : /home/lconti/en-pt_tatoeba/sp.model\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 256\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 100\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/lconti/en-pt_tatoeba/models/finetune_tf/latest.ckpt\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False\n",
      "2022-08-05 17:00:26,619 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -            cfg.training.scheduling : warmupinversesquareroot\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 2000\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0002\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -            cfg.training.batch_size : 512\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 4\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : bleu\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -                cfg.training.epochs : 10\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -               cfg.training.updates : 40000\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/lconti/en-pt_tatoeba/models/finetune2_tf\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -             cfg.training.overwrite : False\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
      "2022-08-05 17:00:26,620 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1\n",
      "2022-08-05 17:00:26,621 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre\n",
      "2022-08-05 17:00:26,651 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-05 17:00:26,736 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 17:00:26,736 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 17:00:26,736 - INFO - joeynmt.data - Loading train set...\n",
      "2022-08-05 17:00:26,975 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/train/cache-4be94cce75c187c1.arrow\n",
      "2022-08-05 17:00:26,991 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/train/cache-0066646d166ee7b1.arrow\n",
      "2022-08-05 17:00:26,994 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-05 17:00:39,258 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-05 17:00:39,599 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-e3360f65f1f28706.arrow\n",
      "2022-08-05 17:00:39,931 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-bbcf864065c98515.arrow\n",
      "2022-08-05 17:00:39,932 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-05 17:00:40,263 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-0099eb081810535f.arrow\n",
      "2022-08-05 17:00:40,595 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-7c540055e789733b.arrow\n",
      "2022-08-05 17:00:40,596 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-05 17:00:40,596 - INFO - joeynmt.helpers - Train dataset: HuggingfaceDataset(len=215642, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=train, path=/home/lconti/en-pt_tatoeba/train)\n",
      "2022-08-05 17:00:40,596 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-05 17:00:40,596 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-05 17:00:40,597 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] ▁What ▁do ▁you ▁want ▁now ?\n",
      "\t[TRG] ▁O ▁que ▁você ▁deseja ▁agora ?\n",
      "2022-08-05 17:00:40,597 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-05 17:00:40,597 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-05 17:00:40,597 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-05 17:00:40,597 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-05 17:00:40,611 - WARNING - joeynmt.tokenizers - /home/lconti/en-pt_tatoeba/models/finetune2_tf/sp.model already exists. Stop copying.\n",
      "2022-08-05 17:00:40,612 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-05 17:00:40,927 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-05 17:00:40,932 - INFO - joeynmt.model - Total params: 19252224\n",
      "2022-08-05 17:00:40,933 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\"),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=32000),\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))\n",
      "2022-08-05 17:00:44,317 - INFO - joeynmt.builders - Adam(lr=0.0002, weight_decay=0.0, betas=[0.9, 0.999])\n",
      "2022-08-05 17:00:44,317 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=2000, decay_rate=0.008944, peak_rate=0.0002, min_rate=1e-08)\n",
      "2022-08-05 17:00:44,317 - INFO - joeynmt.training - Loading model from /home/lconti/en-pt_tatoeba/models/finetune_tf/latest.ckpt\n",
      "2022-08-05 17:00:44,524 - INFO - joeynmt.helpers - Load model from /home/lconti/en-pt_tatoeba/models/finetune_tf/30000.ckpt.\n",
      "2022-08-05 17:00:44,579 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 4\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 4\n",
      "\tbatch size per device: 128\n",
      "\teffective batch size (w. parallel & accumulation): 2048\n",
      "2022-08-05 17:00:44,579 - INFO - joeynmt.training - EPOCH 1\n",
      "/home/lconti/my_jnmt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2022-08-05 17:01:57,976 - INFO - joeynmt.training - Epoch   1, Step:    30100, Batch Loss:     0.163328, Batch Acc: 0.007574, Tokens per Sec:     1284, Lr: 0.000052\n",
      "2022-08-05 17:03:00,063 - INFO - joeynmt.training - Epoch   1, Step:    30200, Batch Loss:     0.170895, Batch Acc: 0.008271, Tokens per Sec:     1554, Lr: 0.000051\n",
      "2022-08-05 17:04:03,714 - INFO - joeynmt.training - Epoch   1, Step:    30300, Batch Loss:     0.235371, Batch Acc: 0.005881, Tokens per Sec:     1512, Lr: 0.000051\n",
      "2022-08-05 17:05:09,306 - INFO - joeynmt.training - Epoch   1, Step:    30400, Batch Loss:     0.196867, Batch Acc: 0.008328, Tokens per Sec:     1404, Lr: 0.000051\n",
      "2022-08-05 17:06:13,609 - INFO - joeynmt.training - Epoch   1, Step:    30500, Batch Loss:     0.217897, Batch Acc: 0.007563, Tokens per Sec:     1520, Lr: 0.000051\n",
      "2022-08-05 17:07:16,282 - INFO - joeynmt.training - Epoch   1, Step:    30600, Batch Loss:     0.197540, Batch Acc: 0.008400, Tokens per Sec:     1508, Lr: 0.000051\n",
      "2022-08-05 17:08:22,081 - INFO - joeynmt.training - Epoch   1, Step:    30700, Batch Loss:     0.202493, Batch Acc: 0.008698, Tokens per Sec:     1447, Lr: 0.000051\n",
      "2022-08-05 17:09:27,313 - INFO - joeynmt.training - Epoch   1, Step:    30800, Batch Loss:     0.202917, Batch Acc: 0.008340, Tokens per Sec:     1469, Lr: 0.000051\n",
      "2022-08-05 17:10:32,724 - INFO - joeynmt.training - Epoch   1, Step:    30900, Batch Loss:     0.174225, Batch Acc: 0.007572, Tokens per Sec:     1438, Lr: 0.000051\n",
      "2022-08-05 17:11:37,974 - INFO - joeynmt.training - Epoch   1, Step:    31000, Batch Loss:     0.173293, Batch Acc: 0.007877, Tokens per Sec:     1459, Lr: 0.000051\n",
      "2022-08-05 17:11:38,309 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-06617e88cb272dab.arrow\n",
      "2022-08-05 17:11:38,641 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-3d931698fbe82046.arrow\n",
      "2022-08-05 17:11:38,643 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-f959ed6f4f652086.arrow\n",
      "2022-08-05 17:11:38,645 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=31000\n",
      "2022-08-05 17:11:38,645 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:11:56,503 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:11:56,504 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.01, loss:   1.55, ppl:   4.70, acc:   0.69, generation: 17.8359[sec], evaluation: 0.0181[sec]\n",
      "2022-08-05 17:11:56,790 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 17:11:56,791 - INFO - joeynmt.training - \tSource:     One million people lost their lives in the war.\n",
      "2022-08-05 17:11:56,792 - INFO - joeynmt.training - \tReference:  Um milhão de pessoas perderam as suas vidas na guerra.\n",
      "2022-08-05 17:11:56,792 - INFO - joeynmt.training - \tHypothesis: Uma milhão de pessoas perderam sua vida na guerra.\n",
      "2022-08-05 17:11:56,792 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 17:11:56,793 - INFO - joeynmt.training - \tSource:     I will be back soon.\n",
      "2022-08-05 17:11:56,793 - INFO - joeynmt.training - \tReference:  Eu voltarei em breve.\n",
      "2022-08-05 17:11:56,793 - INFO - joeynmt.training - \tHypothesis: Voltarei logo.\n",
      "2022-08-05 17:11:56,793 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 17:11:56,794 - INFO - joeynmt.training - \tSource:     Why are you sorry for something you haven't done?\n",
      "2022-08-05 17:11:56,794 - INFO - joeynmt.training - \tReference:  Por que a senhora se desculpa de algo que não fez?\n",
      "2022-08-05 17:11:56,794 - INFO - joeynmt.training - \tHypothesis: Por que você está meio alguma coisa que não fez?\n",
      "2022-08-05 17:11:56,794 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 17:11:56,795 - INFO - joeynmt.training - \tSource:     I've always wondered what it'd be like to have siblings.\n",
      "2022-08-05 17:11:56,796 - INFO - joeynmt.training - \tReference:  Eu sempre imaginei como seria ter irmãos.\n",
      "2022-08-05 17:11:56,796 - INFO - joeynmt.training - \tHypothesis: Eu sempre me perguntei o que seria como ter irmãos.\n",
      "2022-08-05 17:13:01,971 - INFO - joeynmt.training - Epoch   1, Step:    31100, Batch Loss:     0.192771, Batch Acc: 0.008691, Tokens per Sec:     1414, Lr: 0.000051\n",
      "2022-08-05 17:14:06,674 - INFO - joeynmt.training - Epoch   1, Step:    31200, Batch Loss:     0.195126, Batch Acc: 0.008447, Tokens per Sec:     1462, Lr: 0.000051\n",
      "2022-08-05 17:15:12,214 - INFO - joeynmt.training - Epoch   1, Step:    31300, Batch Loss:     0.228345, Batch Acc: 0.008488, Tokens per Sec:     1443, Lr: 0.000051\n",
      "2022-08-05 17:16:18,086 - INFO - joeynmt.training - Epoch   1, Step:    31400, Batch Loss:     0.177120, Batch Acc: 0.008100, Tokens per Sec:     1449, Lr: 0.000050\n",
      "2022-08-05 17:17:23,621 - INFO - joeynmt.training - Epoch   1, Step:    31500, Batch Loss:     0.203908, Batch Acc: 0.009052, Tokens per Sec:     1438, Lr: 0.000050\n",
      "2022-08-05 17:18:29,337 - INFO - joeynmt.training - Epoch   1, Step:    31600, Batch Loss:     0.223415, Batch Acc: 0.007020, Tokens per Sec:     1470, Lr: 0.000050\n",
      "2022-08-05 17:19:34,715 - INFO - joeynmt.training - Epoch   1, Step:    31700, Batch Loss:     0.167297, Batch Acc: 0.008243, Tokens per Sec:     1462, Lr: 0.000050\n",
      "2022-08-05 17:20:39,719 - INFO - joeynmt.training - Epoch   1, Step:    31800, Batch Loss:     0.154623, Batch Acc: 0.009496, Tokens per Sec:     1443, Lr: 0.000050\n",
      "2022-08-05 17:21:44,280 - INFO - joeynmt.training - Epoch   1, Step:    31900, Batch Loss:     0.165084, Batch Acc: 0.009224, Tokens per Sec:     1463, Lr: 0.000050\n",
      "2022-08-05 17:22:48,543 - INFO - joeynmt.training - Epoch   1, Step:    32000, Batch Loss:     0.190590, Batch Acc: 0.008977, Tokens per Sec:     1461, Lr: 0.000050\n",
      "2022-08-05 17:22:48,878 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-029a20f0641e58ce.arrow\n",
      "2022-08-05 17:22:49,212 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-b2361f443518ab42.arrow\n",
      "2022-08-05 17:22:49,213 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-66a8e8d4a8012d2f.arrow\n",
      "2022-08-05 17:22:49,215 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=32000\n",
      "2022-08-05 17:22:49,215 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:23:06,184 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:23:06,184 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  33.01, loss:   1.66, ppl:   5.25, acc:   0.68, generation: 16.9453[sec], evaluation: 0.0172[sec]\n",
      "2022-08-05 17:23:06,185 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 17:23:06,187 - INFO - joeynmt.training - \tSource:     This is a pun.\n",
      "2022-08-05 17:23:06,187 - INFO - joeynmt.training - \tReference:  Isso é um jogo de palavras.\n",
      "2022-08-05 17:23:06,187 - INFO - joeynmt.training - \tHypothesis: Isto é um trocadilho.\n",
      "2022-08-05 17:23:06,187 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 17:23:06,188 - INFO - joeynmt.training - \tSource:     I do not have an account in these forums.\n",
      "2022-08-05 17:23:06,188 - INFO - joeynmt.training - \tReference:  Eu não tenho uma conta nesses fóruns.\n",
      "2022-08-05 17:23:06,188 - INFO - joeynmt.training - \tHypothesis: Eu não tenho uma conta nesta época.\n",
      "2022-08-05 17:23:06,188 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 17:23:06,190 - INFO - joeynmt.training - \tSource:     Wine is poetry filled in bottles.\n",
      "2022-08-05 17:23:06,190 - INFO - joeynmt.training - \tReference:  O vinho é poesia engarrafada.\n",
      "2022-08-05 17:23:06,190 - INFO - joeynmt.training - \tHypothesis: O vinho é cheio de poesias.\n",
      "2022-08-05 17:23:06,190 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 17:23:06,191 - INFO - joeynmt.training - \tSource:     I made my decision.\n",
      "2022-08-05 17:23:06,191 - INFO - joeynmt.training - \tReference:  Minha decisão foi tomada.\n",
      "2022-08-05 17:23:06,191 - INFO - joeynmt.training - \tHypothesis: Eu fiz minha decisão.\n",
      "2022-08-05 17:23:15,330 - INFO - joeynmt.training - Epoch   1: total training loss 375.43\n",
      "2022-08-05 17:23:15,331 - INFO - joeynmt.training - EPOCH 2\n",
      "2022-08-05 17:24:11,807 - INFO - joeynmt.training - Epoch   2, Step:    32100, Batch Loss:     0.191061, Batch Acc: 0.008735, Tokens per Sec:     1431, Lr: 0.000050\n",
      "2022-08-05 17:25:18,396 - INFO - joeynmt.training - Epoch   2, Step:    32200, Batch Loss:     0.190167, Batch Acc: 0.008478, Tokens per Sec:     1414, Lr: 0.000050\n",
      "2022-08-05 17:26:24,534 - INFO - joeynmt.training - Epoch   2, Step:    32300, Batch Loss:     0.182773, Batch Acc: 0.007937, Tokens per Sec:     1448, Lr: 0.000050\n",
      "2022-08-05 17:27:30,184 - INFO - joeynmt.training - Epoch   2, Step:    32400, Batch Loss:     0.165334, Batch Acc: 0.009318, Tokens per Sec:     1439, Lr: 0.000050\n",
      "2022-08-05 17:28:35,981 - INFO - joeynmt.training - Epoch   2, Step:    32500, Batch Loss:     0.194110, Batch Acc: 0.007324, Tokens per Sec:     1442, Lr: 0.000050\n",
      "2022-08-05 17:29:41,037 - INFO - joeynmt.training - Epoch   2, Step:    32600, Batch Loss:     0.206834, Batch Acc: 0.008157, Tokens per Sec:     1468, Lr: 0.000050\n",
      "2022-08-05 17:30:46,586 - INFO - joeynmt.training - Epoch   2, Step:    32700, Batch Loss:     0.153649, Batch Acc: 0.008675, Tokens per Sec:     1440, Lr: 0.000049\n",
      "2022-08-05 17:31:51,945 - INFO - joeynmt.training - Epoch   2, Step:    32800, Batch Loss:     0.161164, Batch Acc: 0.009152, Tokens per Sec:     1449, Lr: 0.000049\n",
      "2022-08-05 17:32:57,343 - INFO - joeynmt.training - Epoch   2, Step:    32900, Batch Loss:     0.169746, Batch Acc: 0.008373, Tokens per Sec:     1446, Lr: 0.000049\n",
      "2022-08-05 17:34:02,699 - INFO - joeynmt.training - Epoch   2, Step:    33000, Batch Loss:     0.195142, Batch Acc: 0.008349, Tokens per Sec:     1457, Lr: 0.000049\n",
      "2022-08-05 17:34:03,034 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-c19a7f080d2bdbf5.arrow\n",
      "2022-08-05 17:34:03,365 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-9a393b598127d5fc.arrow\n",
      "2022-08-05 17:34:03,366 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-038cd91b82f23b04.arrow\n",
      "2022-08-05 17:34:03,368 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=33000\n",
      "2022-08-05 17:34:03,368 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:34:18,813 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:34:18,813 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  33.09, loss:   1.62, ppl:   5.04, acc:   0.67, generation: 15.4227[sec], evaluation: 0.0172[sec]\n",
      "2022-08-05 17:34:18,814 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 17:34:18,816 - INFO - joeynmt.training - \tSource:     What do you think I've been doing?\n",
      "2022-08-05 17:34:18,816 - INFO - joeynmt.training - \tReference:  Que pensais que eu estive fazendo?\n",
      "2022-08-05 17:34:18,816 - INFO - joeynmt.training - \tHypothesis: O que você acha que eu estive fazendo?\n",
      "2022-08-05 17:34:18,816 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 17:34:18,818 - INFO - joeynmt.training - \tSource:     Why do you ask?\n",
      "2022-08-05 17:34:18,818 - INFO - joeynmt.training - \tReference:  Por que você está perguntando?\n",
      "2022-08-05 17:34:18,818 - INFO - joeynmt.training - \tHypothesis: Por que você pergunta?\n",
      "2022-08-05 17:34:18,818 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 17:34:18,819 - INFO - joeynmt.training - \tSource:     You're by my side; everything's fine now.\n",
      "2022-08-05 17:34:18,819 - INFO - joeynmt.training - \tReference:  Você está do meu lado. Agora tudo está bem.\n",
      "2022-08-05 17:34:18,819 - INFO - joeynmt.training - \tHypothesis: Você está do meu lado; tudo está bem agora.\n",
      "2022-08-05 17:34:18,819 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 17:34:18,821 - INFO - joeynmt.training - \tSource:     You're so impatient with me.\n",
      "2022-08-05 17:34:18,821 - INFO - joeynmt.training - \tReference:  Você é tão impaciente comigo.\n",
      "2022-08-05 17:34:18,821 - INFO - joeynmt.training - \tHypothesis: Você está tão impaciente comigo.\n",
      "2022-08-05 17:35:24,680 - INFO - joeynmt.training - Epoch   2, Step:    33100, Batch Loss:     0.192134, Batch Acc: 0.008275, Tokens per Sec:     1429, Lr: 0.000049\n",
      "2022-08-05 17:36:30,647 - INFO - joeynmt.training - Epoch   2, Step:    33200, Batch Loss:     0.167450, Batch Acc: 0.008873, Tokens per Sec:     1437, Lr: 0.000049\n",
      "2022-08-05 17:37:35,749 - INFO - joeynmt.training - Epoch   2, Step:    33300, Batch Loss:     0.160009, Batch Acc: 0.009067, Tokens per Sec:     1481, Lr: 0.000049\n",
      "2022-08-05 17:38:41,589 - INFO - joeynmt.training - Epoch   2, Step:    33400, Batch Loss:     0.217591, Batch Acc: 0.007131, Tokens per Sec:     1446, Lr: 0.000049\n",
      "2022-08-05 17:39:47,213 - INFO - joeynmt.training - Epoch   2, Step:    33500, Batch Loss:     0.200845, Batch Acc: 0.006967, Tokens per Sec:     1446, Lr: 0.000049\n",
      "2022-08-05 17:40:53,072 - INFO - joeynmt.training - Epoch   2, Step:    33600, Batch Loss:     0.180087, Batch Acc: 0.008390, Tokens per Sec:     1459, Lr: 0.000049\n",
      "2022-08-05 17:41:58,294 - INFO - joeynmt.training - Epoch   2, Step:    33700, Batch Loss:     0.165163, Batch Acc: 0.008466, Tokens per Sec:     1458, Lr: 0.000049\n",
      "2022-08-05 17:43:03,584 - INFO - joeynmt.training - Epoch   2, Step:    33800, Batch Loss:     0.233428, Batch Acc: 0.007017, Tokens per Sec:     1458, Lr: 0.000049\n",
      "2022-08-05 17:44:08,107 - INFO - joeynmt.training - Epoch   2, Step:    33900, Batch Loss:     0.163092, Batch Acc: 0.009486, Tokens per Sec:     1472, Lr: 0.000049\n",
      "2022-08-05 17:45:11,560 - INFO - joeynmt.training - Epoch   2, Step:    34000, Batch Loss:     0.192720, Batch Acc: 0.008080, Tokens per Sec:     1482, Lr: 0.000049\n",
      "2022-08-05 17:45:11,893 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-47995618f09ccc42.arrow\n",
      "2022-08-05 17:45:12,223 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-092e370b688faf3a.arrow\n",
      "2022-08-05 17:45:12,225 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-4359931daf5a7fe6.arrow\n",
      "2022-08-05 17:45:12,226 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=34000\n",
      "2022-08-05 17:45:12,226 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:45:28,186 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:45:28,186 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  36.45, loss:   1.47, ppl:   4.36, acc:   0.70, generation: 15.9388[sec], evaluation: 0.0163[sec]\n",
      "2022-08-05 17:45:28,484 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 17:45:28,486 - INFO - joeynmt.training - \tSource:     We can't sleep because of the noise.\n",
      "2022-08-05 17:45:28,486 - INFO - joeynmt.training - \tReference:  Nós não conseguimos dormir por causa do barulho.\n",
      "2022-08-05 17:45:28,486 - INFO - joeynmt.training - \tHypothesis: Não podemos dormir por causa do barulho.\n",
      "2022-08-05 17:45:28,486 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 17:45:28,488 - INFO - joeynmt.training - \tSource:     It's too expensive!\n",
      "2022-08-05 17:45:28,488 - INFO - joeynmt.training - \tReference:  É muito caro!\n",
      "2022-08-05 17:45:28,488 - INFO - joeynmt.training - \tHypothesis: É caro demais!\n",
      "2022-08-05 17:45:28,488 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 17:45:28,489 - INFO - joeynmt.training - \tSource:     I wouldn't have thought I would someday look up \"Viagra\" in Wikipedia.\n",
      "2022-08-05 17:45:28,489 - INFO - joeynmt.training - \tReference:  Eu jamais imaginaria que algum dia buscaria por \"Viagra\" na Wikipédia.\n",
      "2022-08-05 17:45:28,489 - INFO - joeynmt.training - \tHypothesis: Eu não teria pensado que eu ficaria um dia tomeei \"Vado\" na Wikipédia.\n",
      "2022-08-05 17:45:28,489 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 17:45:28,490 - INFO - joeynmt.training - \tSource:     It is unfortunately true.\n",
      "2022-08-05 17:45:28,490 - INFO - joeynmt.training - \tReference:  Infelizmente é verdade.\n",
      "2022-08-05 17:45:28,490 - INFO - joeynmt.training - \tHypothesis: Infelizmente, é verdade.\n",
      "2022-08-05 17:45:45,476 - INFO - joeynmt.training - Epoch   2: total training loss 363.34\n",
      "2022-08-05 17:45:45,476 - INFO - joeynmt.training - EPOCH 3\n",
      "2022-08-05 17:46:30,994 - INFO - joeynmt.training - Epoch   3, Step:    34100, Batch Loss:     0.198851, Batch Acc: 0.010689, Tokens per Sec:     1523, Lr: 0.000048\n",
      "2022-08-05 17:47:33,712 - INFO - joeynmt.training - Epoch   3, Step:    34200, Batch Loss:     0.176386, Batch Acc: 0.008072, Tokens per Sec:     1523, Lr: 0.000048\n",
      "2022-08-05 17:48:49,510 - INFO - joeynmt.training - Epoch   3, Step:    34300, Batch Loss:     0.192433, Batch Acc: 0.008733, Tokens per Sec:     1243, Lr: 0.000048\n",
      "2022-08-05 17:50:03,772 - INFO - joeynmt.training - Epoch   3, Step:    34400, Batch Loss:     0.157712, Batch Acc: 0.008383, Tokens per Sec:     1283, Lr: 0.000048\n",
      "2022-08-05 17:51:13,704 - INFO - joeynmt.training - Epoch   3, Step:    34500, Batch Loss:     0.178149, Batch Acc: 0.009267, Tokens per Sec:     1380, Lr: 0.000048\n",
      "2022-08-05 17:52:28,342 - INFO - joeynmt.training - Epoch   3, Step:    34600, Batch Loss:     0.173800, Batch Acc: 0.009253, Tokens per Sec:     1276, Lr: 0.000048\n",
      "2022-08-05 17:53:43,391 - INFO - joeynmt.training - Epoch   3, Step:    34700, Batch Loss:     0.184123, Batch Acc: 0.008061, Tokens per Sec:     1261, Lr: 0.000048\n",
      "2022-08-05 17:54:59,075 - INFO - joeynmt.training - Epoch   3, Step:    34800, Batch Loss:     0.245676, Batch Acc: 0.007679, Tokens per Sec:     1244, Lr: 0.000048\n",
      "2022-08-05 17:56:11,512 - INFO - joeynmt.training - Epoch   3, Step:    34900, Batch Loss:     0.174905, Batch Acc: 0.008034, Tokens per Sec:     1323, Lr: 0.000048\n",
      "2022-08-05 17:57:20,066 - INFO - joeynmt.training - Epoch   3, Step:    35000, Batch Loss:     0.184735, Batch Acc: 0.007855, Tokens per Sec:     1376, Lr: 0.000048\n",
      "2022-08-05 17:57:20,431 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-a4f2d3ec98ced3d7.arrow\n",
      "2022-08-05 17:57:20,788 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-c501b805c2f76cfd.arrow\n",
      "2022-08-05 17:57:20,790 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-282f3224b2ca87c2.arrow\n",
      "2022-08-05 17:57:20,792 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=35000\n",
      "2022-08-05 17:57:20,792 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 17:57:38,046 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 17:57:38,047 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.00, loss:   1.49, ppl:   4.43, acc:   0.69, generation: 17.2255[sec], evaluation: 0.0195[sec]\n",
      "2022-08-05 17:57:38,364 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 17:57:38,365 - INFO - joeynmt.training - \tSource:     Nobody came.\n",
      "2022-08-05 17:57:38,365 - INFO - joeynmt.training - \tReference:  Não veio ninguém.\n",
      "2022-08-05 17:57:38,365 - INFO - joeynmt.training - \tHypothesis: Ninguém veio.\n",
      "2022-08-05 17:57:38,365 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 17:57:38,367 - INFO - joeynmt.training - \tSource:     I'd be unhappy, but I wouldn't kill myself.\n",
      "2022-08-05 17:57:38,367 - INFO - joeynmt.training - \tReference:  Eu não ficaria feliz, mas eu não me mataria.\n",
      "2022-08-05 17:57:38,367 - INFO - joeynmt.training - \tHypothesis: Eu seria infeliz, mas não me mataria.\n",
      "2022-08-05 17:57:38,367 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 17:57:38,368 - INFO - joeynmt.training - \tSource:     I never liked biology.\n",
      "2022-08-05 17:57:38,368 - INFO - joeynmt.training - \tReference:  Eu nunca gostei de biologia.\n",
      "2022-08-05 17:57:38,368 - INFO - joeynmt.training - \tHypothesis: Eu nunca gostei de biologia.\n",
      "2022-08-05 17:57:38,369 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 17:57:38,370 - INFO - joeynmt.training - \tSource:     You should sleep.\n",
      "2022-08-05 17:57:38,370 - INFO - joeynmt.training - \tReference:  Você deveria dormir.\n",
      "2022-08-05 17:57:38,370 - INFO - joeynmt.training - \tHypothesis: Você deveria dormir.\n",
      "2022-08-05 17:58:46,324 - INFO - joeynmt.training - Epoch   3, Step:    35100, Batch Loss:     0.178060, Batch Acc: 0.007522, Tokens per Sec:     1366, Lr: 0.000048\n",
      "2022-08-05 17:59:54,922 - INFO - joeynmt.training - Epoch   3, Step:    35200, Batch Loss:     0.170964, Batch Acc: 0.007265, Tokens per Sec:     1373, Lr: 0.000048\n",
      "2022-08-05 18:01:03,692 - INFO - joeynmt.training - Epoch   3, Step:    35300, Batch Loss:     0.144771, Batch Acc: 0.009515, Tokens per Sec:     1383, Lr: 0.000048\n",
      "2022-08-05 18:02:13,146 - INFO - joeynmt.training - Epoch   3, Step:    35400, Batch Loss:     0.152513, Batch Acc: 0.007662, Tokens per Sec:     1379, Lr: 0.000048\n",
      "2022-08-05 18:03:21,681 - INFO - joeynmt.training - Epoch   3, Step:    35500, Batch Loss:     0.172692, Batch Acc: 0.009063, Tokens per Sec:     1365, Lr: 0.000047\n",
      "2022-08-05 18:04:30,193 - INFO - joeynmt.training - Epoch   3, Step:    35600, Batch Loss:     0.225222, Batch Acc: 0.006757, Tokens per Sec:     1389, Lr: 0.000047\n",
      "2022-08-05 18:05:38,327 - INFO - joeynmt.training - Epoch   3, Step:    35700, Batch Loss:     0.147465, Batch Acc: 0.008527, Tokens per Sec:     1389, Lr: 0.000047\n",
      "2022-08-05 18:06:47,808 - INFO - joeynmt.training - Epoch   3, Step:    35800, Batch Loss:     0.141346, Batch Acc: 0.008899, Tokens per Sec:     1363, Lr: 0.000047\n",
      "2022-08-05 18:07:56,017 - INFO - joeynmt.training - Epoch   3, Step:    35900, Batch Loss:     0.177540, Batch Acc: 0.009285, Tokens per Sec:     1388, Lr: 0.000047\n",
      "2022-08-05 18:09:04,651 - INFO - joeynmt.training - Epoch   3, Step:    36000, Batch Loss:     0.170988, Batch Acc: 0.007764, Tokens per Sec:     1372, Lr: 0.000047\n",
      "2022-08-05 18:09:05,006 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-88726572b25f603f.arrow\n",
      "2022-08-05 18:09:05,363 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-3679f97fdc78862a.arrow\n",
      "2022-08-05 18:09:05,364 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-864be320efffa082.arrow\n",
      "2022-08-05 18:09:05,366 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=36000\n",
      "2022-08-05 18:09:05,367 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 18:09:20,499 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 18:09:20,500 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  35.81, loss:   1.52, ppl:   4.56, acc:   0.69, generation: 15.1092[sec], evaluation: 0.0183[sec]\n",
      "2022-08-05 18:09:20,810 - INFO - joeynmt.helpers - delete /home/lconti/en-pt_tatoeba/models/finetune2_tf/31000.ckpt\n",
      "2022-08-05 18:09:20,828 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 18:09:20,829 - INFO - joeynmt.training - \tSource:     Today is June 18th and it is Muiriel's birthday!\n",
      "2022-08-05 18:09:20,830 - INFO - joeynmt.training - \tReference:  Hoje é dia 18 de junho, aniversário do Muiriel!\n",
      "2022-08-05 18:09:20,830 - INFO - joeynmt.training - \tHypothesis: Hoje é 18 e o dia de junho de Muiriel!\n",
      "2022-08-05 18:09:20,830 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 18:09:20,831 - INFO - joeynmt.training - \tSource:     The archer killed the deer.\n",
      "2022-08-05 18:09:20,831 - INFO - joeynmt.training - \tReference:  O arqueiro matou o cervo.\n",
      "2022-08-05 18:09:20,831 - INFO - joeynmt.training - \tHypothesis: O ser humano matou o mais.\n",
      "2022-08-05 18:09:20,831 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 18:09:20,833 - INFO - joeynmt.training - \tSource:     I'm going to go.\n",
      "2022-08-05 18:09:20,833 - INFO - joeynmt.training - \tReference:  Eu irei.\n",
      "2022-08-05 18:09:20,833 - INFO - joeynmt.training - \tHypothesis: Eu vou.\n",
      "2022-08-05 18:09:20,833 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 18:09:20,834 - INFO - joeynmt.training - \tSource:     Why are you sorry for something you haven't done?\n",
      "2022-08-05 18:09:20,834 - INFO - joeynmt.training - \tReference:  Por que as senhoras se desculpam de algo que não fizeram?\n",
      "2022-08-05 18:09:20,834 - INFO - joeynmt.training - \tHypothesis: Por que você está com desculpa por algo que não fez?\n",
      "2022-08-05 18:09:50,274 - INFO - joeynmt.training - Epoch   3: total training loss 353.11\n",
      "2022-08-05 18:09:50,274 - INFO - joeynmt.training - EPOCH 4\n",
      "2022-08-05 18:10:28,854 - INFO - joeynmt.training - Epoch   4, Step:    36100, Batch Loss:     0.181724, Batch Acc: 0.012491, Tokens per Sec:     1380, Lr: 0.000047\n",
      "2022-08-05 18:11:38,842 - INFO - joeynmt.training - Epoch   4, Step:    36200, Batch Loss:     0.196862, Batch Acc: 0.009289, Tokens per Sec:     1360, Lr: 0.000047\n",
      "2022-08-05 18:12:43,933 - INFO - joeynmt.training - Epoch   4, Step:    36300, Batch Loss:     0.169576, Batch Acc: 0.007649, Tokens per Sec:     1460, Lr: 0.000047\n",
      "2022-08-05 18:13:48,631 - INFO - joeynmt.training - Epoch   4, Step:    36400, Batch Loss:     0.174821, Batch Acc: 0.008240, Tokens per Sec:     1456, Lr: 0.000047\n",
      "2022-08-05 18:14:52,844 - INFO - joeynmt.training - Epoch   4, Step:    36500, Batch Loss:     0.134855, Batch Acc: 0.008888, Tokens per Sec:     1484, Lr: 0.000047\n",
      "2022-08-05 18:15:57,162 - INFO - joeynmt.training - Epoch   4, Step:    36600, Batch Loss:     0.166280, Batch Acc: 0.007400, Tokens per Sec:     1467, Lr: 0.000047\n",
      "2022-08-05 18:17:01,640 - INFO - joeynmt.training - Epoch   4, Step:    36700, Batch Loss:     0.144623, Batch Acc: 0.009808, Tokens per Sec:     1455, Lr: 0.000047\n",
      "2022-08-05 18:18:05,586 - INFO - joeynmt.training - Epoch   4, Step:    36800, Batch Loss:     0.199953, Batch Acc: 0.008778, Tokens per Sec:     1468, Lr: 0.000047\n",
      "2022-08-05 18:19:09,594 - INFO - joeynmt.training - Epoch   4, Step:    36900, Batch Loss:     0.184701, Batch Acc: 0.008171, Tokens per Sec:     1490, Lr: 0.000047\n",
      "2022-08-05 18:20:13,896 - INFO - joeynmt.training - Epoch   4, Step:    37000, Batch Loss:     0.163194, Batch Acc: 0.008379, Tokens per Sec:     1474, Lr: 0.000046\n",
      "2022-08-05 18:20:14,225 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-77eaaa270e19dd54.arrow\n",
      "2022-08-05 18:20:14,550 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-ed0023f35ba28384.arrow\n",
      "2022-08-05 18:20:14,551 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-e1b67c1b4238629a.arrow\n",
      "2022-08-05 18:20:14,553 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=37000\n",
      "2022-08-05 18:20:14,553 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 18:20:27,854 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 18:20:27,854 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  32.73, loss:   1.50, ppl:   4.46, acc:   0.69, generation: 13.2802[sec], evaluation: 0.0160[sec]\n",
      "2022-08-05 18:20:27,855 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 18:20:27,856 - INFO - joeynmt.training - \tSource:     I never liked biology.\n",
      "2022-08-05 18:20:27,856 - INFO - joeynmt.training - \tReference:  Nunca gostei de biologia.\n",
      "2022-08-05 18:20:27,856 - INFO - joeynmt.training - \tHypothesis: Eu nunca gostei de biologia.\n",
      "2022-08-05 18:20:27,856 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 18:20:27,858 - INFO - joeynmt.training - \tSource:     You're so impatient with me.\n",
      "2022-08-05 18:20:27,858 - INFO - joeynmt.training - \tReference:  Você está tão impaciente comigo.\n",
      "2022-08-05 18:20:27,858 - INFO - joeynmt.training - \tHypothesis: Você está tão impaciente comigo.\n",
      "2022-08-05 18:20:27,858 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 18:20:27,859 - INFO - joeynmt.training - \tSource:     Seeing that you're not surprised, I think you must have known.\n",
      "2022-08-05 18:20:27,859 - INFO - joeynmt.training - \tReference:  Vendo como não está surpreso, acho que você já deveria estar sabendo.\n",
      "2022-08-05 18:20:27,859 - INFO - joeynmt.training - \tHypothesis: Até que você não está surpresa, eu acho que você deve saber.\n",
      "2022-08-05 18:20:27,859 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 18:20:27,860 - INFO - joeynmt.training - \tSource:     Thanks, that's all.\n",
      "2022-08-05 18:20:27,860 - INFO - joeynmt.training - \tReference:  Obrigado, é só isso.\n",
      "2022-08-05 18:20:27,860 - INFO - joeynmt.training - \tHypothesis: Obrigado, tudo isso é.\n",
      "2022-08-05 18:21:32,111 - INFO - joeynmt.training - Epoch   4, Step:    37100, Batch Loss:     0.172644, Batch Acc: 0.007791, Tokens per Sec:     1481, Lr: 0.000046\n",
      "2022-08-05 18:22:36,185 - INFO - joeynmt.training - Epoch   4, Step:    37200, Batch Loss:     0.150387, Batch Acc: 0.008047, Tokens per Sec:     1499, Lr: 0.000046\n",
      "2022-08-05 18:23:40,162 - INFO - joeynmt.training - Epoch   4, Step:    37300, Batch Loss:     0.193403, Batch Acc: 0.006265, Tokens per Sec:     1489, Lr: 0.000046\n",
      "2022-08-05 18:24:43,996 - INFO - joeynmt.training - Epoch   4, Step:    37400, Batch Loss:     0.155759, Batch Acc: 0.008952, Tokens per Sec:     1484, Lr: 0.000046\n",
      "2022-08-05 18:25:47,622 - INFO - joeynmt.training - Epoch   4, Step:    37500, Batch Loss:     0.174290, Batch Acc: 0.009175, Tokens per Sec:     1478, Lr: 0.000046\n",
      "2022-08-05 18:26:51,977 - INFO - joeynmt.training - Epoch   4, Step:    37600, Batch Loss:     0.169565, Batch Acc: 0.008644, Tokens per Sec:     1469, Lr: 0.000046\n",
      "2022-08-05 18:27:56,146 - INFO - joeynmt.training - Epoch   4, Step:    37700, Batch Loss:     0.161667, Batch Acc: 0.007967, Tokens per Sec:     1498, Lr: 0.000046\n",
      "2022-08-05 18:28:59,570 - INFO - joeynmt.training - Epoch   4, Step:    37800, Batch Loss:     0.180739, Batch Acc: 0.009045, Tokens per Sec:     1499, Lr: 0.000046\n",
      "2022-08-05 18:30:03,710 - INFO - joeynmt.training - Epoch   4, Step:    37900, Batch Loss:     0.165224, Batch Acc: 0.008579, Tokens per Sec:     1472, Lr: 0.000046\n",
      "2022-08-05 18:31:07,630 - INFO - joeynmt.training - Epoch   4, Step:    38000, Batch Loss:     0.193335, Batch Acc: 0.006577, Tokens per Sec:     1511, Lr: 0.000046\n",
      "2022-08-05 18:31:07,957 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-8ea045cecd86721e.arrow\n",
      "2022-08-05 18:31:08,282 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-ede689a93b5afe0c.arrow\n",
      "2022-08-05 18:31:08,284 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-a162bd069e031250.arrow\n",
      "2022-08-05 18:31:08,285 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=38000\n",
      "2022-08-05 18:31:08,286 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 18:31:25,444 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 18:31:25,444 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.99, loss:   1.63, ppl:   5.10, acc:   0.68, generation: 17.1371[sec], evaluation: 0.0166[sec]\n",
      "2022-08-05 18:31:25,444 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 18:31:25,446 - INFO - joeynmt.training - \tSource:     Why are you sorry for something you haven't done?\n",
      "2022-08-05 18:31:25,446 - INFO - joeynmt.training - \tReference:  Por que o senhor está se desculpando de algo que não fez?\n",
      "2022-08-05 18:31:25,446 - INFO - joeynmt.training - \tHypothesis: Por que você está com desculpa por algo que não fez?\n",
      "2022-08-05 18:31:25,446 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 18:31:25,448 - INFO - joeynmt.training - \tSource:     Excuse me; allow me to point out three errors in the above article.\n",
      "2022-08-05 18:31:25,448 - INFO - joeynmt.training - \tReference:  Com licença; permita-me apontar três erros no artigo acima.\n",
      "2022-08-05 18:31:25,448 - INFO - joeynmt.training - \tHypothesis: Com licença; eu permite apontar três erros no artigo acima.\n",
      "2022-08-05 18:31:25,448 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 18:31:25,449 - INFO - joeynmt.training - \tSource:     Most people think I'm crazy.\n",
      "2022-08-05 18:31:25,449 - INFO - joeynmt.training - \tReference:  A maioria das pessoas acham que eu sou louco.\n",
      "2022-08-05 18:31:25,450 - INFO - joeynmt.training - \tHypothesis: A maioria das pessoas acha que eu sou louco.\n",
      "2022-08-05 18:31:25,450 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 18:31:25,451 - INFO - joeynmt.training - \tSource:     America is a lovely place to be, if you are here to earn money.\n",
      "2022-08-05 18:31:25,451 - INFO - joeynmt.training - \tReference:  Os EUA são um bom lugar para viver, se você estiver aqui para ganhar dinheiro.\n",
      "2022-08-05 18:31:25,451 - INFO - joeynmt.training - \tHypothesis: A América é um belo lugar para ser, se você estiver aqui para ganhar dinheiro.\n",
      "2022-08-05 18:32:01,949 - INFO - joeynmt.training - Epoch   4: total training loss 343.02\n",
      "2022-08-05 18:32:01,949 - INFO - joeynmt.training - EPOCH 5\n",
      "2022-08-05 18:32:29,504 - INFO - joeynmt.training - Epoch   5, Step:    38100, Batch Loss:     0.139371, Batch Acc: 0.022873, Tokens per Sec:     1501, Lr: 0.000046\n",
      "2022-08-05 18:33:34,252 - INFO - joeynmt.training - Epoch   5, Step:    38200, Batch Loss:     0.161480, Batch Acc: 0.007927, Tokens per Sec:     1453, Lr: 0.000046\n",
      "2022-08-05 18:34:38,437 - INFO - joeynmt.training - Epoch   5, Step:    38300, Batch Loss:     0.176155, Batch Acc: 0.007240, Tokens per Sec:     1491, Lr: 0.000046\n",
      "2022-08-05 18:35:42,813 - INFO - joeynmt.training - Epoch   5, Step:    38400, Batch Loss:     0.157239, Batch Acc: 0.007738, Tokens per Sec:     1484, Lr: 0.000046\n",
      "2022-08-05 18:36:48,055 - INFO - joeynmt.training - Epoch   5, Step:    38500, Batch Loss:     0.157267, Batch Acc: 0.008383, Tokens per Sec:     1477, Lr: 0.000046\n",
      "2022-08-05 18:37:53,693 - INFO - joeynmt.training - Epoch   5, Step:    38600, Batch Loss:     0.154615, Batch Acc: 0.009064, Tokens per Sec:     1469, Lr: 0.000046\n",
      "2022-08-05 18:38:57,806 - INFO - joeynmt.training - Epoch   5, Step:    38700, Batch Loss:     0.153325, Batch Acc: 0.009295, Tokens per Sec:     1485, Lr: 0.000045\n",
      "2022-08-05 18:40:02,201 - INFO - joeynmt.training - Epoch   5, Step:    38800, Batch Loss:     0.164328, Batch Acc: 0.008123, Tokens per Sec:     1455, Lr: 0.000045\n",
      "2022-08-05 18:41:06,652 - INFO - joeynmt.training - Epoch   5, Step:    38900, Batch Loss:     0.153806, Batch Acc: 0.007667, Tokens per Sec:     1451, Lr: 0.000045\n",
      "2022-08-05 18:42:11,405 - INFO - joeynmt.training - Epoch   5, Step:    39000, Batch Loss:     0.150090, Batch Acc: 0.008887, Tokens per Sec:     1474, Lr: 0.000045\n",
      "2022-08-05 18:42:11,735 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-c983c24d1f6cf6b8.arrow\n",
      "2022-08-05 18:42:12,057 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-1a85fac48ce4f6e7.arrow\n",
      "2022-08-05 18:42:12,059 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/lconti/en-pt_tatoeba/validation/cache-1cca74dde2a5e259.arrow\n",
      "2022-08-05 18:42:12,060 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=39000\n",
      "2022-08-05 18:42:12,060 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 18:42:29,990 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 18:42:29,990 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.67, loss:   1.61, ppl:   4.98, acc:   0.68, generation: 17.9078[sec], evaluation: 0.0168[sec]\n",
      "2022-08-05 18:42:29,990 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 18:42:29,992 - INFO - joeynmt.training - \tSource:     I didn't like it.\n",
      "2022-08-05 18:42:29,992 - INFO - joeynmt.training - \tReference:  Eu não gostei.\n",
      "2022-08-05 18:42:29,992 - INFO - joeynmt.training - \tHypothesis: Eu não gostei.\n",
      "2022-08-05 18:42:29,992 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 18:42:29,994 - INFO - joeynmt.training - \tSource:     You look stupid.\n",
      "2022-08-05 18:42:29,994 - INFO - joeynmt.training - \tReference:  Você está ridículo.\n",
      "2022-08-05 18:42:29,994 - INFO - joeynmt.training - \tHypothesis: Você parece estúpido.\n",
      "2022-08-05 18:42:29,994 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 18:42:29,995 - INFO - joeynmt.training - \tSource:     I miss you.\n",
      "2022-08-05 18:42:29,995 - INFO - joeynmt.training - \tReference:  Sinto a tua falta.\n",
      "2022-08-05 18:42:29,995 - INFO - joeynmt.training - \tHypothesis: Sinto saudades de você.\n",
      "2022-08-05 18:42:29,995 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 18:42:29,996 - INFO - joeynmt.training - \tSource:     You are in my way.\n",
      "2022-08-05 18:42:29,996 - INFO - joeynmt.training - \tReference:  As senhoras estão a impedir-me a passagem.\n",
      "2022-08-05 18:42:29,996 - INFO - joeynmt.training - \tHypothesis: Você está no meu caminho.\n",
      "2022-08-05 18:43:34,041 - INFO - joeynmt.training - Epoch   5, Step:    39100, Batch Loss:     0.163549, Batch Acc: 0.008221, Tokens per Sec:     1444, Lr: 0.000045\n",
      "2022-08-05 18:44:37,190 - INFO - joeynmt.training - Epoch   5, Step:    39200, Batch Loss:     0.136661, Batch Acc: 0.008634, Tokens per Sec:     1517, Lr: 0.000045\n",
      "2022-08-05 18:45:41,097 - INFO - joeynmt.training - Epoch   5, Step:    39300, Batch Loss:     0.175714, Batch Acc: 0.008588, Tokens per Sec:     1487, Lr: 0.000045\n",
      "2022-08-05 18:46:44,861 - INFO - joeynmt.training - Epoch   5, Step:    39400, Batch Loss:     0.180359, Batch Acc: 0.009024, Tokens per Sec:     1502, Lr: 0.000045\n",
      "2022-08-05 18:47:48,897 - INFO - joeynmt.training - Epoch   5, Step:    39500, Batch Loss:     0.143176, Batch Acc: 0.008685, Tokens per Sec:     1483, Lr: 0.000045\n",
      "2022-08-05 18:48:53,042 - INFO - joeynmt.training - Epoch   5, Step:    39600, Batch Loss:     0.169708, Batch Acc: 0.009115, Tokens per Sec:     1483, Lr: 0.000045\n",
      "2022-08-05 18:49:57,485 - INFO - joeynmt.training - Epoch   5, Step:    39700, Batch Loss:     0.155393, Batch Acc: 0.009095, Tokens per Sec:     1462, Lr: 0.000045\n",
      "2022-08-05 18:51:01,912 - INFO - joeynmt.training - Epoch   5, Step:    39800, Batch Loss:     0.171886, Batch Acc: 0.008437, Tokens per Sec:     1474, Lr: 0.000045\n",
      "2022-08-05 18:52:06,201 - INFO - joeynmt.training - Epoch   5, Step:    39900, Batch Loss:     0.179092, Batch Acc: 0.007858, Tokens per Sec:     1463, Lr: 0.000045\n",
      "2022-08-05 18:53:10,384 - INFO - joeynmt.training - Epoch   5, Step:    40000, Batch Loss:     0.158983, Batch Acc: 0.008042, Tokens per Sec:     1484, Lr: 0.000045\n",
      "Dropping NaN...: 100%|████████████████████████████| 1/1 [00:00<00:00, 81.65ba/s]\n",
      "Preprocessing...: 100%|██████████████████| 1000/1000 [00:00<00:00, 16260.27ex/s]\n",
      "2022-08-05 18:53:11,118 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=40000\n",
      "2022-08-05 18:53:11,118 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-08-05 18:53:27,719 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\n",
      "2022-08-05 18:53:27,719 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  34.80, loss:   1.56, ppl:   4.77, acc:   0.68, generation: 16.5811[sec], evaluation: 0.0156[sec]\n",
      "2022-08-05 18:53:27,720 - INFO - joeynmt.training - Example #0\n",
      "2022-08-05 18:53:27,721 - INFO - joeynmt.training - \tSource:     The police will get you to find the bullets.\n",
      "2022-08-05 18:53:27,722 - INFO - joeynmt.training - \tReference:  A polícia vai fazer você encontrar as balas.\n",
      "2022-08-05 18:53:27,722 - INFO - joeynmt.training - \tHypothesis: A polícia vai te encontrar a balas.\n",
      "2022-08-05 18:53:27,722 - INFO - joeynmt.training - Example #1\n",
      "2022-08-05 18:53:27,723 - INFO - joeynmt.training - \tSource:     Who doesn't know this problem?!\n",
      "2022-08-05 18:53:27,723 - INFO - joeynmt.training - \tReference:  Quem não conhece esse problema?\n",
      "2022-08-05 18:53:27,723 - INFO - joeynmt.training - \tHypothesis: Quem não sabe este problema?\n",
      "2022-08-05 18:53:27,723 - INFO - joeynmt.training - Example #2\n",
      "2022-08-05 18:53:27,724 - INFO - joeynmt.training - \tSource:     \"Why aren't you going?\" \"Because I don't want to.\"\n",
      "2022-08-05 18:53:27,724 - INFO - joeynmt.training - \tReference:  \"Por que você não vai?\" \"Porque não estou a fim de ir.\"\n",
      "2022-08-05 18:53:27,724 - INFO - joeynmt.training - \tHypothesis: \"Por que você não vai?\" \"Porque eu não quero.\"\n",
      "2022-08-05 18:53:27,724 - INFO - joeynmt.training - Example #3\n",
      "2022-08-05 18:53:27,726 - INFO - joeynmt.training - \tSource:     Many people drift through life without a purpose.\n",
      "2022-08-05 18:53:27,726 - INFO - joeynmt.training - \tReference:  Muita gente vagueia pela vida sem um propósito.\n",
      "2022-08-05 18:53:27,726 - INFO - joeynmt.training - \tHypothesis: Muitas pessoas se aproximam por vida sem um propósito.\n",
      "2022-08-05 18:53:27,728 - INFO - joeynmt.training - Training ended since maximum num. of updates 40000 was reached.\n",
      "2022-08-05 18:53:27,728 - INFO - joeynmt.training - Best validation result (greedy) at step    25000:  37.78 bleu.\n",
      "2022-08-05 18:53:27,748 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-05 18:53:28,025 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-05 18:53:28,028 - INFO - joeynmt.model - Total params: 19252224\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lconti/.linuxbrew/opt/python@3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/lconti/.linuxbrew/opt/python@3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lconti/joeynmt/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/home/lconti/joeynmt/joeynmt/__main__.py\", line 44, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/home/lconti/joeynmt/joeynmt/training.py\", line 861, in train\n",
      "    test(\n",
      "  File \"/home/lconti/joeynmt/joeynmt/prediction.py\", line 368, in test\n",
      "    model_checkpoint = load_checkpoint(ckpt, device=device)\n",
      "  File \"/home/lconti/joeynmt/joeynmt/helpers.py\", line 511, in load_checkpoint\n",
      "    assert path.is_file(), f\"Checkpoint {path} not found.\"\n",
      "AssertionError: Checkpoint /home/lconti/en-pt_tatoeba/models/finetune2_tf/25000.ckpt not found.\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/config_finetune2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaHtCsyJf1yD"
   },
   "source": [
    "> 💡 It starts counting the epochs from the beginning again, but step numbers should continue from before and you should find a \"reloading\" line in the training log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVv1ja0eCk66"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The `test` mode can be used to translate (and evaluate on) the test set specified in the configuration. We usually do this only once after we've tuned hyperparameters on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5T0OEp22BnX",
    "outputId": "327025da-0e8a-4b54-d602-019e1289c2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 18:56:39,107 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-08-05 18:56:39,107 - INFO - joeynmt.data - Building tokenizer...\n",
      "2022-08-05 18:56:39,192 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 18:56:39,192 - INFO - joeynmt.tokenizers - pt tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-08-05 18:56:39,192 - INFO - joeynmt.data - Building vocabulary...\n",
      "2022-08-05 18:56:51,465 - INFO - joeynmt.data - Loading dev set...\n",
      "2022-08-05 18:56:52,039 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-e3360f65f1f28706.arrow\n",
      "2022-08-05 18:56:52,364 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/validation/cache-bbcf864065c98515.arrow\n",
      "2022-08-05 18:56:52,365 - INFO - joeynmt.data - Loading test set...\n",
      "2022-08-05 18:56:52,698 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-0099eb081810535f.arrow\n",
      "2022-08-05 18:56:53,031 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lconti/en-pt_tatoeba/test/cache-7c540055e789733b.arrow\n",
      "2022-08-05 18:56:53,032 - INFO - joeynmt.data - Data loaded.\n",
      "2022-08-05 18:56:53,032 - INFO - joeynmt.helpers - Train dataset: None\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=200, split=validation, path=/home/lconti/en-pt_tatoeba/validation)\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1000, src_lang=en, trg_lang=pt, has_trg=True, random_subset=-1, split=test, path=/home/lconti/en-pt_tatoeba/test)\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ▁Tom (6) ' (7) ▁I (8) ? (9) ▁a\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 32000\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 32000\n",
      "2022-08-05 18:56:53,033 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-08-05 18:56:53,337 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-08-05 18:56:53,340 - INFO - joeynmt.model - Total params: 19252224\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lconti/.linuxbrew/opt/python@3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/lconti/.linuxbrew/opt/python@3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lconti/joeynmt/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/home/lconti/joeynmt/joeynmt/__main__.py\", line 46, in main\n",
      "    test(\n",
      "  File \"/home/lconti/joeynmt/joeynmt/prediction.py\", line 368, in test\n",
      "    model_checkpoint = load_checkpoint(ckpt, device=device)\n",
      "  File \"/home/lconti/joeynmt/joeynmt/helpers.py\", line 511, in load_checkpoint\n",
      "    assert path.is_file(), f\"Checkpoint {path} not found.\"\n",
      "AssertionError: Checkpoint /home/lconti/en-pt_tatoeba/models/finetune2_tf/best.ckpt not found.\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt test {data_dir}/config_finetune2.yaml --ckpt /home/lconti/en-pt_tatoeba/models/finetune2_tf/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy5Y4Qr_3p3I"
   },
   "source": [
    "> ⚠ In beam search, the batch size is expanded {beam_size} times. For instance, if batch_size=10, batch_type=sentence and beam_size=5, joeynmt internally creates a batch of length 10*5=50. It may cause an out-of-memory error. Please specify the batch_size in `testing` section of config.yaml by taking this into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ripZMg6kCqzd"
   },
   "source": [
    "The `translate` mode is more interactive and takes prompts to translate interactively.\n",
    "\n",
    "Let's Translate a few examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEcyEwpS1Pvi",
    "outputId": "2259e7d7-ca22-409c-f6ea-593fa3b5de83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:43:59,643 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:44:17,067 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:44:17,429 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:44:21,857 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:44:22,065 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Übersetzung macht Spaß!\n",
      "2022-06-04 22:46:22,281 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:46:22,379 - INFO - joeynmt.prediction - Generation took 0.0963[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Übersetzung Sinn?\n",
      "2022-06-04 22:47:21,390 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:47:21,489 - INFO - joeynmt.prediction - Generation took 0.0975[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\n",
      "Please enter a source sentence:\n",
      "\n",
      "Bye.\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 2037, in shutdown\n",
      "    h.close()\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1103, in close\n",
      "    stream.close()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH5QwafwIYZu"
   },
   "source": [
    "You can also get the n-best hypotheses (up to the size of the beam, in our example 5), not only the highest scoring one. The better your model gets, the more interesting should the alternatives be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzpnG_3IJwkj"
   },
   "outputs": [],
   "source": [
    "nbest_config = config.replace('n_best: 1', 'n_best: 5')\\\n",
    "  .replace('#return_prob: \"hyp\"', 'return_prob: \"hyp\"')\n",
    "\n",
    "with (Path(data_dir) / \"nbest_config.yaml\").open('w') as f:\n",
    "    f.write(nbest_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nya4YXTGRurr",
    "outputId": "b879c6e2-2d12-4d22-d1ad-d441f87eaae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:48:11,849 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
      "2022-06-04 22:48:29,174 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2022-06-04 22:48:29,557 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2022-06-04 22:48:34,476 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/models/tatoeba_deen_resume/19000.ckpt.\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - de tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "2022-06-04 22:48:34,743 - INFO - joeynmt.tokenizers - en tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
      "\n",
      "Please enter a source sentence:\n",
      "Maschinelle Übersetzung macht Spaß!\n",
      "2022-06-04 22:49:28,668 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:28,766 - INFO - joeynmt.prediction - Generation took 0.0962[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: Don't drink translation is fun.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁drink', '▁translation', '▁is', '▁fun', '.', '</s>']\n",
      "\tsequence score: -3.362283706665039\n",
      "#2: Don't make any translation fun.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁make', '▁any', '▁translation', '▁fun', '.', '</s>']\n",
      "\tsequence score: -3.677445411682129\n",
      "#3: Don't make sense to the machine.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁make', '▁sense', '▁to', '▁the', '▁machine', '.', '</s>']\n",
      "\tsequence score: -3.9387967586517334\n",
      "#4: Don't drink translation is fun!\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁drink', '▁translation', '▁is', '▁fun', '!', '</s>']\n",
      "\tsequence score: -3.9769201278686523\n",
      "#5: Don't make a good translation.\n",
      "\ttokens: ['▁Don', \"'\", 't', '▁make', '▁a', '▁good', '▁translation', '.', '</s>']\n",
      "\tsequence score: -4.1912617683410645\n",
      "\n",
      "Please enter a source sentence:\n",
      "Wann macht maschinelle Übersetzung Sinn?\n",
      "2022-06-04 22:49:47,934 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=100, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
      "2022-06-04 22:49:48,022 - INFO - joeynmt.prediction - Generation took 0.0862[sec]. (No references given)\n",
      "JoeyNMT:\n",
      "#1: When does machine make sense?\n",
      "\ttokens: ['▁When', '▁does', '▁machine', '▁make', '▁sense', '?', '</s>']\n",
      "\tsequence score: -2.5040993690490723\n",
      "#2: When does the translation make sense?\n",
      "\ttokens: ['▁When', '▁does', '▁the', '▁translation', '▁make', '▁sense', '?', '</s>']\n",
      "\tsequence score: -2.5658979415893555\n",
      "#3: When does machine make sense to sense?\n",
      "\ttokens: ['▁When', '▁does', '▁machine', '▁make', '▁sense', '▁to', '▁sense', '?', '</s>']\n",
      "\tsequence score: -3.325543165206909\n",
      "#4: When does machine make sense to make?\n",
      "\ttokens: ['▁When', '▁does', '▁machine', '▁make', '▁sense', '▁to', '▁make', '?', '</s>']\n",
      "\tsequence score: -3.422945976257324\n",
      "#5: When does the translation use?\n",
      "\ttokens: ['▁When', '▁does', '▁the', '▁translation', '▁use', '?', '</s>']\n",
      "\tsequence score: -3.6186904907226562\n",
      "\n",
      "Please enter a source sentence:\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 64, in <module>\n",
      "    main()\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/__main__.py\", line 57, in main\n",
      "    output_path=args.output_path,\n",
      "  File \"/content/drive/MyDrive/joeynmt/joeynmt/prediction.py\", line 557, in translate\n",
      "    src_input = input(\"\\nPlease enter a source sentence:\\n\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt translate {data_dir}/nbest_config.yaml --ckpt {model_dir}_resume/best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28sXN0GNIYAM"
   },
   "source": [
    "> 💡 In BPE decoding, there are multiple ways to tokenize one sequence. That is, the same output string sequence might appear multiple times in the n best list, because they have different tokenization and thus different sequence in the generation.\n",
    "> For instance, say 3-best generation were:\n",
    "> ```\n",
    "> #1 best ['▁', 'N', 'e', 'w', '▁York']\n",
    "> #2 best ['▁', 'New', '▁York']\n",
    "> #3 best ['▁', 'New', '▁Y', 'o', 'r', 'k']\n",
    "> ````\n",
    "All three were different in next-token prediction, but ended up the same string sequence `New York` after being un-bpe-ed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "\n",
    "### plotting learning curves\n",
    "\n",
    "`plot_validations.py` script will generate validation learning curves.\n",
    "\n",
    "We resumed the training once, so first we concatenate the validations.txt file, and use the concatenated validations.txt for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/lconti/en-pt_tatoeba/models/normal_tf/validations.txt /home/lconti/en-pt_tatoeba/models/finetune_tf/validations.txt > /home/lconti/en-pt_tatoeba/models/finetune_tf/_validations.txt\n",
    "!mv /home/lconti/en-pt_tatoeba/models/finetune_tf/validations.txt /home/lconti/en-pt_tatoeba/models/finetune_tf/resumed_valudations.txt\n",
    "!mv /home/lconti/en-pt_tatoeba/models/finetune_tf/_validations.txt /home/lconti/en-pt_tatoeba/models/finetune_tf/validations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/lconti/joeynmt/scripts/plot_validations.py /home/lconti/en-pt_tatoeba/models/finetune_tf --output_path /home/lconti/results/finetune_learning_curve.png"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "quick-start-with-joeynmt2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('my_jnmt': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "86dc5b16c9dc4e6fbdf96ccbf0568163a9040f9d731ebaeec1938114527cf668"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1795a2b1d6354fd2b1c69b12c9487f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27e4365db7d644d486745b9f334d06db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ff1b97d66404e488862d694facdfc23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fdb10ecb2040f8bd5e07834b422ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a00af231d8a44e9972f6916948418fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79a7716ac54b4f50981033df15f06f8e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c26c414246154062bbd0682340613e40",
      "value": 1
     }
    },
    "4a5a5a13229f404f9e4085751b0e2104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c447e15d9354c3c9a47c3ff843e74ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51afb334205f4a26b43fd6ddd0036cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52fc941d087447aa88ca1336e89d1d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6505b17c75e64c709fc6a91cc362444d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a841a34dd549f4bbe9fed60fc03853",
      "placeholder": "​",
      "style": "IPY_MODEL_31fdb10ecb2040f8bd5e07834b422ca0",
      "value": "Generating train split: "
     }
    },
    "65a841a34dd549f4bbe9fed60fc03853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6690085804c940ccb8dbd87e3f563678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6505b17c75e64c709fc6a91cc362444d",
       "IPY_MODEL_3a00af231d8a44e9972f6916948418fd",
       "IPY_MODEL_e79f7e89c05d4ce4935f53929b8aaaa1"
      ],
      "layout": "IPY_MODEL_4c447e15d9354c3c9a47c3ff843e74ce"
     }
    },
    "6d393ab342cc46109d1c723d736320ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52fc941d087447aa88ca1336e89d1d0f",
      "placeholder": "​",
      "style": "IPY_MODEL_1795a2b1d6354fd2b1c69b12c9487f0b",
      "value": " 1/1 [00:00&lt;00:00, 24.21ba/s]"
     }
    },
    "79a7716ac54b4f50981033df15f06f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9c838f660f7a46da92cfdb54c25fcb58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e9312e5e6646739ae3e6738de0e90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff1b97d66404e488862d694facdfc23",
      "placeholder": "​",
      "style": "IPY_MODEL_27e4365db7d644d486745b9f334d06db",
      "value": "Flattening the indices: 100%"
     }
    },
    "b669da1ab44a4e8cbfa6674f2dac1e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14c2320cc944c469a52a5579ee471bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c838f660f7a46da92cfdb54c25fcb58",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51afb334205f4a26b43fd6ddd0036cbe",
      "value": 1
     }
    },
    "c26c414246154062bbd0682340613e40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c951a5fc9f1540249dbb03a0ecf2f2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e9312e5e6646739ae3e6738de0e90c",
       "IPY_MODEL_c14c2320cc944c469a52a5579ee471bb",
       "IPY_MODEL_6d393ab342cc46109d1c723d736320ad"
      ],
      "layout": "IPY_MODEL_b669da1ab44a4e8cbfa6674f2dac1e5b"
     }
    },
    "e73400a7748241d29340d21f94784a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e79f7e89c05d4ce4935f53929b8aaaa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e73400a7748241d29340d21f94784a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_4a5a5a13229f404f9e4085751b0e2104",
      "value": " 304551/0 [00:13&lt;00:00, 21566.82 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
